{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c0bfc3",
   "metadata": {},
   "source": [
    "# 載入必要套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe889070",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers faiss-cpu tqdm\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c2bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db85dc31",
   "metadata": {},
   "source": [
    "# Step 1：資料清洗＋段落切分＋向量化＋ScaNN建構"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90a211d",
   "metadata": {},
   "source": [
    "第一版本:FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eb0677f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "分段&結構化清洗: 100%|██████████| 100/100 [00:00<00:00, 1087.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共保留有效段落數: 1364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "向量化: 100%|██████████| 43/43 [00:29<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index 儲存至: faiss_index_public.bin\n",
      "metadata 儲存至: faiss_index_public_metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 1. 載入 JSON 資料\n",
    "def load_jsonl(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# 2. 資料清洗規則\n",
    "EXCLUDED_HEADERS = [\n",
    "    \"abstract\", \"references\", \"acknowledgement\", \"acknowledgments\", \n",
    "    \"appendix\", \"author contributions\", \"supplementary materials\"\n",
    "]\n",
    "\n",
    "def clean_paragraph(p):\n",
    "    p = p.strip()\n",
    "    p = re.sub(r'\\s+', ' ', p)\n",
    "    return p\n",
    "\n",
    "def is_informative(p, min_len=30):\n",
    "    if not p or len(p) < min_len:\n",
    "        return False\n",
    "    lower_p = p.lower()\n",
    "    return not any(lower_p.startswith(h) for h in EXCLUDED_HEADERS)\n",
    "\n",
    "# 3. 分段+清洗\n",
    "def chunk_papers(data, section_splitter=True):\n",
    "    all_chunks = []\n",
    "    metadata = []\n",
    "    for item in tqdm(data, desc=\"分段&結構化清洗\"):\n",
    "        title = item[\"title\"]\n",
    "        # === 第 1 層切分（以 \\n\\n\\n 當作大章節）\n",
    "        raw_sections = item[\"full_text\"].split(\"\\n\\n\\n\")\n",
    "        for section_id, raw_section in enumerate(raw_sections):\n",
    "            # === 第 2 層切分（針對 ::: 子章節）\n",
    "            if section_splitter and \" ::: \" in raw_section:\n",
    "                sub_sections = raw_section.split(\" ::: \")\n",
    "                first_header, *rest = sub_sections\n",
    "                sub_chunks = [first_header.strip()] + [s.strip() for s in rest]\n",
    "            else:\n",
    "                sub_chunks = [raw_section.strip()]\n",
    "\n",
    "            for chunk_id, chunk in enumerate(sub_chunks):\n",
    "                clean_chunk = clean_paragraph(chunk)\n",
    "                if is_informative(clean_chunk):\n",
    "                    all_chunks.append(clean_chunk)\n",
    "                    metadata.append({\n",
    "                        \"title\": title,\n",
    "                        \"section_id\": section_id,\n",
    "                        \"chunk_id\": chunk_id,\n",
    "                        \"section_name\": clean_chunk.split(\" \")[0][:50]  # 取開頭詞當section tag\n",
    "                    })\n",
    "    return all_chunks, metadata\n",
    "\n",
    "# 4. 編碼\n",
    "def encode_chunks(chunks, model_name=\"BAAI/bge-base-en-v1.5\", batch_size=32):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(chunks), batch_size), desc=\"向量化\"):\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        embs = model.encode(batch, show_progress_bar=False, normalize_embeddings=True)\n",
    "        embeddings.append(embs)\n",
    "    embeddings = np.vstack(embeddings).astype('float32')\n",
    "    return embeddings\n",
    "\n",
    "# 5. 建立 FAISS 向量索引\n",
    "def build_faiss(embeddings, dim=None, save_path=\"faiss_index\"):\n",
    "    if dim is None:\n",
    "        dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)   # 內積 (更快且常見)\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, f\"{save_path}.bin\")\n",
    "    print(f\"FAISS index 儲存至: {save_path}.bin\")\n",
    "    return index\n",
    "\n",
    "def save_metadata(metadata, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"metadata 儲存至: {path}\")\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    sentences = re.split(r'(?<=[。！？!?\\.])\\s+', text.strip())\n",
    "    return [s.strip() for s in sentences if len(s.strip()) > 10]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 載入資料\n",
    "    data = load_jsonl(\"datasets/private_dataset.json\")\n",
    "    # 2. 分段與清洗\n",
    "    chunks, meta = chunk_papers(data)\n",
    "    print(f\"共保留有效段落數: {len(chunks)}\")\n",
    "    with open(\"chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # 3. 嵌入向量\n",
    "    emb = encode_chunks(chunks, model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "    # 4. 建立 FAISS 向量索引\n",
    "    build_faiss(emb, dim=emb.shape[1], save_path=\"faiss_index_public\")\n",
    "    # 5. 儲存 metadata (方便之後根據檢索結果找回原始資料)\n",
    "    save_metadata(meta, \"faiss_index_public_metadata.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02870f0",
   "metadata": {},
   "source": [
    "第二版本:ScaNN <-先跑這版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7462ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scann\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 1. 載入 JSON 資料\n",
    "def load_jsonl(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# 2. 資料清洗規則\n",
    "EXCLUDED_HEADERS = [\n",
    "    \"abstract\", \"references\", \"acknowledgement\", \"acknowledgments\", \n",
    "    \"appendix\", \"author contributions\", \"supplementary materials\"\n",
    "]\n",
    "\n",
    "def clean_paragraph(p):\n",
    "    p = p.strip()\n",
    "    p = re.sub(r'\\s+', ' ', p)\n",
    "    return p\n",
    "\n",
    "def is_informative(p, min_len=30):\n",
    "    if not p or len(p) < min_len:\n",
    "        return False\n",
    "    lower_p = p.lower()\n",
    "    return not any(lower_p.startswith(h) for h in EXCLUDED_HEADERS)\n",
    "\n",
    "# 3. 分段+清洗\n",
    "def split_into_sentences(text):\n",
    "    sentences = re.split(r'(?<=[。！？!?\\.])\\s+', text.strip())\n",
    "    return [s.strip() for s in sentences if len(s.strip()) > 10]\n",
    "\n",
    "def sentence_overlap_chunking(sentences, window_size=5, stride=3):\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), stride):\n",
    "        window = sentences[i:i+window_size]\n",
    "        if window:\n",
    "            chunk = \" \".join(window).strip()\n",
    "            if len(chunk) > 30:\n",
    "                chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# 分段技巧\n",
    "def chunk_papers(data, section_splitter=True, use_overlap=True, window_size=5, stride=3):\n",
    "    all_chunks = []\n",
    "    metadata = []\n",
    "\n",
    "    for item in tqdm(data, desc=\"分段&結構化清洗\"):\n",
    "        title = item[\"title\"]\n",
    "        raw_sections = item[\"full_text\"].split(\"\\n\\n\\n\")\n",
    "\n",
    "        for section_id, raw_section in enumerate(raw_sections):\n",
    "            # === 第二層切 ::: 子章節\n",
    "            if section_splitter and \" ::: \" in raw_section:\n",
    "                sub_sections = raw_section.split(\" ::: \")\n",
    "                first_header, *rest = sub_sections\n",
    "                sub_chunks = [first_header.strip()] + [s.strip() for s in rest]\n",
    "            else:\n",
    "                sub_chunks = [raw_section.strip()]\n",
    "\n",
    "            for chunk_id, chunk in enumerate(sub_chunks):\n",
    "                clean_chunk = clean_paragraph(chunk)\n",
    "\n",
    "                if not is_informative(clean_chunk):\n",
    "                    continue\n",
    "\n",
    "                if use_overlap:\n",
    "                    # 句子重疊切分\n",
    "                    sentences = split_into_sentences(clean_chunk)\n",
    "                    overlapped = sentence_overlap_chunking(sentences, window_size, stride)\n",
    "                    for i, o_chunk in enumerate(overlapped):\n",
    "                        all_chunks.append(o_chunk)\n",
    "                        metadata.append({\n",
    "                            \"title\": title,\n",
    "                            \"section_id\": section_id,\n",
    "                            \"chunk_id\": f\"{chunk_id}_{i}\",\n",
    "                            \"section_name\": o_chunk.split(\" \")[0][:50]\n",
    "                        })\n",
    "                else:\n",
    "                    all_chunks.append(clean_chunk)\n",
    "                    metadata.append({\n",
    "                        \"title\": title,\n",
    "                        \"section_id\": section_id,\n",
    "                        \"chunk_id\": chunk_id,\n",
    "                        \"section_name\": clean_chunk.split(\" \")[0][:50]\n",
    "                    })\n",
    "\n",
    "    return all_chunks, metadata\n",
    "\n",
    "# 4. 編碼\n",
    "def encode_chunks(chunks, model_name=\"BAAI/bge-large-en-v1.5\", batch_size=32):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(chunks), batch_size), desc=\"向量化\"):\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        embs = model.encode(batch, show_progress_bar=False, normalize_embeddings=True)\n",
    "        embeddings.append(embs)\n",
    "    embeddings = np.vstack(embeddings).astype('float32')\n",
    "    return embeddings\n",
    "\n",
    "# 5. 建立 ScaNN 向量索引\n",
    "def build_scann(embeddings, save_path=\"scann_index\", num_leaves=150, num_leaves_to_search=60, reordering_k=100):\n",
    "    # 建立 ScaNN index\n",
    "    searcher = scann.scann_ops_pybind.builder(embeddings, 10, \"dot_product\") \\\n",
    "        .tree(num_leaves=num_leaves, num_leaves_to_search=num_leaves_to_search) \\\n",
    "        .score_ah(2, anisotropic_quantization_threshold=0.2) \\\n",
    "        .reorder(reordering_k).build()\n",
    "    \n",
    "    print(f\"ScaNN index 建立完成\")\n",
    "    return searcher\n",
    "\n",
    "def save_metadata(metadata, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"metadata 儲存至: {path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 載入資料\n",
    "    data = load_jsonl(\"datasets/public_dataset.json\")\n",
    "    \n",
    "    # 2. 分段與清洗\n",
    "    chunks, meta = chunk_papers(data)\n",
    "    print(f\"共保留有效段落數: {len(chunks)}\")\n",
    "    \n",
    "    with open(\"chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 3. 嵌入向量\n",
    "    emb = encode_chunks(chunks, model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "    \n",
    "    # 4. 建立 ScaNN 向量索引\n",
    "    np.save(\"scann_embeddings.npy\", emb)\n",
    "    searcher = build_scann(emb, save_path=\"scann_index\")\n",
    "    \n",
    "    # 5. 儲存 metadata \n",
    "    save_metadata(meta, \"scann_index_metadata.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab50d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_scann(query, chunks, metadata, searcher, encoder, top_k):\n",
    "    # 將 query 轉為向\n",
    "    query_vec = encoder.encode([query], normalize_embeddings=True).astype('float32')\n",
    "    \n",
    "    # 查詢 top-k 相似向量\n",
    "    neighbors, scores = searcher.search_batched(query_vec)\n",
    "    \n",
    "    results = []\n",
    "    for idx, score in zip(neighbors[0], scores[0]):\n",
    "        results.append({\n",
    "            \"score\": float(score),\n",
    "            \"index\": int(idx),\n",
    "            \"chunk\": chunks[idx],           # 原始段落\n",
    "            \"metadata\": metadata[idx]       # 該段落的來源資訊（title、section等）\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a69d95",
   "metadata": {},
   "source": [
    "# Step 2：建構向量檢索器（Retriever）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfeffd9",
   "metadata": {},
   "source": [
    "faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82f32b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class VectorRetriever:\n",
    "    def __init__(self, index_path, metadata_path, chunk_path=None,\n",
    "                 model_name=\"BAAI/bge-large-en-v1.5\"):\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        # 載入 metadata\n",
    "        with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.metadata = json.load(f)\n",
    "        # 載入原始 chunks（可選）\n",
    "        if chunk_path:\n",
    "            with open(chunk_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                self.chunks = json.load(f)\n",
    "        else:\n",
    "            self.chunks = None\n",
    "            \n",
    "        self.encoder = SentenceTransformer(model_name)\n",
    "\n",
    "    def retrieve(self, query, top_k=5):\n",
    "        # 編碼 query\n",
    "        q_vec = self.encoder.encode([query], normalize_embeddings=True).astype('float32')\n",
    "        # 搜尋 top-k 相似向量\n",
    "        scores, indices = self.index.search(q_vec, top_k)\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            meta = self.metadata[idx]\n",
    "            content = self.chunks[idx] if self.chunks else None\n",
    "            results.append({\n",
    "                \"score\": float(score),\n",
    "                \"index\": int(idx),\n",
    "                \"metadata\": meta,\n",
    "                \"content\": content\n",
    "            })\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaa0c7a",
   "metadata": {},
   "source": [
    "ScaNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88492c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scann\n",
    "\n",
    "class ScaNNRetriever:\n",
    "    def __init__(self, embedding_path, metadata_path, chunk_path=None,\n",
    "                 model_name=\"BAAI/bge-large-en-v1.5\"):\n",
    "        self.embeddings = np.load(embedding_path)\n",
    "        \n",
    "        with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.metadata = json.load(f)\n",
    "        \n",
    "        if chunk_path:\n",
    "            with open(chunk_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                self.chunks = json.load(f)\n",
    "        else:\n",
    "            self.chunks = None\n",
    "\n",
    "        # 建立 ScaNN index\n",
    "        self.searcher = scann.scann_ops_pybind.builder(self.embeddings, 10, \"dot_product\") \\\n",
    "            .tree(num_leaves=150, num_leaves_to_search=60) \\\n",
    "            .score_ah(2, anisotropic_quantization_threshold=0.2) \\\n",
    "            .reorder(100).build()\n",
    "\n",
    "        self.encoder = SentenceTransformer(model_name)\n",
    "\n",
    "    def retrieve(self, query, top_k=5):\n",
    "        q_vec = self.encoder.encode([query], normalize_embeddings=True).astype('float32')\n",
    "        neighbors, scores = self.searcher.search_batched(q_vec)\n",
    "\n",
    "        results = []\n",
    "        for idx, score in zip(neighbors[0], scores[0]):\n",
    "            meta = self.metadata[idx]\n",
    "            content = self.chunks[idx] if self.chunks else None\n",
    "            results.append({\n",
    "                \"score\": float(score),\n",
    "                \"index\": int(idx),\n",
    "                \"metadata\": meta,\n",
    "                \"content\": content\n",
    "            })\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083111ad",
   "metadata": {},
   "source": [
    "# Step 3：設計 Prompt 模板 & RAG 回答模組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60e782cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a research assistant. Answer the question strictly based on the provided context and any clues or evidence.\n",
    "Carefully analyze the context and extract only the most relevant information.\n",
    "Provide a concise, direct answer to the question in a single sentence. \n",
    "If the context provides no useful information, try your best to infer an answer from what you have.\n",
    "Only respond with \"I don't know\" if absolutely no relevant clues can be found in the context.\n",
    "Avoid explanations, introductory phrases, or unnecessary details. No extra steps or reasoning, just the final answer.\n",
    "Do NOT copy full sentences verbatim from the context.\n",
    "If the answer is too cryptic and unclear, just help me answer based on known clues to minimize the chances of answering “I don't know”.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer (no explanation):\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d6ebee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "bert_model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "def select_top_sentences(query, candidate_chunks, max_sentences):\n",
    "    all_sentences = []\n",
    "    for chunk in candidate_chunks:\n",
    "        all_sentences.extend(split_into_sentences(chunk))\n",
    "    query_emb = bert_model.encode(query, convert_to_tensor=True)\n",
    "    sent_embs = bert_model.encode(all_sentences, convert_to_tensor=True)\n",
    "    scores = util.pytorch_cos_sim(query_emb, sent_embs)[0]\n",
    "    top_idx = scores.argsort(descending=True)[:max_sentences]\n",
    "    return [all_sentences[i] for i in top_idx]\n",
    "\n",
    "\n",
    "def build_context(sentences, max_char=4500):\n",
    "    context = \"\"\n",
    "    for s in sentences:\n",
    "        if len(context) + len(s) + 1 > max_char:\n",
    "            break\n",
    "        context += s + \" \"\n",
    "    return context.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d77b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特例:計數題型\n",
    "def is_count_question(query):\n",
    "    q = query.lower()\n",
    "    return any(q.startswith(p) for p in [\"how many\", \"how much\", \"how long\", \"how often\"])\n",
    "\n",
    "def extract_numeric_evidence(sentences):\n",
    "    return [s for s in sentences if re.search(r\"\\b\\d{1,6}\\b\", s)]\n",
    "\n",
    "def extract_best_count_answer(sentences, query=None):\n",
    "    \"\"\"\n",
    "    根據句子中的數字與名詞組合，選出最可能是 count 答案的句子。\n",
    "    \"\"\"\n",
    "    keyword_whitelist = {\"articles\", \"documents\", \"samples\", \"questions\", \"entries\", \"sentences\", \"reviews\", \"papers\"}\n",
    "    query_keywords = set(re.findall(r'\\b\\w+\\b', query.lower())) if query else set()\n",
    "\n",
    "    scored = []\n",
    "    for s in sentences:\n",
    "        s_lower = s.lower()\n",
    "        nums = re.findall(r\"\\b\\d{1,6}\\b\", s_lower)\n",
    "        if not nums:\n",
    "            continue\n",
    "\n",
    "        match_score = 0\n",
    "        for word in keyword_whitelist:\n",
    "            if word in s_lower:\n",
    "                match_score += 2\n",
    "                if word in query_keywords:\n",
    "                    match_score += 3  \n",
    "\n",
    "        # 額外根據數字個數加小分（例如句子中有兩個數字，可能更豐富）\n",
    "        match_score += len(nums)\n",
    "\n",
    "        scored.append((match_score, s.strip()))\n",
    "\n",
    "    if scored:\n",
    "        scored.sort(reverse=True)  # 根據分數排序\n",
    "        return scored[0][1]  # 回傳分數最高的句子\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e719b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"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\",\n",
    "    base_url=\"https://api.groq.com/openai/v1\"\n",
    ")\n",
    "\n",
    "def query_llm(prompt, model=\"gemma2-9b-it\"):\n",
    "    response = client.chat.completions.create(  \n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=600,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def extract_list_items(text):\n",
    "    if isinstance(text, list):\n",
    "        return [s.strip().rstrip(\".\") for s in text if isinstance(s, str)]\n",
    "\n",
    "    text = text.strip()\n",
    "    items = []\n",
    "\n",
    "    lines = re.split(r\"[•\\-\\n]\", text)  # 支援條列符號與換行\n",
    "    for line in lines:\n",
    "        parts = re.split(r\",|，|、| and | or |;\", line)  # 更靈活的分割器\n",
    "        for part in parts:\n",
    "            clean = part.strip(\" -•0123456789.\\\"\\n\").rstrip(\".\")\n",
    "            if len(clean.split()) <= 8 and len(clean) >= 2:\n",
    "                items.append(clean)\n",
    "\n",
    "    items = list(dict.fromkeys(items))\n",
    "\n",
    "    if not items:\n",
    "        return [text]\n",
    "    return items\n",
    "\n",
    "def extract_final_sentence(text):\n",
    "    # 找最後一句 answer\n",
    "    lines = text.strip().splitlines()\n",
    "    candidates = [l for l in lines if l.strip() and not l.lower().startswith(\"step\")]\n",
    "    \n",
    "    # 取最後一個合理句子\n",
    "    for line in reversed(candidates):\n",
    "        if len(line.strip().split()) > 4:\n",
    "            return re.sub(r'^[Aa]nswer\\s*:\\s*', '', line.strip())\n",
    "    \n",
    "    # fallback 萃取不到就整段丟回\n",
    "    return text.strip()\n",
    "\n",
    "def answer_question(query, chunks, metadata, index, encoder, model=\"gemma2-9b-it\", title=None, top_k=40):\n",
    "    q_vec = encoder.encode([query], normalize_embeddings=True).astype('float32')\n",
    "    scores, indices = index.search(q_vec, top_k)\n",
    "\n",
    "    candidate_chunks = [chunks[i] for i in indices[0]]\n",
    "\n",
    "    best_sentences = select_top_sentences(query, candidate_chunks, max_sentences=18)\n",
    "    \n",
    "    if is_count_question(query):\n",
    "        numeric_evidence = extract_numeric_evidence(best_sentences)\n",
    "        if numeric_evidence:\n",
    "            count_answer = extract_best_count_answer(numeric_evidence)\n",
    "            if count_answer:\n",
    "                return {\n",
    "                    \"title\": title,\n",
    "                    \"answer\": count_answer,\n",
    "                    \"evidence\": numeric_evidence\n",
    "                }\n",
    "\n",
    "    # === 6. 去除重複 & 太短的句子\n",
    "    seen = set()\n",
    "    unique_evidence = []\n",
    "    for s in best_sentences:\n",
    "        if s not in seen and len(s.split()) > 4:\n",
    "            seen.add(s)\n",
    "            unique_evidence.append(s)\n",
    "\n",
    "    # === 7. 組 context 並生成 prompt\n",
    "    context = build_context(unique_evidence, max_char=5000)\n",
    "    prompt = PROMPT_TEMPLATE.format(context=context, question=query)\n",
    "\n",
    "    # === 8. 使用 Groq 的 LLM 進行生成\n",
    "    reasoning_output = query_llm(prompt, model=model)\n",
    "    final_answer = extract_final_sentence(reasoning_output)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"answer\": final_answer,\n",
    "        \"evidence\": unique_evidence\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0a7b66",
   "metadata": {},
   "source": [
    "# Step 4：處理 QA 任務"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294c5eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "處理 QA 問題: 100%|██████████| 100/100 [04:01<00:00,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "處理完成，結果已儲存於 sample_submission_public.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "chunks = json.load(open(\"chunks.json\"))\n",
    "metadata = json.load(open(\"faiss_index_public_metadata.json\"))\n",
    "index = faiss.read_index(\"faiss_index_public.bin\")\n",
    "encoder = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "private_data = json.load(open(\"datasets/private_dataset.json\"))\n",
    "outputs = []\n",
    "\n",
    "for item in tqdm(private_data, desc=\"處理 QA 問題\"):\n",
    "    title = item[\"title\"]\n",
    "    query = item[\"question\"]\n",
    "\n",
    "    result = answer_question(\n",
    "        query=query,\n",
    "        chunks=chunks,\n",
    "        metadata=metadata,\n",
    "        index=index,\n",
    "        encoder=encoder,\n",
    "        model=\"gemma2-9b-it\",\n",
    "        title=title\n",
    "    )\n",
    "\n",
    "    outputs.append(result)\n",
    "\n",
    "with open(\"sample_submission_private.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(outputs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"處理完成，結果已儲存於 sample_submission_private.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7059f76",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc154667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_evidence_score = 0.1438\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "fmeasure_scores = []\n",
    "\n",
    "result = outputs[demo_id]\n",
    "unique_evidence = result[\"evidence\"]\n",
    "\n",
    "for chunk in unique_evidence:\n",
    "  scores = scorer.score_multi(  # using maximum f-measure\n",
    "    targets=demo_evidence,\n",
    "    prediction=chunk\n",
    "    )\n",
    "\n",
    "  fmeasure_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "final_evidence_score = sum(fmeasure_scores) / len(fmeasure_scores)\n",
    "print(f\"{final_evidence_score = :.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
