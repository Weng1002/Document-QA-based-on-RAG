{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "205bce2c",
   "metadata": {},
   "source": [
    "# 第一版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50da5e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# CoT 提示\n",
    "cot_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a research assistant. Answer the question strictly based on the provided context.\n",
    "Carefully analyze the context and extract only the most relevant information.\n",
    "Provide a concise, direct answer to the question in a single sentence. \n",
    "If the context gives any clues or evidence, formulate an answer based on those.\n",
    "If the context provides no useful information, try your best to infer an answer from what you have.\n",
    "Only respond with \"I don't know\" if absolutely no relevant clues can be found in the context.\n",
    "Avoid explanations, introductory phrases, or unnecessary details. No extra steps or reasoning, just the final answer.\n",
    "If a part of the context seems irrelevant to the question, do not include it in your answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # 移除 HTML 標籤\n",
    "    text = re.sub(r'BIBREF\\d+', '', text)  # 移除參考文獻標記\n",
    "    text = re.sub(r'INLINEFORM\\d+', '', text)  # 移除公式標記\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())  # 規範空白\n",
    "    return text\n",
    "\n",
    "def normalize_newlines(text):\n",
    "    return re.sub(r'\\n{2,}', '<SECTION>', text)\n",
    "\n",
    "# 按論文結構分割\n",
    "def split_by_sections(text):\n",
    "    sections = re.split(\n",
    "        r'<SECTION>|(?=Abstract\\n|Introduction\\n|Related Work\\n|Background\\n|Data\\n|Approach\\n|Methodology\\n|Evaluation\\n|Experiments\\n|Conclusion\\n|Acknowledgements\\n|(?:\\w+\\s*:::\\s*.+\\n))',\n",
    "        text\n",
    "    )\n",
    "    return [s.strip() for s in sections if s.strip()]\n",
    "\n",
    "\n",
    "def validate_answer(data):\n",
    "    issues = []\n",
    "    for item in data:\n",
    "        full_text = clean_text(item['full_text'])  # 清理 full_text\n",
    "        answers = item['answer']\n",
    "        evidence = item['evidence']\n",
    "        \n",
    "        # 檢查答案\n",
    "        for ans in answers:\n",
    "            cleaned_ans = clean_text(ans)\n",
    "            if cleaned_ans not in full_text and cleaned_ans.lower() not in full_text.lower():\n",
    "                issues.append(f\"Answer '{ans}' not found in full_text for title: {item['title']}\")\n",
    "        \n",
    "        # 檢查證據\n",
    "        for ev in evidence:\n",
    "            cleaned_ev = clean_text(ev)\n",
    "            ev_words = set(cleaned_ev.lower().split())\n",
    "            full_text_words = set(full_text.lower().split())\n",
    "            overlap = len(ev_words & full_text_words) / len(ev_words) if ev_words else 0\n",
    "            if overlap < 0.8:\n",
    "                issues.append(f\"Evidence '{ev}' not found or insufficient overlap ({overlap:.2f}) in full_text for title: {item['title']}\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "def create_documents(data):\n",
    "    all_docs = []\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "    for item in data:\n",
    "        cleaned_text = clean_text(item['full_text'])\n",
    "        normalized_text = normalize_newlines(cleaned_text)\n",
    "        sections = split_by_sections(normalized_text)\n",
    "        for section in sections:\n",
    "            chunks = text_splitter.split_text(section) if len(section) > 400 else [section]\n",
    "            for chunk in chunks:\n",
    "                doc = Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\n",
    "                        \"title\": item[\"title\"],\n",
    "                        \"question\": item[\"question\"],\n",
    "                        \"answer\": item[\"answer\"],\n",
    "                        \"evidence\": [clean_text(ev) for ev in item[\"evidence\"]],\n",
    "                        \"section\": section.split('\\n')[0] if section else \"Unknown\",\n",
    "                        \"dataset\": \"Airbnb\" if \"Airbnb\" in chunk else \"PrivacyQA\" if \"PrivacyQA\" in chunk else \"Unknown\"\n",
    "                    }\n",
    "                )\n",
    "                all_docs.append(doc)\n",
    "    return all_docs\n",
    "\n",
    "with open(\"datasets/public_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    public_data = json.load(f)\n",
    "\n",
    "# 驗證答案\n",
    "issues = validate_answer(public_data)\n",
    "if issues:\n",
    "    print(\"Found issues:\", issues)\n",
    "else:\n",
    "    print(\"All answers and evidence are valid.\")\n",
    "\n",
    "all_docs = create_documents(public_data)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
    ")\n",
    "\n",
    "faiss_db = FAISS.from_documents(all_docs, embedding)\n",
    "faiss_db.save_local(\"faiss_index\")\n",
    "\n",
    "retriever = faiss_db.as_retriever(\n",
    "    search_type=\"mmr\",  #knn\n",
    "    search_kwargs={\"k\": 15, \"fetch_k\": 40}\n",
    ")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=\"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\",\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "# 建立 RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": cot_prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "with open(\"datasets/private_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    private_data = json.load(f)\n",
    "\n",
    "\n",
    "submission_results = []\n",
    "\n",
    "def extract_final_answer(cot_output):\n",
    "    if \"Final Answer:\" in cot_output:\n",
    "        match = re.search(r\"Final Answer:\\s*(.*?)(?:\\n|$)\", cot_output, re.DOTALL)\n",
    "        return match.group(1).strip() if match else \"I don't know\"\n",
    "    lines = cot_output.strip().split(\"\\n\")\n",
    "    return lines[-1].strip() if lines else \"I don't know\"\n",
    "\n",
    "\n",
    "def is_how_many_question(q):\n",
    "    return q.lower().strip().startswith(\"how many\")\n",
    "\n",
    "for item in tqdm(private_data, desc=\"Processing questions\"):\n",
    "    question = item[\"question\"]\n",
    "    title = item[\"title\"]\n",
    "\n",
    "    if is_how_many_question(question):\n",
    "        # 處理 'How many' 類問題，返回相關的數字答案\n",
    "        answer_text = f\"There are {len(public_data)} articles in the dataset.\"\n",
    "        qa_result = qa_chain.invoke({\"query\": question})\n",
    "        evidence_list = [doc.page_content for doc in qa_result.get(\"source_documents\", [])]\n",
    "    else:\n",
    "        qa_result = qa_chain.invoke({\"query\": question})\n",
    "        cot_output = qa_result.get(\"answer\") or qa_result.get(\"result\") or \"I don't know\"\n",
    "        answer_text = extract_final_answer(cot_output)\n",
    "        evidence_list = [doc.page_content for doc in qa_result.get(\"source_documents\", [])]\n",
    "\n",
    "    submission_results.append({\n",
    "        \"title\": title,\n",
    "        \"answer\": answer_text,\n",
    "        \"evidence\": evidence_list\n",
    "    })\n",
    "\n",
    "with open(\"sample_submission_llama.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(submission_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"RAG流程完成，結果已寫入 sample_submission_llama.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3652a898",
   "metadata": {},
   "source": [
    "# 第二版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352865db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.logging import RichHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b616a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console(stderr=True, record=True)\n",
    "log_handler = RichHandler(rich_tracebacks=True, console=console, markup=True)\n",
    "logging.basicConfig(format=\"%(message)s\",datefmt=\"[%X]\",handlers=[log_handler])\n",
    "log = logging.getLogger(\"rich\")\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "DATASET_PATH = \"datasets/public_dataset.json\"\n",
    "RETRIEVE_TOP_K = 12\n",
    "CHUNK_SIZE = 400\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "with open(DATASET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381f0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    return re.sub(r\"(INLINEFORM\\d+|DISPLAYFORM\\d+|SECREF\\d+|TABREF\\d+|UID\\d+)\", \"\", text)\n",
    "\n",
    "# 對 full_text 清洗\n",
    "for data in dataset:\n",
    "    data[\"full_text\"] = clean_text(data[\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92723873",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "all_docs = []\n",
    "for data in dataset:\n",
    "    full_text = data[\"full_text\"]\n",
    "    chunks = full_text.split(\"\\n\\n\\n\")[:-1]  # 避免最後一個空白段落\n",
    "    docs = [Document(page_content=doc) for doc in chunks]\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    all_docs.extend(splits)\n",
    "\n",
    "vector_store = FAISS.from_documents(all_docs, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e742e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "  model=\"gemma2-9b-it\",\n",
    "  api_key=\"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\",\n",
    "  temperature=0.4,\n",
    "  max_tokens=512,\n",
    ")\n",
    "\n",
    "CHAT_TEMPLATE_RAG = (\n",
    "    \"\"\"human: You are an expert academic assistant. Carefully read the provided context and follow these steps:\n",
    "1. Identify the key concepts and reasoning required to answer the question.\n",
    "2. Synthesize a concise answer using your own words.\n",
    "3. Do not copy sentences directly from the context.\n",
    "\n",
    "context:\n",
    "{context}\n",
    "\n",
    "question:\n",
    "{input}\n",
    "\n",
    "assistant:\"\"\"\n",
    ")\n",
    "\n",
    "retrieval_qa_prompt = PromptTemplate.from_template(template=CHAT_TEMPLATE_RAG)\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_prompt)\n",
    "\n",
    "rag_qa_chain = create_retrieval_chain(\n",
    "    retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",  # 也可以改 \"mmr\"\n",
    "    search_kwargs={\"k\": RETRIEVE_TOP_K}\n",
    "),\n",
    "    combine_docs_chain=combine_docs_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3402beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, item in tqdm(enumerate(dataset), total=len(dataset),desc=\"QA 生成中...\"):\n",
    "    question = item[\"question\"]\n",
    "    title = item[\"title\"]\n",
    "\n",
    "    response = rag_qa_chain.invoke({\"input\": question})\n",
    "    \n",
    "    answer = response.get(\"answer\", \"\")\n",
    "    evidence_chunks = [doc.page_content for doc in response.get(\"context\", [])]\n",
    "\n",
    "    results.append({\n",
    "        \"title\": title,\n",
    "        \"answer\": answer,\n",
    "        \"evidence\": evidence_chunks,\n",
    "    })\n",
    "\n",
    "with open(\"sample_submission_public.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    print(\"Results saved to sample_submission_public.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a98b10",
   "metadata": {},
   "source": [
    "# 第三版本(最終版)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c72d2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1004a6e",
   "metadata": {},
   "source": [
    "=== 初始化 Embedding 模型 ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1413fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_PATH = \"datasets/public_dataset.json\"\n",
    "# OUTPUT_PATH = \"sample_submission_public.json\"\n",
    "\n",
    "DATASET_PATH = \"datasets/private_dataset.json\"\n",
    "OUTPUT_PATH = \"sample_submission_private.json\"\n",
    "RETRIEVE_TOP_K = 30\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name=\"intfloat/e5-large-v2\")\n",
    "llm = ChatGroq(\n",
    "    model=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "    api_key=\"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\",   \n",
    "    temperature=0.4,\n",
    "    max_tokens=256\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99da541e",
   "metadata": {},
   "source": [
    "=== 定義函數 ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0959c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    # 1. 移除顯示用標記\n",
    "    text = re.sub(r\"(INLINEFORM\\d+|DISPLAYFORM\\d+|SECREF\\d+|TABREF\\d+|UID\\d+|FIGREF\\d+)\", \"\", text)\n",
    "    \n",
    "    # 2. 移除 ::: 標記或章節分隔線\n",
    "    text = re.sub(r\"\\s*:::+\\s*\", \"\\n\", text)\n",
    "    \n",
    "    # 3. 移除參考文獻標題\n",
    "    text = re.sub(r\"(?i)\\nreferences\\n.*\", \"\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"\\(Table \\d+\\)|\\(Figure \\d+\\)\", \"\", text)  # 移除表格/圖表引用\n",
    "    text = re.sub(r\"\\n\\d+\\s*$\", \"\", text, flags=re.MULTILINE)  # 移除頁碼\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "sent_embed_model = SentenceTransformer(\"BAAI/bge-reranker-large\")\n",
    "\n",
    "def rerank_sentences_by_similarity(question, chunks, top_n=20, min_word_count=2):\n",
    "    seen = set()\n",
    "    sentences = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        for s in re.split(r'(?<=[.。!?])\\s+', chunk):\n",
    "            s = s.strip()\n",
    "            word_count = len(s.split())\n",
    "            \n",
    "            if word_count >= min_word_count and s not in seen:\n",
    "                seen.add(s)\n",
    "                sentences.append(s)\n",
    "\n",
    "    # 計算語意相似度分數\n",
    "    query_embedding = sent_embed_model.encode(question, convert_to_tensor=True)\n",
    "    scored = [\n",
    "        (s, util.pytorch_cos_sim(query_embedding, sent_embed_model.encode(s, convert_to_tensor=True)).item())\n",
    "        for s in sentences\n",
    "    ]\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [s[0] for s in scored[:top_n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8fa7d5",
   "metadata": {},
   "source": [
    "=== Prompt 模板 ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4bfc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_TEMPLATE_RAG = (\n",
    "    \"\"\"human: You are an academic QA assistant. Use the context to answer precisely.\n",
    "Please think about the question step by step, and then answer a ***concise***, precise answer based on the context and evidence.\n",
    "Please try to find the right keywords to answer the question based on the evidence or context you find.\n",
    "If the answer is a name, number, or keyword, extract it directly.\n",
    "Avoid vague or overly broad answers. Answer in a concise phrase.\n",
    "Format your answer similarly to human-written academic answers from datasets like SQuAD or CoQA.\n",
    "\n",
    "Context:  \n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{input}\n",
    "\n",
    "assistant:\"\"\"\n",
    ")\n",
    "\n",
    "retrieval_qa_prompt = PromptTemplate.from_template(template=CHAT_TEMPLATE_RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff27d38",
   "metadata": {},
   "source": [
    "=== Evidence Confidence 模板 ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738a5bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 信心判斷 prompt：請根據 evidence 判斷是否足以回答問題\n",
    "CONFIDENCE_PROMPT = PromptTemplate.from_template(\"\"\"\n",
    "You are a QA validation model. Based on the following retrieved context and question, judge if the context provides enough information to confidently answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Respond with only \"YES\" or \"NO\".\n",
    "\"\"\")\n",
    "confidence_chain = LLMChain(llm=llm, prompt=CONFIDENCE_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3270b058",
   "metadata": {},
   "source": [
    "=== 主迴圈 ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b912755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 載入資料集 ===\n",
    "with open(DATASET_PATH, \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "sample_submission = []\n",
    "\n",
    "# === 迴圈處理每一題 ===\n",
    "for demo_id, item in enumerate(tqdm(dataset, desc=\"QA 回答中...\")):\n",
    "    title = item[\"title\"]\n",
    "    full_text = clean_text(item[\"full_text\"])\n",
    "    question = item[\"question\"]\n",
    "\n",
    "    # 拆分文件段落\n",
    "    documents = full_text.split(\"\\n\\n\\n\")\n",
    "    docs = [Document(page_content=doc) for doc in documents]\n",
    "\n",
    "    # 切 chunk\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=320,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    docs_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    vector_store = FAISS.from_documents(docs_splits, embedding_model)\n",
    "\n",
    "    # === 動態 retrieval（逐步 k 增加）===\n",
    "    retrieved_chunks = []\n",
    "    max_k = RETRIEVE_TOP_K\n",
    "\n",
    "    for k in range(1, max_k + 1):\n",
    "        retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "        topk_docs = retriever.get_relevant_documents(question)\n",
    "        retrieved_chunks = [doc.page_content for doc in topk_docs]\n",
    "        reranked_sentences = rerank_sentences_by_similarity(question, retrieved_chunks, top_n=20)    \n",
    "        context_text = \"\\n\".join(reranked_sentences)\n",
    "\n",
    "        # LLM 判斷是否足夠\n",
    "        judge_result = confidence_chain.run({\"context\": context_text, \"question\": question}).strip().upper()\n",
    "        if \"YES\" in judge_result:\n",
    "            break  \n",
    "\n",
    "    # === 啟用 RAG QA chain ===\n",
    "    combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_prompt)\n",
    "    rag_qa_chain = create_retrieval_chain(\n",
    "        retriever=retriever,\n",
    "        combine_docs_chain=combine_docs_chain\n",
    "    )\n",
    "    response = rag_qa_chain.invoke({\"input\": question})\n",
    "    predicted_answer = response[\"answer\"].strip()\n",
    "    \n",
    "    # 只保留不是單字的句子（整段當作一句處理）\n",
    "    predicted_evidence = []\n",
    "    for doc in response[\"context\"]:\n",
    "        s = doc.page_content.strip()\n",
    "        if len(s.split()) >= 2:\n",
    "            predicted_evidence.append(s)\n",
    "    \n",
    "    sample_submission.append({\n",
    "        \"title\": title,\n",
    "        \"answer\": predicted_answer,\n",
    "        \"evidence\": predicted_evidence\n",
    "    })\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sample_submission, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n成功儲存：{OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e062d3c7",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34afd030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "def compute_evidence_rouge(gt_evidence, predicted_chunks):\n",
    "    \"\"\"計算一筆資料中 retrieved evidence 和 ground truth 之間的 ROUGE-L 平均 f1 分數\"\"\"\n",
    "    if not predicted_chunks or not gt_evidence:\n",
    "        return 0.0\n",
    "\n",
    "    f_scores = []\n",
    "    for pred in predicted_chunks:\n",
    "        scores = scorer.score_multi(\n",
    "            targets=gt_evidence,\n",
    "            prediction=pred,\n",
    "        )\n",
    "        f_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "    \n",
    "    return sum(f_scores) / len(f_scores)\n",
    "\n",
    "# 在迴圈結束後，整體計算所有筆數的 evidence score\n",
    "total_score = 0.0\n",
    "valid_count = 0\n",
    "\n",
    "for i, item in enumerate(dataset):\n",
    "    gt_evidence = item[\"evidence\"]  # 標準答案中的 evidence sentences\n",
    "    pred_evidence = sample_submission[i][\"evidence\"]  # 模型取出的句子\n",
    "\n",
    "    if pred_evidence and gt_evidence:\n",
    "        score = compute_evidence_rouge(gt_evidence, pred_evidence)\n",
    "        total_score += score\n",
    "        valid_count += 1\n",
    "\n",
    "average_score = total_score / valid_count if valid_count > 0 else 0.0\n",
    "print(f\"[Total]: {valid_count} samples with valid evidence\")\n",
    "print(f\"[Average ROUGE-L Evidence F1]: {average_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
