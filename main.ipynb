{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014bc508",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas==2.2.3 \n",
    "!pip install jupyter==1.1.1 \n",
    "!pip install langchain==0.3.23 \n",
    "!pip install langchain-community==0.3.21 \n",
    "!pip install rich==14.0.0 \n",
    "!pip install openai==1.71.0 \n",
    "!pip install langchain-groq==0.3.2 \n",
    "!pip install langchain-ollama==0.3.1 \n",
    "!pip install faiss-gpu==1.7.2 \n",
    "!pip install \"numpy<2\"\n",
    "!pip install rouge-score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb09fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6882e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.logging import RichHandler\n",
    "\n",
    "console = Console(stderr=True, record=True)\n",
    "log_handler = RichHandler(rich_tracebacks=True, console=console, markup=True)\n",
    "logging.basicConfig(format=\"%(message)s\",datefmt=\"[%X]\",handlers=[log_handler])\n",
    "log = logging.getLogger(\"rich\")\n",
    "# log.setLevel(logging.INFO)\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "DEBUG: bool = False\n",
    "DATASET_PATH: str = \"../datasets/public_dataset.json\"\n",
    "\n",
    "USING_MODEL: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "USING_PORT: int = 8092\n",
    "API_ENDPOINT: str = f\"http://192.168.0.7:{USING_PORT}/v1\"\n",
    "API_KEY: str = \"abc\"\n",
    "\n",
    "MODEL_TEMPERATURE: float = 0.3\n",
    "MODEL_MAX_TOKENS: int = 128\n",
    "RETRIEVE_TOP_K: int = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f06f73",
   "metadata": {},
   "source": [
    "Groq API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4ec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c0625",
   "metadata": {},
   "outputs": [],
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b753a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38ceae1",
   "metadata": {},
   "source": [
    "# 改良版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4e2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 優化 CoT 提示，要求簡潔答案\n",
    "cot_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a research assistant. Answer the question strictly based on the provided context.\n",
    "Carefully analyze the context and extract only the most relevant information.\n",
    "Provide a concise, direct answer to the question in a single sentence. \n",
    "If the context gives any clues or evidence, formulate an answer based on those.\n",
    "If the context provides no useful information, try your best to infer an answer from what you have.\n",
    "Only respond with \"I don't know\" if absolutely no relevant clues can be found in the context.\n",
    "Avoid explanations, introductory phrases, or unnecessary details. No extra steps or reasoning, just the final answer.\n",
    "If a part of the context seems irrelevant to the question, do not include it in your answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # 移除 HTML 標籤\n",
    "    text = re.sub(r'BIBREF\\d+', '', text)  # 移除參考文獻標記\n",
    "    text = re.sub(r'INLINEFORM\\d+', '', text)  # 移除公式標記\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())  # 規範空白\n",
    "    return text\n",
    "\n",
    "# 規範換行符\n",
    "def normalize_newlines(text):\n",
    "    return re.sub(r'\\n{2,}', '<SECTION>', text)\n",
    "\n",
    "# 按論文結構分割\n",
    "def split_by_sections(text):\n",
    "    sections = re.split(\n",
    "        r'<SECTION>|(?=Abstract\\n|Introduction\\n|Related Work\\n|Background\\n|Data\\n|Approach\\n|Methodology\\n|Evaluation\\n|Experiments\\n|Conclusion\\n|Acknowledgements\\n|(?:\\w+\\s*:::\\s*.+\\n))',\n",
    "        text\n",
    "    )\n",
    "    return [s.strip() for s in sections if s.strip()]\n",
    "\n",
    "\n",
    "def validate_answer(data):\n",
    "    issues = []\n",
    "    for item in data:\n",
    "        full_text = clean_text(item['full_text'])  # 清理 full_text\n",
    "        answers = item['answer']\n",
    "        evidence = item['evidence']\n",
    "        \n",
    "        # 檢查答案\n",
    "        for ans in answers:\n",
    "            cleaned_ans = clean_text(ans)\n",
    "            if cleaned_ans not in full_text and cleaned_ans.lower() not in full_text.lower():\n",
    "                issues.append(f\"Answer '{ans}' not found in full_text for title: {item['title']}\")\n",
    "        \n",
    "        # 檢查證據\n",
    "        for ev in evidence:\n",
    "            cleaned_ev = clean_text(ev)\n",
    "            ev_words = set(cleaned_ev.lower().split())\n",
    "            full_text_words = set(full_text.lower().split())\n",
    "            overlap = len(ev_words & full_text_words) / len(ev_words) if ev_words else 0\n",
    "            if overlap < 0.8:\n",
    "                issues.append(f\"Evidence '{ev}' not found or insufficient overlap ({overlap:.2f}) in full_text for title: {item['title']}\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "\n",
    "# 創建文檔\n",
    "def create_documents(data):\n",
    "    all_docs = []\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "    for item in data:\n",
    "        cleaned_text = clean_text(item['full_text'])\n",
    "        normalized_text = normalize_newlines(cleaned_text)\n",
    "        sections = split_by_sections(normalized_text)\n",
    "        for section in sections:\n",
    "            chunks = text_splitter.split_text(section) if len(section) > 400 else [section]\n",
    "            for chunk in chunks:\n",
    "                doc = Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\n",
    "                        \"title\": item[\"title\"],\n",
    "                        \"question\": item[\"question\"],\n",
    "                        \"answer\": item[\"answer\"],\n",
    "                        \"evidence\": [clean_text(ev) for ev in item[\"evidence\"]],\n",
    "                        \"section\": section.split('\\n')[0] if section else \"Unknown\",\n",
    "                        \"dataset\": \"Airbnb\" if \"Airbnb\" in chunk else \"PrivacyQA\" if \"PrivacyQA\" in chunk else \"Unknown\"\n",
    "                    }\n",
    "                )\n",
    "                all_docs.append(doc)\n",
    "    return all_docs\n",
    "\n",
    "# 載入數據\n",
    "with open(\"datasets/public_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    public_data = json.load(f)\n",
    "\n",
    "# 驗證答案\n",
    "issues = validate_answer(public_data)\n",
    "if issues:\n",
    "    print(\"Found issues:\", issues)\n",
    "else:\n",
    "    print(\"All answers and evidence are valid.\")\n",
    "\n",
    "# 創建文檔\n",
    "all_docs = create_documents(public_data)\n",
    "\n",
    "# 初始化嵌入模型\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
    ")\n",
    "\n",
    "# 構建 FAISS 向量資料庫\n",
    "faiss_db = FAISS.from_documents(all_docs, embedding)\n",
    "faiss_db.save_local(\"faiss_index\")\n",
    "\n",
    "# 優化檢索器\n",
    "retriever = faiss_db.as_retriever(\n",
    "    search_type=\"mmr\",  #knn\n",
    "    search_kwargs={\"k\": 15, \"fetch_k\": 40}\n",
    ")\n",
    "\n",
    "# 準備 LLM\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=\"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\",\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "# 建立 RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": cot_prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 載入 private_dataset.json\n",
    "with open(\"datasets/private_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    private_data = json.load(f)\n",
    "\n",
    "# 4. 處理問題並生成答案\n",
    "submission_results = []\n",
    "\n",
    "def extract_final_answer(cot_output):\n",
    "    if \"Final Answer:\" in cot_output:\n",
    "        match = re.search(r\"Final Answer:\\s*(.*?)(?:\\n|$)\", cot_output, re.DOTALL)\n",
    "        return match.group(1).strip() if match else \"I don't know\"\n",
    "    lines = cot_output.strip().split(\"\\n\")\n",
    "    return lines[-1].strip() if lines else \"I don't know\"\n",
    "\n",
    "\n",
    "def is_how_many_question(q):\n",
    "    return q.lower().strip().startswith(\"how many\")\n",
    "\n",
    "for item in tqdm(private_data, desc=\"Processing questions\"):\n",
    "    question = item[\"question\"]\n",
    "    title = item[\"title\"]\n",
    "\n",
    "    if is_how_many_question(question):\n",
    "        # 處理 'How many' 類問題，返回相關的數字答案\n",
    "        answer_text = f\"There are {len(public_data)} articles in the dataset.\"\n",
    "        qa_result = qa_chain.invoke({\"query\": question})\n",
    "        evidence_list = [doc.page_content for doc in qa_result.get(\"source_documents\", [])]\n",
    "    else:\n",
    "        qa_result = qa_chain.invoke({\"query\": question})\n",
    "        cot_output = qa_result.get(\"answer\") or qa_result.get(\"result\") or \"I don't know\"\n",
    "        answer_text = extract_final_answer(cot_output)\n",
    "        evidence_list = [doc.page_content for doc in qa_result.get(\"source_documents\", [])]\n",
    "\n",
    "    submission_results.append({\n",
    "        \"title\": title,\n",
    "        \"answer\": answer_text,\n",
    "        \"evidence\": evidence_list\n",
    "    })\n",
    "\n",
    "# 儲存結果\n",
    "with open(\"sample_submission_llama.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(submission_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"RAG流程完成，結果已寫入 sample_submission_llama.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
