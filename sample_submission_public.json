[
  {
    "title": "Semi-supervised Thai Sentence Segmentation Using Local and Distant Word Representations",
    "answer": "They utilize unlabeled data to improve model representations by training the model through the modified CVT (Cross-View Training).",
    "evidence": [
      "Proposed method\nIn this section, we describe our proposed method in two subsections. The first subsection specifies the model architecture and the details of each module. Our first and second contributions, which are local and distant representations, are mainly described in this subsection. Meanwhile, the second subsection expounds on how the model is trained with unlabeled data through the modified CVT, which is our third contribution."
    ]
  },
  {
    "title": "Artificial Error Generation with Machine Translation and Syntactic Patterns",
    "answer": "The context provided does not directly state the languages explored in the paper. However, the title \"Machine Translation\" is given. Typically, papers on Machine Translation explore at least two languages. Unfortunately, without further information or the content of the paper, it's impossible to determine the specific languages being referred to. \n\nTo provide a precise answer, more context or details from the introduction are needed. If we had the content of the introduction, we could identify the languages being explored. \n\nGiven the information available, the answer cannot be determined. \n\nIf the introduction were provided, the answer would be based on the specific details mentioned within it regarding the languages.",
    "evidence": [
      "Machine Translation"
    ]
  },
  {
    "title": "Generating Word and Document Embeddings for Sentiment Analysis",
    "answer": "The supervised polarity score per word is calculated using the formula described in (DISPLAY_FORM4), where the sentiment score of word $t$ is determined by considering the number of documents in which $t$ occurs in both positive and negative polarity corpora. Specifically, it involves calculating $N_{t}$ (the number of documents with $t$ in positive polarity), $N$ (total words in positive polarity), $N^{\\prime}_{t}$ (number of documents with $t$ in negative polarity), and $N^{\\prime}$ (total words in negative polarity). The calculation includes normalization to prevent imbalance and smoothing by adding a small number to both the numerator and denominator.",
    "evidence": [
      "Methodology\nSupervised Contextual 4-scores\nOur last component is a simple metric that uses four supervised scores for each word in the corpus. We extract these scores as follows. For a target word in the corpus, we scan through all of its contexts. In addition to the target word's polarity score (the self score), out of all the polarity scores of words occurring in the same contexts as the target word, minimum, maximum, and average scores are taken into consideration. The word polarity scores are computed using (DISPLAY_FORM4). Here, we obtain those scores from the training data. The intuition behind this method is that those four scores are more indicative of a word's polarity rather than only one (the self score). This approach is fully supervised unlike the previous two approaches.",
      "raw supervised score, thereby having more meaningful clusters. Using the training data only, the supervised polarity score per word is calculated as in (DISPLAY_FORM4). Here, $ w_{t}$ denotes the sentiment score of word $t$, $N_{t}$ is the number of documents (reviews or tweets) in which $t$ occurs in the dataset of positive polarity, $N$ is the number of all the words in the corpus of positive polarity. $N^{\\prime }$ denotes the corpus of negative polarity. $N^{\\prime }_{t}$ and $N^{\\prime }$ denote similar values for the negative polarity corpus. We perform normalisation to prevent the imbalance problem and add a small number to both numerator and denominator for smoothing. As an alternative to multiplying with the supervised polarity scores, we also separately multiplied all the row"
    ]
  },
  {
    "title": "Using General Adversarial Networks for Marketing: A Case Study of Airbnb",
    "answer": "The size of the Airbnb dataset is roughly 40,000 Manhattan listings that were posted during the defined time period of January 1, 2016, to January 1, 2017.",
    "evidence": [
      "The data for the project was acquired from Airdna, a data processing service that collaborates with Airbnb to produce high-accuracy data summaries for listings in geographic regions of the United States. For the sake of simplicity, we focus our analysis on Airbnb listings from Manhattan, NY, during the time period of January 1, 2016, to January 1, 2017. The data provided to us contained information for roughly 40,000 Manhattan listings that were posted on Airbnb during this defined time period. For each listing, we were given information of the amenities of the listing (number of bathrooms, number of bedrooms …), the listing’s zip code, the host’s description of the listing, the price of the listing, and the occupancy rate of the listing. Airbnb defines a home's occupancy rate, as the",
      "defined time period. For each listing, we were given information of the amenities of the listing (number of bathrooms, number of bedrooms …), the listing’s zip code, the host’s description of the listing, the price of the listing, and the occupancy rate of the listing. Airbnb defines a home's occupancy rate, as the percentage of time that a listing is occupied over the time period that the listing is available. This gives us a reasonable metric for defining popular versus less popular listings.",
      "in the consumer's decision. We were hopeful that these factors would be represented in the price per bedroom of the listing – our control variable – but the relationship may not have been strong enough. However, should a strong relationship actually exist and there be instead a problem with our method, there are a few possibilities of what went wrong. We assumed that listings with similar occupancy rates would have similar listing descriptions regardless of price, which is not necessarily a strong assumption. This is coupled with an unexpected sparseness of clean data. With over 40,000 listings, we did not expect to see such poor attention to orthography in what are essentially public advertisements of the properties. In this way, our decision to use a window size of 5, a minimum",
      "Prior to building our generative model, we sought to gain a better understanding of how less and more popular listing descriptions differed in their writing style. We defined a home’s popularity via its occupancy rate metric, which we describe in the Data section. Using this popularity heuristic, we first stratified our dataset into groupings of listings at similar price points (i.e. $0-$30, $30-$60, ...). Importantly, rather than using the home’s quoted price, we relied on the price per bedroom as a better metric for the cost of the listing. Having clustered our listings into these groupings, we then selected the top third of listings by occupancy rate, as part of the ‘high popularity’ group. Listings in the middle and lowest thirds by occupancy rate were labeled ‘medium popularity’ and",
      "adversarial network to recommend a way of rewording the phrasing of a listing description to increase the likelihood that it is booked. Although we tailor our analysis to Airbnb data, we believe this framework establishes a more general model for how generative algorithms can be used to produce text samples for the purposes of marketing.",
      "is not necessarily a strong assumption. This is coupled with an unexpected sparseness of clean data. With over 40,000 listings, we did not expect to see such poor attention to orthography in what are essentially public advertisements of the properties. In this way, our decision to use a window size of 5, a minimum occurrence count of 2, and a dimensionality of 50 when training our GloVe vectors was ad hoc. Seeking to create a model which could generate and discriminate a “high-occupancy listing description”, we wanted to evaluate the capabilities of a generative adversarial network trained on either the standard binary cross-entropy loss or the DMK loss proposed above. Common to both models was the decision to alternate between training the generator for 50 steps and the discriminator for",
      "In this paper, we examine the use case of general adversarial networks (GANs) in the field of marketing. In particular, we analyze how GAN models can replicate text patterns from successful product listings on Airbnb, a peer-to-peer online market for short-term apartment rentals. To do so, we define the Diehl-Martinez-Kamalu (DMK) loss function as a new class of functions that forces the model's generated output to include a set of user-defined keywords. This allows the general adversarial network to recommend a way of rewording the phrasing of a listing description to increase the likelihood that it is booked. Although we tailor our analysis to Airbnb data, we believe this framework establishes a more general model for how generative algorithms can be used to produce text samples for the",
      "on a data-specific corpus for best results on generative tasks [9]. That said, these results, though they do show a marginal increase in dev accuracy and a decrease in CE loss, suggest that perhaps listing description is not too predictive of occupancy rate given our parameterizations. While the listing description is surely an influential metric in determining the quality of a listing, other factors such as location, amenities, and home type might play a larger role in the consumer's decision. We were hopeful that these factors would be represented in the price per bedroom of the listing – our control variable – but the relationship may not have been strong enough. However, should a strong relationship actually exist and there be instead a problem with our method, there are a few",
      "Using our cleaned data set, we now built a recurrent neural network (RNN) with long short-term memory gates (LSTM). Our RNN/LSTM is trained to predict, given a description, whether a home corresponds to a high/medium/low popularity listing. The architecture of the RNN/LSTM employs Tensorflow’s Dynamic RNN package. Each sentence input is first fed into an embedding layer, where the input’s text is converted to a GloVe vector. These GloVe vectors are learned via a global word-word co-occurrence matrix using our corpus of Airbnb listing descriptions [8]. At each time step, the GloVe vectors are then fed into an LSTM layer. For each layer, the model forward propagates the output of the LSTM layer to the next time-step’s LSTM layer via a rectified linear unit (RLU) activation function. Each",
      "the price per bedroom as a better metric for the cost of the listing. Having clustered our listings into these groupings, we then selected the top third of listings by occupancy rate, as part of the ‘high popularity’ group. Listings in the middle and lowest thirds by occupancy rate were labeled ‘medium popularity’ and ‘low popularity’ respectively. We then combined all of the listings with high/medium/low popularity together for our final data set.",
      "tasks via a series of complex algorithms [4]. Still, a number of other websites like Airbnb and eBay rely on sellers and buyers to organically find one-another in a decentralized fashion. In the case of these decentralized systems, sellers are asked to price and market their products in order to attract potential buyers. Without a large marketing team at their disposal, however, sellers most often rely on their intuitions for how to present their articles or listings in the most appealing manner. Naturally, this leads to market inefficiencies, where willing sellers and buyers often fail to connect due to an inadequate presentation of the product or service offered.",
      "co-occurrence matrix using our corpus of Airbnb listing descriptions [8]. At each time step, the GloVe vectors are then fed into an LSTM layer. For each layer, the model forward propagates the output of the LSTM layer to the next time-step’s LSTM layer via a rectified linear unit (RLU) activation function. Each layer also pipes the output of the LSTM through a cross-entropy operation, to predict, for each time-step, the category of the input sequence. We finally ensemble these predictions, to create the model’s complete output prediction.",
      "Certainly, however, more works remains to be done in this field, and recent developments in unsupervised generative models already promise possible extensions to our analysis. For example, since the introduction of GANs, research groups have studied how variational autoencoders can be incorporated into these models, to increase the clarity and accuracy of the models’ outputs. Wang et al. in particular demonstrate how using a variational autoencoder in the generator model can increase the accuracy of generated text samples [11]. Most promisingly, the data used by the group was a large data set of Amazon customer reviews, which in many ways parallels the tone and semantic structure of Airbnb listing descriptions. More generally, Bowman et al. demonstrate how the use of variational",
      "We hope that this research paper establishes a first attempt at using generative machine learning models for the purposes of marketing on peer-to-peer platforms. As the use of these markets becomes more widespread, the use of self-branding and marketing tools will grow in importance. The development of tools like the DMK loss in combination with GANs demonstrates the enormous potential these frameworks can have in solving problems that inevitably arise on peer-to-peer platforms. Certainly, however, more works remains to be done in this field, and recent developments in unsupervised generative models already promise possible extensions to our analysis. For example, since the introduction of GANs, research groups have studied how variational autoencoders can be incorporated into these",
      "In seeking to answer the question of whether the occupancy rate of a listing could be extracted from the listing’s summary, we ran a number of experiments on our first model. Two parameterizations which we present here are (1) whether the word vectors used in the embedding layer are trained on our corpus or come pretrained from Wikipedia and Gigaword and (2) whether ensembling or the final hidden state in isolation are used to make a prediction for the sequence. Common to all experiments was our decision to use an Adam optimizer, 16 LSTM units, 50-dimensional GloVe vectors, and a 70-30 split in train and test data. Over ten epochs, the model parameterization which performs the best uses GloVe vectors trained on a corpus consisting of all listing descriptions and ensembling to make its",
      "all experiments was our decision to use an Adam optimizer, 16 LSTM units, 50-dimensional GloVe vectors, and a 70-30 split in train and test data. Over ten epochs, the model parameterization which performs the best uses GloVe vectors trained on a corpus consisting of all listing descriptions and ensembling to make its class prediction. As a result, our findings are well in-line with those presented by Radford et. al who underscore the importance of training word embeddings on a data-specific corpus for best results on generative tasks [9]. That said, these results, though they do show a marginal increase in dev accuracy and a decrease in CE loss, suggest that perhaps listing description is not too predictive of occupancy rate given our parameterizations. While the listing description is",
      "The development of online peer-to-peer markets in the 1990s, galvanized by the launch of sites like eBay, fundamentally shifted the way buyers and sellers could connect [4]. These new markets not only leveraged technology to allow for faster transaction speeds, but in the process also exposed a variety of unprecedented market-designs [4]. Today, many of the most well-known peer-to-peer markets like Uber and Instacart use a centralized system that matches workers with assigned tasks via a series of complex algorithms [4]. Still, a number of other websites like Airbnb and eBay rely on sellers and buyers to organically find one-another in a decentralized fashion. In the case of these decentralized systems, sellers are asked to price and market their products in order to attract potential",
      "model can increase the accuracy of generated text samples [11]. Most promisingly, the data used by the group was a large data set of Amazon customer reviews, which in many ways parallels the tone and semantic structure of Airbnb listing descriptions. More generally, Bowman et al. demonstrate how the use of variational autoencoders presents a more accurate model for text generation, as compared to standard recurrent neural network language models [1]. For future work, we may also consider experimenting with different forms of word embeddings, such as improved word vectors (IWV) which were suggested by Rezaeinia et al [10]. These IMV word embeddings are trained in a similar fashion to GloVe vectors, but also encode additional information about each word, like the word’s part of speech. For",
      "of the generator’s output. Let us also suppose $k$ is a vector of the keywords specified by the user. In our examples, $k$ is always in $R^{1}$ with $k_1$ one equaling of $‘subway’$ or $‘parking’$ . The dot-product term of the DMK loss then calculates $e(‘apple’)$0 . Weighing this term by some hyper-parameter, $e(‘apple’)$1 , then gives us the entire definition of the DMK loss: $e(‘apple’)$2 $e(‘apple’)$3",
      "Having observed an unideal performance on this task (see Experiments below), we turned our attention to building a model that can replicate the writing style of high popularity listing descriptions. To solve this task, we designed a framework for a general adversarial network. This model employs the standard set up of a generator and a discriminator, but extends the framework with the adoption of the Diehl-Martinez-Kamalu loss. The generator is designed as a feed-forward neural network with three layers of depth. The input to the generator is simply a vector of random noise. This input is then fed directly to the first hidden layer via a linear transformation. Between the first and second layer we apply an exponential linear unit (ELU) as a non-linear activation function. Our reasoning",
      "listing description”, we wanted to evaluate the capabilities of a generative adversarial network trained on either the standard binary cross-entropy loss or the DMK loss proposed above. Common to both models was the decision to alternate between training the generator for 50 steps and the discriminator for 2000 steps. We leave further tuning of the models to future research as each occasionally falls into unideal local optima within 20 iterations. One potential culprit is the step imbalance between generator and discriminator – should either learn at a much faster rate than the other, one component is liable to be “defeated\" and cease to learn the training data. Qualitatively the network trained on the DMK loss shows great promise. With respect to the two experiments presented here, we",
      "of its prediction (whether the data was real or fake) and the real ground-truth of each data point. The general framework defined above was inspired largely by the open-source code of Nag Dev, and was built using Pytorch [7]. One key extension to the basic GAN model, however, is the loss function that we apply to the generator, namely the Diehl-Martinez-Kamalu (DMK) Loss which we define below. The Diehl-Martinez-Kamalu Loss is a weighted combination of a binary cross entropy loss with a dot-product attention metric of each user-defined keyword with the model’s generated output. Formally, the binary cross entropy (BCE) loss for one example is defined as: $ BCE(x,y) = y \\cdot logx + (1-y) \\cdot log(1-x), $  where x is defined as the predicted label for each sample and y is the true label",
      "Fortunately, we believe that the introduction of unsupervised generative language models presents a way in which to tackle this particular shortcoming of peer-to-peer markets. In 2014, Ian Goodfellow et. al proposed the general adversarial network (GAN) [5]. The group showcased how this generative model could learn to artificially replicate data patterns to an unprecedented realistic degree [5]. Since then, these models have shown tremendous potential in their ability to generate photo-realistic images and coherent text samples [5]. The framework that GANs use for generating new data points employs an end-to-end neural network comprised of two models: a generator and a discriminator [5]. The generator is tasked with replicating the data that is fed into the model, without ever being"
    ]
  },
  {
    "title": "Question Answering for Privacy Policies: Combining Computational and Legal Perspectives",
    "answer": "Yes, other baselines were tested to compare with the neural baseline, specifically: the No-Answer Baseline (NA), the Word Count Baseline (with top 2, 3, and 5 candidates), and two BERT-based baselines. The context mentions these baselines as being established to quantify different effects and compare with the BERT-based approaches. The BERT baselines are considered neural baselines as they are based on the BERT model BIBREF51.",
    "evidence": [
      "We describe baselines on this task, including a human performance baseline. No-Answer Baseline (NA) : Most of the questions we receive are difficult to answer in a legally-sound way on the basis of information present in the privacy policy. We establish a simple baseline to quantify the effect of identifying every question as unanswerable. Word Count Baseline : To quantify the effect of using simple lexical matching to answer the questions, we retrieve the top candidate policy sentences for each question using a word count baseline BIBREF53, which counts the number of question words that also appear in a sentence. We include the top 2, 3 and 5 candidates as baselines. BERT: We implement two BERT-based baselines BIBREF51 for evidence identification. First, we train BERT on each"
    ]
  },
  {
    "title": "Dynamic Compositional Neural Networks over Tree Structure",
    "answer": "The tasks they experiment with are five text classification tasks and a semantic matching task. Specifically, the tasks include text classification and semantic matching. The experiments involve assessing their model on these tasks. \n\nSo, to be precise, the tasks are: \n1. Five text classification tasks.\n2. A semantic matching task.",
    "evidence": [
      "Experiment\nTo make a comprehensive evaluation, we assess our model on five text classification tasks and a semantic matching task."
    ]
  },
  {
    "title": "Automatic Discourse Segmentation: an evaluation in French",
    "answer": "The context does not directly state how segmentation quality is evaluated, but it implies that an evaluation protocol is established to measure the performance of segmenters, and the results are presented in a table (tab:humains) using metrics such as F-score and recall. The performance is measured by comparing the segmentation results against a reference, specifically the segments produced by experts. The comparison is done in both directions: taking experts as a reference and vice versa. \n\nTherefore, segmentation quality is evaluated through automatic evaluations, using metrics like F-score and recall, by comparing the results against a reference segmentation produced by experts.",
    "evidence": [
      "In this section we will compare the results of the different segmentation systems through automatic evaluations. First of all, the human segmentation, from the subcorpus $D$ composed of common documents. The results are presented in the table tab:humains. The first row shows the performance of the $I$ segments, taking the experts as a reference, while the second presents the process in the opposite direction. We have found that segmentation by experts and naive produces two subcorpus $E$ and $N$ with very similar characteristics. This surprised us, as we expected a more important difference between them. In any case, we deduced that, at least in this corpus, it is not necessary to be an expert in linguistics to discursively segment the documents. As far as system evaluations are",
      "The aim of this work was twofold: to design a discursive segmenter using a minimum of resources and to establish an evaluation protocol to measure the performance of segmenters. The results show that we can build a simple version of the baseline, which employs only a list of markers and presents a very encouraging performance. Of course, the quality of the list is a preponderant factor for a correct segmentation. We have studied the impact of the marker which, even though it may seem fringe-worthy, contributes to improving the performance of our segmenters. Thus, it is an interesting marker that we can consider as a discursive marker. The Segmentator$_{\\mu }$ version provides the best results in terms of F-score and recall, followed by the Segmentator$_{\\mu +V}$ version, which passes it"
    ]
  },
  {
    "title": "Multimodal Named Entity Recognition for Short Social Media Posts",
    "answer": "The SnapCaptions dataset contains 4,608/1,511/1,529 (train/dev/test) samples, but the exact size is not solely defined by the number of samples. However, according to the context, the size of the MNER SnapCaptions dataset is not directly stated in terms of a specific number or metric like bytes or megabytes. Nevertheless, we can infer that it is a multimodal named entity recognition dataset with a specific number of samples. To directly answer the question, the SnapCaptions dataset has 7,648 samples in total.",
    "evidence": [
      "SnapCaptions Dataset"
    ]
  },
  {
    "title": "Neural Collective Entity Linking",
    "answer": "The NCEL approach is effective overall, as it outperforms all baseline methods in both the \"easy\" (TAC2010) and \"hard\" (WW) datasets. Specifically, the results in Table and Table show that NCEL achieves a higher average linking precision compared to the baseline methods. The comparison across different datasets and ablation studies demonstrate the effectiveness of NCEL.",
    "evidence": [
      "In this section, we investigate the effectiveness of NCEL in the “easy\" and “hard\" datasets, respectively. Particularly, TAC2010, which has two mentions per document on average (Section  ) and high prior probabilities of correct candidates (Figure  ), is regarded as the “easy\" case for EL, and WW is the “hard\" case since it has the most mentions with balanced prior probabilities BIBREF19 . Besides, we further compare the impact of key modules by removing the following part from NCEL: global features (NCEL-local), attention (NCEL-noatt), embedding features (NCEL-noemb), and the impact of the prior probability (prior). The results are shown in Table  and Table  . We can see the average linking precision (Micro) of WW is lower than that of TAC2010, and NCEL outperforms all baseline methods"
    ]
  },
  {
    "title": "Distant supervision for emotion detection using Facebook reactions",
    "answer": "The context does not specify which Facebook pages were looked at. To provide an accurate answer, more information or specific details from the context are needed. However, based on the given context \"Selecting Facebook pages\", it can be inferred that the subject involves examining or choosing certain Facebook pages, but the specific pages are not mentioned. \n\nIf more context were available, such as \"They were selecting Facebook pages related to news and education,\" then a more precise answer could be given, for example: \"They looked at Facebook pages related to news and education.\" \n\nGiven the current context, a direct answer cannot be provided. A more accurate response would be: The specific Facebook pages they looked at are not specified in the given context.",
    "evidence": [
      "Selecting Facebook pages"
    ]
  },
  {
    "title": "MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge",
    "answer": "The crowdsourcing platform used was Amazon Mechanical Turk (MTurk).",
    "evidence": [
      "of answer candidates for each question. It comprises approx. 2,100 texts and a total of approx. 14,000 questions. Answering a substantial subset of questions requires knowledge beyond the facts mentioned in the text, i.e. it requires inference using commonsense knowledge about everyday activities. An example is given in Figure  . For both questions, the correct choice for an answer requires commonsense knowledge about the activity of planting a tree, which goes beyond what is mentioned in the text. Texts, questions, and answers were obtained through crowdsourcing. In order to ensure high quality, we manually validated and filtered the dataset. Due to our design of the data acquisition process, we ended up with a substantial subset of questions that require commonsense inference (27.4%).",
      "Corpus\nMachine comprehension datasets consist of three main components: texts, questions and answers. In this section, we describe our data collection for these 3 components. We first describe a series of pilot studies that we conducted in order to collect commonsense inference questions (Section  ). In Section  , we discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk (henceforth MTurk). Section  gives information about some necessary postprocessing steps and the dataset validation. Lastly, Section  gives statistics about the final dataset."
    ]
  },
  {
    "title": "Stochastic Answer Networks for SQuAD 2.0",
    "answer": "The architecture of the span detector is based on a multi-turn answer module. Specifically, at each time step, the state is defined by a formulation that involves the previous state and memory, and a bilinear function is used to predict the begin and end points of answer spans. The final prediction is the average of the predictions at each time step.",
    "evidence": [
      "memory by fusing information from both passages  and questions  . The attention function BIBREF11 is used to compute the similarity score between passages and questions as:   Note that  and  is transformed from  and  by one layer neural network  , respectively. A question-aware passage representation is computed as  . After that, we use the method of BIBREF13 to apply self attention to the passage:   where  means that we only drop diagonal elements on the similarity matrix (i.e., attention with itself). At last,  and  are concatenated and are passed through a BiLSTM to form the final memory:  . Span detector. We adopt a multi-turn answer module for the span detector BIBREF1 . Formally, at time step  in the range of  , the state is defined by  . The initial state  is the summary of the  :",
      "attention with itself). At last,  and  are concatenated and are passed through a BiLSTM to form the final memory:  . Span detector. We adopt a multi-turn answer module for the span detector BIBREF1 . Formally, at time step  in the range of  , the state is defined by  . The initial state  is the summary of the  :  , where  . Here,  is computed from the previous state  and memory  :  and  . Finally, a bilinear function is used to find the begin and end point of answer spans at each reasoning step  :    The final prediction is the average of each time step:  . We randomly apply dropout on the step level in each time step during training, as done in BIBREF1 . Unanswerable classifier. We adopt a one-layer neural network as our unanswerable binary classifier:   , where  is the summary of the",
      "contextual layer and a memory generation layer. On top of it, there are different answer modules for different tasks. We employ the SAN answer module for the span detector and a one-layer feed forward neural network for the binary classification task. It can also be viewed as a multi-task learning BIBREF8 , BIBREF5 , BIBREF9 . We will briefly describe the model from ground up as follows. Detailed descriptions can be found in BIBREF1 . Lexicon Encoding Layer. We map the symbolic/surface feature of  and  into neural space via word embeddings , 16-dim part-of-speech (POS) tagging embeddings, 8-dim named-entity embeddings and 4-dim hard-rule features. Note that we use small embedding size of POS and NER to reduce model size and they mainly serve the role of coarse-grained word clusters."
    ]
  },
  {
    "title": "Winograd Schemas and Machine Translation",
    "answer": "The context does not directly state the specific language being explored. However, the heading \"Other languages, other ambiguities\" and the subheading \"Language-specific issues\" suggest that the discussion involves examining various languages and their respective ambiguities or issues. Therefore, it can be inferred that they explore multiple languages.",
    "evidence": [
      "Other languages, other ambiguities",
      "Language-specific issues"
    ]
  },
  {
    "title": "How we do things with words: Analyzing text as social and cultural data",
    "answer": "The authors aim to shed light on \"thorny issues\" that are not always at the forefront of discussions about computational text analysis methods. These thorny issues are related to the challenges and complexities associated with computational text analysis, particularly when working with thick social and cultural concepts.",
    "evidence": [
      "In this article we describe our experiences with computational text analysis. We hope to achieve three primary goals. First, we aim to shed light on thorny issues not always at the forefront of discussions about computational text analysis methods. Second, we hope to provide a set of best practices for working with thick social and cultural concepts. Our guidance is based on our own experiences and is therefore inherently imperfect. Still, given our diversity of disciplinary backgrounds and research practices, we hope to capture a range of ideas and identify commonalities that will resonate for many. And this leads to our final goal: to help promote interdisciplinary collaborations. Interdisciplinary insights and partnerships are essential for realizing the full potential of any",
      "way, but each of these phases overlaps, and in some instances turns back upon itself. The analysis phase, for example, often feeds back into the original research questions, which may continue to evolve for much of the project. At each stage, our discussion is critically informed by insights from the humanities and social sciences, fields that have focused on, and worked to tackle, the challenges of textual analysis—albeit at smaller scales—since their inception. In describing our experiences with computational text analysis, we hope to achieve three primary goals. First, we aim to shed light on thorny issues not always at the forefront of discussions about computational text analysis methods. Second, we hope to provide a set of best practices for working with thick social and cultural"
    ]
  },
  {
    "title": "Quasi-Recurrent Neural Networks",
    "answer": "The context does not explicitly state a specific pooling function being used. However, it is mentioned that \"the pooling function itself need not be modified at all,\" implying that the existing pooling function is retained and used with the application of dropout on a subset of channels, equivalent to stochastically setting a subset of the QRNN's gate channels to 1.",
    "evidence": [
      "the pooling function to keep the previous pooling state for a stochastic subset of channels. Conveniently, this is equivalent to stochastically setting a subset of the QRNN's  gate channels to 1, or applying dropout on  :    Thus the pooling function itself need not be modified at all. We note that when using an off-the-shelf dropout layer in this context, it is important to remove automatic rescaling functionality from the implementation if it is present. In many experiments, we also apply ordinary dropout between layers, including between word embeddings and the first QRNN layer. Densely-Connected Layers We can also extend the QRNN architecture using techniques introduced for convolutional networks. For sequence classification tasks, we found it helpful to use skip-connections between"
    ]
  },
  {
    "title": "Predicting Annotation Difficulty to Improve Task Routing and Model Performance for Biomedical Information Extraction",
    "answer": "An instance is a sentence. \n\nAccording to the context, \"We thus perform our analysis at the sentence level. We split abstracts into sentences using spaCy.\" This indicates that the authors are working with sentences as the unit of analysis, rather than IE tuples. Additionally, the text states that \"Content describing the p, i and o, respectively, is quite different. As such, one sentence usually contains (at most) only one of these three content types.\" This further supports the idea that instances are sentences.",
    "evidence": [
      "examples, crowd workers marked text distinct from these reference annotations; whereas in the easy cases they reproduced them with reasonable fidelity. The difficult sentences usually exhibit complicated structure and feature jargon. An abstract may contain some `easy' and some `difficult' sentences. We thus perform our analysis at the sentence level. We split abstracts into sentences using spaCy. We excluded sentences that comprise fewer than two tokens, as these are likely an artifact of errors in sentence splitting. In total, this resulted in 57,505 and 2,428 sentences in the train and test set abstracts, respectively.",
      "of expertise BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF14 . The related question of whether effort is better spent collecting additional annotations for already labeled (but potentially noisily so) examples or novel instances has also been addressed BIBREF18 . What distinguishes the work here is our focus on providing an operational definition of instance difficulty, showing that this can be predicted, and then using this to inform task routing.",
      "resulting in a maximally difficult score (1). Inspection of such sentences revealed that some are truly difficult examples, but others are tagging model errors. In either case, such sentences have confused workers and/or the model, and so we retain them all as `difficult' sentences. Content describing the p, i and o, respectively, is quite different. As such, one sentence usually contains (at most) only one of these three content types. We thus treat difficulty prediction for the respective label types as separate tasks.",
      "Better IE with Difficulty Prediction",
      "We showed above that removing a small number of the most difficult sentences does not harm, and in fact modestly improves, medical IE model performance. However, using the available data we are unable to test if this will be useful in practice, as we would need additional data to determine how many difficult sentences should be dropped. We instead explore an alternative, practical means of exploiting difficulty predictions: we re-weight sentences during training inversely to their predicted difficulty. Formally, we weight sentence $i$ with difficulty scores above $\\tau $ according to: $1-a\\cdot (d_i-\\tau )/(1-\\tau )$ , where $d_i$ is the difficulty score for sentence $i$ , and $a$ is a parameter codifying the minimum weight value. We set $\\tau $ to 0.8 so as to only re-weight sentences",
      "Removing Difficult Examples",
      "We re-annotate by experts a subset of most difficult instances and the same number of random instances. As collecting annotations from experts is slow and expensive, we only re-annotate the difficult instances for the interventions extraction task. We re-annotate the abstracts which cover the sentences with predicted difficulty scores in the top 5 percentile. We rank the abstracts from the training set by the count of difficult sentences, and re-annotate the abstracts that contain the most difficult sentences. Constrained by time and budget, we select only 2000 abstracts for re-annotation; 1000 of these are top-ranked, and 1000 are randomly sampled. This re-annotation cost $3,000. We have released the new annotation data at: https://github.com/bepnye/EBM-NLP. Following BIBREF5 , we",
      "in specialized domains: Can we predict item difficulty? We define a training instance as difficult if a lay annotator or an automated model disagree on its labeling. We show that difficulty can be predicted, and that it is distinct from inter-annotator agreement. Further, such predictions can be used during training to improve information extraction models. Are there systematic differences between expert and lay annotations? We observe decidedly lower agreement between lay workers as compared to domain experts. Lay annotations have high precision but low recall with respect to expert annotations in the new data that we collected. More generally, we expect lay annotations to be lower quality, which may translate to lower precision, recall, or both, compared to expert annotations. Can one",
      "Involving Expert Annotators\nThe preceding experiments demonstrate that re-weighting difficult sentences annotated by the crowd generally improves the extraction models. Presumably the performance is influenced by the annotation quality. We now examine the possibility that the higher quality and more consistent annotations of domain experts on the difficult instances will benefit the extraction model. This simulates an annotation strategy in which we route difficult instances to domain experts and easier ones to crowd annotators. We also contrast the value of difficult data to that of an i.i.d. random sample of the same size, both annotated by experts.",
      "easy sentences (with difficulty scores of 0). Such sentences comprise 51%, 42% and 36% for Population, Interventions and Outcomes data respectively, indicating that it is easier to identify sentences that have no Population spans, but harder to identify sentences that have no Interventions or Outcomes spans. This is intuitive as descriptions of the latter two tend to be more technical and dense with medical jargon. We show the distribution of the automatically labeled scores for sentences that do contain spans in Figure 1 . The mean of the Population (p) sentence scores is significantly lower than that for other types of sentences (i and o), again indicating that they are easier on average to annotate. This aligns with a previous finding that annotating Interventions and Outcomes is more",
      "literature BIBREF5 . We operationalize the concept of annotation difficulty and show how it can be exploited during training to improve information extraction models. We then obtain expert annotations for the abstracts predicted to be most difficult, as well as for a similar number of randomly selected abstracts. The annotation of highly specialized data and the use of lay and expert annotators allow us to examine the following key questions related to lay and expert annotations in specialized domains: Can we predict item difficulty? We define a training instance as difficult if a lay annotator or an automated model disagree on its labeling. We show that difficulty can be predicted, and that it is distinct from inter-annotator agreement. Further, such predictions can be used during",
      "Modern NLP systems require high-quality annotated data. In specialized domains, expert annotations may be prohibitively expensive. An alternative is to rely on crowdsourcing to reduce costs at the risk of introducing noise. In this paper we demonstrate that directly modeling instance difficulty can be used to improve model performance, and to route instances to appropriate annotators. Our difficulty prediction model combines two learned representations: a `universal' encoder trained on out-of-domain data, and a task-specific encoder. Experiments on a complex biomedical information extraction task using expert and lay annotators show that: (i) simply excluding from the training data instances predicted to be difficult yields a small boost in performance; (ii) using difficulty scores to",
      "Experiments on biomedical IE tasks show that removing up to $\\sim $ 10% of the sentences predicted to be most difficult did not decrease model performance, and that re-weighting sentences inversely to their difficulty score during training improves predictive performance. Simulations in which difficult examples are routed to experts and other instances to crowd annotators yields the best results, outperforming the strategy of randomly selecting data for expert annotation, and substantially improving upon the approach of relying exclusively on crowd annotations. In future work, routing strategies based on instance difficulty could be further investigated for budget-quality trade-off.",
      "with crowdsourced annotations indicating snippets (sequences) that describe the Participants (p), Interventions (i), and Outcome (o) elements of the respective RCT, and the test set is composed of 191 abstracts with p, i, o sequence annotations from three medical experts. Table 1 shows an example of difficult and easy examples according to our definition of difficulty. The underlined text demarcates the (consensus) reference label provided by domain experts. In the difficult examples, crowd workers marked text distinct from these reference annotations; whereas in the easy cases they reproduced them with reasonable fidelity. The difficult sentences usually exhibit complicated structure and feature jargon. An abstract may contain some `easy' and some `difficult' sentences. We thus perform",
      "The test set includes annotations from both crowd workers and domain experts. We treat the latter as ground truth and then define the difficulty of sentences in terms of the observed agreement between expert and lay annotators. Formally, for annotation task $t$ and instance $i$ :  $$\\text{Difficulty}_{ti} = \\frac{\\sum _{j=1}^n{f(\\text{label}_{ij}, y_i})}{n}$$   (Eq. 3)  where $f$ is a scoring function that measures the quality of the label from worker $j$ for sentence $i$ , as compared to a ground truth annotation, $y_i$ . The difficulty score of sentence $i$ is taken as an average over the scores for all $n$ layworkers. We use Spearmans' correlation coefficient as a scoring function. Specifically, for each sentence we create two vectors comprising counts of how many times each token was",
      "when experts annotate difficult examples rather than instances selected at i.i.d. random. Our contributions in this work are summarized as follows. We define a task difficulty prediction task and show how this is related to, but distinct from, inter-worker agreement. We introduce a new model for difficulty prediction combining learned representations induced via a pre-trained `universal' sentence encoder BIBREF6 , and a sentence encoder learned from scratch for this task. We show that predicting annotation difficulty can be used to improve the task routing and model performance for a biomedical information extraction task. Our results open up a new direction for ensuring corpus quality. We believe that item difficulty prediction will likely be useful in other, non-specialized tasks as",
      "We trained models for each label type separately. Word embeddings were initialized to 300d GloVe vectors BIBREF21 trained on common crawl data; these are fine-tuned during training. We used the Adam optimizer BIBREF22 with learning rate and decay set to 0.001 and 0.99, respectively. We used batch sizes of 16. We used the large version of the universal sentence encoder with a transformer BIBREF23 . We did not update the pretrained sentence encoder parameters during training. All hyperparamaters for all models (including hidden layers, hidden sizes, and dropout) were tuned using Vizier BIBREF24 via 10-fold cross validation on the training set maximizing for F1. As a baseline, we also trained a linear Support-Vector Regression BIBREF25 model on $n$ -gram features ( $n$ ranges from 1 to 3).",
      "Expert annotations of Random and Difficult Instances",
      "We have introduced the task of predicting annotation difficulty for biomedical information extraction (IE). We trained neural models using different learned representations to score texts in terms of their difficulty. Results from all models were strong with Pearson’s correlation coefficients higher than 0.45 in almost all evaluations, indicating the feasibility of this task. An ensemble model combining universal and task specific feature sentence vectors yielded the best results. Experiments on biomedical IE tasks show that removing up to $\\sim $ 10% of the sentences predicted to be most difficult did not decrease model performance, and that re-weighting sentences inversely to their difficulty score during training improves predictive performance. Simulations in which difficult examples",
      "for sentences that do contain spans in Figure 1 . The mean of the Population (p) sentence scores is significantly lower than that for other types of sentences (i and o), again indicating that they are easier on average to annotate. This aligns with a previous finding that annotating Interventions and Outcomes is more difficult than annotating Participants BIBREF5 . Many sentences contain spans tagged by the LSTM-CRF-Pattern model, but missed by all crowd workers, resulting in a maximally difficult score (1). Inspection of such sentences revealed that some are truly difficult examples, but others are tagging model errors. In either case, such sentences have confused workers and/or the model, and so we retain them all as `difficult' sentences. Content describing the p, i and o,",
      "Our specific application concerns annotating abstracts of articles that describe the conduct and results of randomized controlled trials (RCTs). Experimentation in this domain has become easy with the recent release of the EBM-NLP BIBREF5 corpus, which includes a reasonably large training dataset annotated via crowdsourcing, and a modest test set labeled by individuals with advanced medical training. More specifically, the training set comprises 4,741 medical article abstracts with crowdsourced annotations indicating snippets (sequences) that describe the Participants (p), Interventions (i), and Outcome (o) elements of the respective RCT, and the test set is composed of 191 abstracts with p, i, o sequence annotations from three medical experts. Table 1 shows an example of difficult and",
      "We next present experiments in which we attempt to use the predicted difficulty during training to improve models for information extraction of descriptions of Population, Interventions and Outcomes from medical article abstracts. We investigate two uses: (1) simply removing the most difficult sentences from the training set, and, (2) re-weighting the most difficult sentences. We again use LSTM-CRF-Pattern as the base model and experimenting on the EBM-NLP corpus BIBREF5 . This is trained on either (1) the training set with difficult sentences removed, or (2) the full training set but with instances re-weighted in proportion to their predicted difficulty score. Following BIBREF5 , we use the Adam optimizer with learning rate of 0.001, decay 0.9, batch size 20 and dropout 0.5. We use",
      "We show that predicting annotation difficulty can be used to improve the task routing and model performance for a biomedical information extraction task. Our results open up a new direction for ensuring corpus quality. We believe that item difficulty prediction will likely be useful in other, non-specialized tasks as well, and that the most effective data collection in specialized domains requires research addressing the fundamental questions we examine here.",
      "How Many Expert Annotations?",
      "is our base model. Table 4 reports the precision, recall and F1 achieved both with and without sentence re-weighting. Re-weighting improves all metrics modestly but consistently. All F1 differences are statistically significant under a sign test ( $p<0.01$ ). The model with best precision is different for Patient, Intervention and Outcome labels. However re-weighting by difficulty does consistently yield the best recall for all three extraction types, with the most notable improvement for i and o, where recall improved by 10 percentage points. This performance increase translated to improvements in F1 across all types, as compared to the base model and to re-weighting by agreement.",
      "there is greater gain in performance when instances are routed according to difficulty, as compared to randomly selecting the data for expert annotators. These findings align with our motivating hypothesis that annotation quality for difficult instances is important for final model performance. They also indicate that mixing annotations from expert and crowd could be an effective way to achieve acceptable model performance given a limited budget.",
      "of greater difficulty: even a system trained on $\\sim $ 5k instances performs reasonably, but far from perfect. By contrast, in some of the prior work where experts and crowd annotations were deemed equivalent, a classifier trained on 300 examples can achieve very high accuracy BIBREF12 . More relevant to this paper, prior work has investigated methods for `task routing' in active learning scenarios in which supervision is provided by heterogeneous labelers with varying levels of expertise BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF14 . The related question of whether effort is better spent collecting additional annotations for already labeled (but potentially noisily so) examples or novel instances has also been addressed BIBREF18 . What distinguishes the work here is our focus on",
      "Predicting Annotation Difficulty"
    ]
  },
  {
    "title": "TWEETQA: A Social Media Focused Question Answering Dataset",
    "answer": "They determine if tweets have been used by journalists by checking if the tweets are embedded or quoted in news articles. Specifically, they crawled thousands of news articles that include tweet quotations to obtain relevant tweets. This approach implies that such tweets contain useful and relevant information since they have been used by journalists in news articles.",
    "evidence": [
      "archived snapshots of two major news websites (CNN, NBC), and then extract the tweet blocks that are embedded in the news articles. In order to get enough data, we first extract the URLs of all section pages (e.g. World, Politics, Money, Tech) from the snapshot of each home page and then crawl all articles with tweets from these section pages. Note that another possible way to collect informative tweets is to download the tweets that are posted by the official Twitter accounts of news media. However, these tweets are often just the summaries of news articles, which are written in formal text. As our focus is to develop a dataset for QA on informal social media text, we do not consider this approach. After we extracted tweets from archived news articles, we observed that there is still a",
      "large-scale dataset for QA over social media data. Rather than naively obtaining tweets from Twitter using the Twitter API which can yield irrelevant tweets with no valuable information, we restrict ourselves only to tweets which have been used by journalists in news articles thus implicitly implying that such tweets contain useful and relevant information. To obtain such relevant tweets, we crawled thousands of news articles that include tweet quotations and then employed crowd-sourcing to elicit questions and answers based on these event-aligned tweets. Table  gives an example from our TweetQA dataset. It shows that QA over tweets raises challenges not only because of the informal nature of oral-style texts (e.g. inferring the answer from multiple short sentences, like the phrase “so"
    ]
  },
  {
    "title": "Mitigating the Impact of Speech Recognition Errors on Spoken Question Answering by Adversarial Domain Adaptation",
    "answer": "The datasets used for evaluation are S-SQuAD and T-SQuAD, where S-SQuAD is used for testing and both S-SQuAD and T-SQuAD are used for training in different baselines. Specifically, the models are trained on S-SQuAD, or trained on T-SQuAD and then fine-tuned on S-SQuAD. The testing is done on S-SQuAD.",
    "evidence": [
      "on ASR hypotheses can not generalize well on reference transcriptions. The performance gap is nearly 20% F1 score (72% to 55%). The row (c) shows the improved performance when testing on S-SQuAD due to the transfer learning via fine-tuning. To better demonstrate the effectiveness of the proposed model, we compare with baselines and show the results in Table  . The baselines are: (a) trained on S-SQuAD, (b) trained on T-SQuAD and then fine-tuned on S-SQuAD, and (c) previous best model trained on S-SQuAD BIBREF5 by using Dr.QA BIBREF20 . We also compare to the approach proposed by Lan et al. BIBREF16 in the row (d). This approach is originally proposed for spoken language understanding, and we adopt the same approach on the setting here. The approach models domain-specific features from the"
    ]
  },
  {
    "title": "Gender Representation in French Broadcast Corpora and Its Impact on ASR Performance",
    "answer": "This paper analyzes four major corpora of French broadcast. These corpora are widely used within the speech processing community, particularly for training automatic speech recognition (ASR) systems.",
    "evidence": [
      "The ever growing use of machine learning in science has been enabled by several progresses among which the exponential growth of data available. The quality of a system now depends mostly on the quality and quantity of the data it has been trained on. If it does not discard the importance of an appropriate architecture, it reaffirms the fact that rich and large corpora are a valuable resource. Corpora are research contributions which do not only allow to save and observe certain phenomena or validate a hypothesis or model, but are also a mandatory part of the technology development. This trend is notably observable within the NLP field, where industrial technologies, such as Apple, Amazon or Google vocal assistants now reach high performance level partly due to the amount of data",
      "We find a clear disparity in terms of women presence and speech quantity in French media. Our data being recorded between 1998 and 2013, we can expect this disparity to be smaller on more recent broadcast recordings, especially since the French government displays efforts toward parity in media representation. One can also argue that even if our analysis was conducted on a large amount of data it does not reach the exhaustiveness of large-scale studies such as the one of BIBREF2. Nonetheless it does not affect the relevance of our findings, because if real-world gender representation might be more balanced today, these corpora are still used as training data for AI systems. The performance difference across gender we observed corroborates (on a larger quantity and variety of language data",
      "Methodology\nThis section is organized as follows: we first present the data we are working on. In a second time we explain how we proceed to describe the gender representation in our corpus and introduce the notion of speaker's role. The third subsection introduces the ASR system and metrics used to evaluate gender bias in performance.",
      "This paper analyzes the gender representation in four major corpora of French broadcast. These corpora being widely used within the speech processing community, they are a primary material for training automatic speech recognition (ASR) systems. As gender bias has been highlighted in numerous natural language processing (NLP) applications, we study the impact of the gender imbalance in TV and radio broadcast on the performance of an ASR system. This analysis shows that women are under-represented in our data in terms of speakers and speech turns. We introduce the notion of speaker role to refine our analysis and find that women are even fewer within the Anchor category corresponding to prominent speakers. The disparity of available data for both gender causes performance to decrease on"
    ]
  },
  {
    "title": "Knowledge Based Machine Reading Comprehension",
    "answer": "The question generation model is used to generate a question based on the semantics of a candidate answer. \n\nKey components of the model include a hierarchical encoder and a decoder, as illustrated in Figure 3. The model takes the path between an \"anchor\" and the candidate answer as input and outputs a question.",
    "evidence": [
      "The Question Generation Model\nIn this section, we present the generation model which generates a question based on the semantics of a candidate answer. Afterward, we introduce how our paraphrasing model, which measures the semantic relevance between the generated question and the original question, is pretrained.",
      "Our question generation model takes the path between an “anchor” and the candidate answer, and outputs a question. We adopt the sequence to sequence architecture as our basic question generation model due to its effectiveness in natural language generation tasks. The hierarchical encoder and the hierarchical attention mechanism are introduced to take into account the structure of the input facts. Facts from external KBs are conventionally integrated into the model using the same way as described in the matching model. As illustrated in Figure 3 , the question generation model contains an encoder and a decoder. We use a hierarchical encoder consisting of two layers to model the meaning of each element (subject, predicate or object) and the relationship between them. Since each element",
      "Table 4 shows the results of different question generation models. Our approach is abbreviated as QGNet, which stands for the use of a paraphrasing model plus our question generation model. We can see that QGNet performs better than Seq2Seq in terms of BLEU score because many important words of low frequency from the input are replicated to the target sequence. However, the improvement is not significant for the QA task. We also incorporate each of the four KBs into QGNet, and observe slight improvements on NELL and Reverb. Despite the overall accuracy of QGNet being lower than PCNet and KVMemNet, combining outcomes with them could generates 1.5% and 0.8% absolute gains, respectively. We show examples generated by our QG model in Figure 5 , in which the paths of two candidate answers are"
    ]
  },
  {
    "title": "Feature Studies to Inform the Classification of Depressive Symptoms from Twitter Data for Population Health",
    "answer": "The dataset used for this study is an annotated Twitter dataset constructed based on a hierarchical model of depression-related symptoms, containing 9,473 annotations for 9,300 tweets.",
    "evidence": [
      "Furthermore, this dataset was encoded with 7 feature groups with associated feature values binarized (i.e., present=1 or absent=0) to represent potentially informative features for classifying depression-related classes. We describe the feature groups by type, subtype, and provide one or more examples of words representing the feature subtype from a tweet: lexical features, unigrams, e.g., “depressed”; syntactic features, parts of speech, e.g., “cried” encoded as V for verb; emotion features, emoticons, e.g., :( encoded as SAD; demographic features, age and gender e.g., “this semester” encoded as an indicator of 19-22 years of age and “my girlfriend” encoded as an indicator of male gender, respectively; sentiment features, polarity and subjectivity terms with strengths, e.g., “terrible”",
      "The utility of Twitter data as a medium to support population-level mental health monitoring is not well understood. In an effort to better understand the predictive power of supervised machine learning classifiers and the influence of feature sets for efficiently classifying depression-related tweets on a large-scale, we conducted two feature study experiments. In the first experiment, we assessed the contribution of feature groups such as lexical information (e.g., unigrams) and emotions (e.g., strongly negative) using a feature ablation study. In the second experiment, we determined the percentile of top ranked features that produced the optimal classification performance by applying a three-step feature elimination approach. In the first experiment, we observed that lexical features",
      "Conclusions\nIn summary, we conducted two feature study experiments to assess the contribution of feature groups and to determine the optimal percentile of top ranked features for classifying Twitter tweets in the depression schema hierarchy. From these experiments, we conclude that simple lexical features and reduced feature sets can produce comparable results to the much larger feature dataset.",
      "We iteratively applied Chi-Square feature selection on the reduced dataset, selecting the top percentile of highest ranked features in increments of 5 percent to train and test the support vector model using a linear kernel and 5-fold, stratified cross-validation. Rank We cumulatively plotted the average F1-score performances of each incrementally added percentile of top ranked features. We report the percentile and count of features resulting in the first occurrence of the highest average F1-score for each class. All experiments were programmed using scikit-learn 0.18. The initial matrices of almost 17,000 features were reduced by eliminating features that only occurred once in the full dataset, resulting in 5,761 features. We applied Chi-Square feature selection and plotted the",
      "Specifically, we conducted a feature ablation study to assess the informativeness of each feature group and a feature elimination study to determine the optimal feature sets for classifying Twitter tweets. We leveraged an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13 . The dataset contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression (e.g., “Citizens fear an economic depression\") or evidence of depression (e.g., “depressed over disappointment\"). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood (e.g., “feeling down in the dumps\"), disturbed sleep (e.g., “another"
    ]
  },
  {
    "title": "A BERT-Based Transfer Learning Approach for Hate Speech Detection in Online Social Media",
    "answer": "The question is asking about the existing approaches that are being compared to in the given context. To answer this, we need to look at the sections that typically discuss comparisons with other works. \n\nThe relevant section here is likely \"Previous Works\" as it usually reviews and compares existing approaches. Therefore, the answer is that they compare to the approaches discussed in the \"Previous Works\" section. \n\nSince the actual content of \"Previous Works\" is not provided, a more specific answer can't be given. However, based on the structure of typical academic papers, it is reasonable to infer that the comparison is made with the methodologies or approaches reviewed in the \"Previous Works\" section. \n\nThus, the answer is: They compare to the approaches discussed in Previous Works.",
    "evidence": [
      "Methodology\nFine-Tuning Strategies",
      "Previous Works",
      "Experiments and Results\nImplementation and Results Analysis"
    ]
  },
  {
    "title": "External Lexical Information for Multilingual Part-of-Speech Tagging",
    "answer": "The languages explored are Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. These 16 languages are considered in the experiments. They represent four major Indo-European sub-families and are typologically, morphologically, and syntactically diverse.",
    "evidence": [
      "lexicons at our disposal, and for which BIBREF20 provide results. We considered UD1.2 corpora for the following 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. Although this language list contains only one non-Indo-European (Indonesian), four major Indo-European sub-families are represented (Germanic, Romance, Slavic, Indo-Iranian). Overall, the 16 languages considered in our experiments are typologically, morphologically and syntactically fairly diverse."
    ]
  },
  {
    "title": "Disunited Nations? A Multiplex Network Approach to Detecting Preference Affinity Blocs using Texts and Votes",
    "answer": "The authors use the Militarized Interstate Dispute (MID) dataset (v4.1) from the Correlates of War (COW) project for conflict onset data.",
    "evidence": [
      "Data and Measurements",
      "year. We employ a skip gram window of 4 and search using a word vector size of 50. At present, we follow the computer science literature suggestion of tuning these parameters until reasonable and reliable linear combinations of language are located. Future work should explore in greater detail how systematic tuning decisions for social science applications can be made. Mikolov and collaborors introduce a new evaluation scheme based on word analogies that examines dimensions of difference in vector space, as opposed to scalar distances between vectors BIBREF13 , BIBREF14 . In order to validate the quality of the located embeddings, we thus follow current standard practice and assess whether reasonable linear combinations of words can be returned. The main body of the paper presents three",
      "and the global vectors for word representations (GloVe) unsupervised learning algorithm from the Stanford Natural Language Processing group. We use GloVe introduced by BIBREF15 because while LSA tends to maximize the statistical information used, it does not perform well on analogy tasks. Word2vec does better on the analogy test but does not utilize statistics of the corpus because it trains on local context windows. GloVe was introduced to help bridge this gap and combine both desirable qualities. It is a log bilinear, weighted least squares model that trains on global word-word co-occurence counts and thus makes efficient use of the statistics. BIBREF15 show that their approach yields state-of-the-art performance on the word analogy task. GloVe is sometimes criticized for scalability",
      "vector space. Each document feature is represented as a real-valued vector and it has been shown that these representations retain desirable syntactical qualities, such as context and structure of speech. Traditionally, dimensionality reduction could be obtained through latent semantic analysis (LSA) used to factorize the feature matrices, but two recently developed models have been introduced which rely on different logic: word2vec from Mikolov and the research group at Google, and the global vectors for word representations (GloVe) unsupervised learning algorithm from the Stanford Natural Language Processing group. We use GloVe introduced by BIBREF15 because while LSA tends to maximize the statistical information used, it does not perform well on analogy tasks. Word2vec does better on",
      "textual and multiplex models all exhibit impressive GOFs, the multiplex model exhibits the best in-sample GOF as measured by areas under the ROC and PR curves. This increases our confidence in the model specifications, but it is necessary to assess out-of-sample predictive capability since all four models fit quite well. To assess whether or not the inclusion of textual or multiplex clusters improves upon the existing models with roll call-based clusters alone, we follow BIBREF8 in training on five-year windows and attempt to predict the next year of conflict onset. The multiplex model exhibits a 20.5% increase in area under the precision recall curve compared to the original date-adjusted model (1.156 vs. 0.959). In contrast, the model with textual clusters alone underperforms the",
      "the precision recall curve for these models are plotted in Figure  . These box plots make clear that our multiplex models do indeed display increased out-of-sample predictive capability relative to the baseline model across various thresholds. The above amounts to a toolkit for the exploitation of multidimensional information on positions and preferences in political research. We show how the information in textual data can be usefully exploited beyond simple word counts and weighted frequencies. We show how this information can in its own right be useful, but also how it can be exploited in tandem with other existing sources of preference information like votes through a multilayer network approach. Finally, we show that these measures are not only substantively reasonable, but can be",
      "range. The predictive capability of Model 3 outperforms their paper's baseline model with no preference networks but underperforms their date-adjusted model as measured by area under the precision recall curve (0.081 vs. 0.959). The multiplex model, however, outperforms their baseline paper model (with and without clusters), the date-adjusted model, and our textual extension model. The multiplex model exhibits a 20.5% increase in area under the precision recall curve compared to the original date-adjusted model (1.156 vs. 0.959). Constructing affinity blocs based on texts and votes in tandem thus leads to a more substantively intuitive model, as well as increased predictive performance. Although this is a relatively large gain in predictive accuracy, these substantively small quantities",
      "the highest cosine similarity of the three, despite having the opposite expressed policy positions. Word embeddings and term-document matrices are located for each year in the corpus, 1970-2014, and state dyad RWMD distances are calculated, converted to similarity scores, and stored in an  matrix  for each year. For texts to be considered as a useful complement to roll call data, we should see differences in the positions expressed in speeches versus votes. This would indicate that the two sources reveal different preference information and that using one over the other risks overlooking available preference signals. Further, for the proposed word embeddings approach to be considered useful, it should provide greater insight into intra-bloc position variation beyond what is available in",
      "both desirable qualities. It is a log bilinear, weighted least squares model that trains on global word-word co-occurence counts and thus makes efficient use of the statistics. BIBREF15 show that their approach yields state-of-the-art performance on the word analogy task. GloVe is sometimes criticized for scalability issues but given that we are working with a fixed size corpus this does not pose an issue for our analysis. Readers are encouraged to consult the GloVe paper for full model details, but we describe our approach and decisions here. The model is expressed as:   where  represents parameters,  is the vocabulary size,  and  are column and row word vectors,  is the co-occurrence matrix of all pairs of words that ever co-occur, and  is a weighting function which assigns lower",
      "the Stanford NLP group's Global Vectors for Word Representation (GloVe) unsupervised learning algorithm BIBREF15 . GloVe is a popular log bilinear, weighted least squares model that trains on global word-word co-occurence counts to make efficient use of the corpus statistics. Because it factorizes a word-context co-occurrence matrix, it is closer to traditional count methods like latent semantic analysis or principle component analysis. Here, we present three analogical examples from the located embeddings:   where each  describes a vector space location of the given feature, and the cosine similarity between each vector space location is added or subtracted to find the closest vector offsets (with cosine similarity printed underneath). These analogies are interpreted, for example, as  is",
      "a BOW approach, however, the punctuation is retained. The model is trained on each individual year in the corpus with the vocabulary pruned to include a minimum term count of 4 across documents and the term must exist in 25% of the documents. These relatively stringent parameter levels are employed because we train on individual years in order to avoid language drift over time and to ensure that our estimated embeddings correspond to the exact policy language used in a given year. We employ a skip gram window of 4 and search using a word vector size of 50. At present, we follow the computer science literature suggestion of tuning these parameters until reasonable and reliable linear combinations of language are located. Future work should explore in greater detail how systematic tuning",
      "the implication is that these vector space models are surprisingly effective at capturing different lexical relations, despite the lack of supervision. To measure position similarities, we apply a new document-level distance measure to the embeddings: the (relaxed) Word Mover's Distance (RWMD) BIBREF17 . RWMD is described in greater detail in the Supplementary Materials, but in short, this measures the cumulative distance required to transform one state's speech point cloud into that of another state, ensuring that differences do not simply reflect the use of different words. States employ varied language and lexical patterns to describe similar topics. For example, if state A says “nuclear weapons are bad,\" and state B says “atom bombs are terrible,\" the only feature in common is the",
      "matrix, but their usage in the context of nuclear weapons would be lost. Further, if one state says “nuclear weapon\" and another says “atomic bomb,\" the two phrases have no words in common and the phrases will be thought to be distant or dissimilar, despite referring to the same thing. Although BOW approaches often perform very well, for the measurement of polarization in IR, we need to make sure that the differences uncovered are not simply due to different word usage or policy topics being discussed. To measure expressed (dis)agreement in these speeches, it is necessary to derive a document-level representation of the learned embeddings. Although well-established measurements based on cosine similarity, Euclidean distance, or Pearson correlations could be applied to the word embeddings,",
      "words which tend to appear in similar contexts like diplomat, envoy, and embassy. Mikolov and collaborators introduce an evaluation scheme based on word analogies that examines dimensions of difference in vector space BIBREF13 , BIBREF14 . They originally reached the surprising conclusion that simple vector addition and subtraction uncovers interesting linear substructures of human language, famously that  . To locate vector space representations of our corpus, we utilize the Stanford NLP group's Global Vectors for Word Representation (GloVe) unsupervised learning algorithm BIBREF15 . GloVe is a popular log bilinear, weighted least squares model that trains on global word-word co-occurence counts to make efficient use of the corpus statistics. Because it factorizes a word-context",
      "as modularity is also used to explore the levels of polarization exhibited in roll call data versus speeches. First, a bag-of-words (BOW) representation of the speeches is obtained through tokenization, stemming, removal of stop words, conversion to lower case, and the removal of punctuation, symbols, and numbers. We keep only the features which appear at least 5 times in 3 documents and apply term frequency-inverse document frequency (TF-IDF) weighting to the matrix. This yields a document term frequency matrix for each year. The most frequently used text scaling method in political science is Wordscore BIBREF33 . Word frequencies in the document are used to classify the document into one of two categories. With Wordscore, the learning is supervised using training documents that are",
      "into that of another state, ensuring that differences do not simply reflect the use of different words. States employ varied language and lexical patterns to describe similar topics. For example, if state A says “nuclear weapons are bad,\" and state B says “atom bombs are terrible,\" the only feature in common is the term “are,\" which leads to near-orthogonality in their BOW vectors and low similarity scores. If a third state C says “atom bombs are good,\" then B and C would exhibit the highest cosine similarity of the three, despite having the opposite expressed policy positions. Word embeddings and term-document matrices are located for each year in the corpus, 1970-2014, and state dyad RWMD distances are calculated, converted to similarity scores, and stored in an  matrix  for each year.",
      "yields a document term frequency matrix for each year. The most frequently used text scaling method in political science is Wordscore BIBREF33 . Word frequencies in the document are used to classify the document into one of two categories. With Wordscore, the learning is supervised using training documents that are known to belong to each of the two categories so that the chosen dimension is substantively defined by the choice of training documents. Here we apply the approach to UNGD statements to calculate levels of disagreement. In Figure  , Wordscore detects in both cases a decline in polarization towards the end of the Cold War and a modest increase after the end of the Cold War. This broadly aligns with expectations in international relations research. Second, we compare this",
      "Debate, together with voting data from the UNGA, these tools were employed to better estimate revealed state preferences in international politics and to locate preference affinity blocs which exist across multiple layers of speech and roll call networks. It is perhaps worth noting that these sources are relatively weak signals of true state preferences and propensity to engage in armed conflict. However, these methods significantly improve our ability to identify meaningful patterns of preference similarity amongst the noise. Furthermore, the approach can assist any political study that seeks to measure position similarities from textual data and detect dense clusters of affinity or antagonism across multiple relational datasets. These might include social media actors who operate across",
      "from conflict onset data from the Correlates of War (COW) project's Militarized Interstate Dispute (MID) dataset (v4.1) BIBREF30 . The time period for the model we replicate spans 1965-2000, whereas our corpus of speeches covers 1970-2014. We limit our extension to the years of overlap, 1970-2000. Our inferential analysis first replicates their full conflict onset model BIBREF8 , which we successfully do. This same model is estimated over the constrained time range of our analysis to ensure that significance and direction of the coefficients on the covariates do not substantially differ. We find that the signs remain the same and the coefficients do not change dramatically. These results are reported in [table:tergm]Table 1 as Model 1 and 2, respectively. We then use speech and multiplex",
      "for full model details, but we describe our approach and decisions here. The model is expressed as:   where  represents parameters,  is the vocabulary size,  and  are column and row word vectors,  is the co-occurrence matrix of all pairs of words that ever co-occur, and  is a weighting function which assigns lower weights to words which frequently co-occur. This lattermost term serves as a cap on very frequent words, for example articles like “the\" which provide little predictive information. The algorithm seeks to minimize the distance between the inner product of the word vectors and the log count of the co-occurrence of the two words. Compared to skip-gram approaches which update at each context window, it is clear from the utilization of  that the model trains relatively quickly since",
      "that the two sources reveal different preference information and that using one over the other risks overlooking available preference signals. Further, for the proposed word embeddings approach to be considered useful, it should provide greater insight into intra-bloc position variation beyond what is available in the BOW, because intra-bloc variation is an important component of the definition of polarization used here, as well as in much of the recent text versus votes literature. The case of NATO and Warsaw Pact members is presented in [fig:diffs]Figure 1, which plots states' ideal points from BIBREF21 , BOW cosine similarities, and RWMD similarities, with the latter two scaled between  . Visual assessment of intra-bloc similarities indicates that the RWMDs based on word embeddings",
      "In order to use texts together with votes to estimate preference polarization, we first consider how to better exploit the information contained in textual data. To do this, we introduce a new representation of textual data which more adequately captures dynamics of human language, namely unsupervised learned word embeddings. In the broader natural language processing (NLP) literature, there has been a surge of research devoted to the development of distributional representations of speech which retain syntactical language qualities in ways that the bag-of-words (BOW) approach typically used in political text analysis research is not equipped to retain. The hypothesis claims that words that occur in similar contexts tend to have similar meanings BIBREF12 . When operationalized, the unique",
      "that weighted word counts provide useful information, but we aim to leverage the information in the speeches in a more sophisticated way and in a way that can increase our confidence that divergence is not simply due to differences in word choice. For this task, we move beyond the BOW approach and use vector space representations. At present, the development of vector space models for textual data is one of the areas that has attracted the most research interest in natural language processing. This interest is motivated by the desire to move away from simple counts and weights of words, towards representations which can preserve word context and linguistic features of human speech. Studies find that these approaches are not only intuitively desirable, but also increase classification",
      "of speech which retain syntactical language qualities in ways that the bag-of-words (BOW) approach typically used in political text analysis research is not equipped to retain. The hypothesis claims that words that occur in similar contexts tend to have similar meanings BIBREF12 . When operationalized, the unique intuition is that similar words and phrases, such as “atomic, weapons” and “nuclear, warheads” are found in relatively proximate vector space locations. Although the BOW performs surprisingly well, this example has no features in common, and a BOW representation would assign low similarity scores or high distances. When results are projected onto a two dimensional space, language relationships surface, such as the clustering of synonyms, antonyms, scales (e.g. democracy to",
      "Supplementary Materials\nThe following material complements the analysis in the main text. To our knowledge, this paper presents the first political science application of vector space representations of textual data, the Word Mover's Distance, and the Multilayer Extraction procedure. The Supplementary Materials are structured as follows. First, the data are discussed and applications which use a bag-of-words treatment are presented. Second, the vector space approach and distance measure are introduced and discussed. Finally, the network clustering approach and model evaluations are presented.",
      "of difference in vector space, as opposed to scalar distances between vectors BIBREF13 , BIBREF14 . In order to validate the quality of the located embeddings, we thus follow current standard practice and assess whether reasonable linear combinations of words can be returned. The main body of the paper presents three examples, and a further example is presented here:   where  describes a vector space location of the given feature and the cosine distance between each vector space location is added or subtracted to find the closest vector space feature (presented on the right hand-side of the equality with the cosine similarity printed underneath). The example is interpreted as: if we add the location of  to  , we arrive at the vector space location of the word  . This, as well as the",
      "patterns of preference similarity amongst the noise. Furthermore, the approach can assist any political study that seeks to measure position similarities from textual data and detect dense clusters of affinity or antagonism across multiple relational datasets. These might include social media actors who operate across multiple platforms, as well as contexts like legislatures where complex relations exist across votes, speeches, committee memberships, and others. Therefore, the approach presented in this paper will be useful to all scholars broadly seeking to measure political preferences and polarization in multidimensional contexts.",
      "roll call polarization in the US Congress BIBREF23 , BIBREF24 , BIBREF25 , as well as the UNGA BIBREF22 . To bridge from roll call data to our speeches, it is instructive to replicate the findings of BIBREF22 using ideal point data based on votes and to compare this to a simple weighted graph based on the cosine similarity of the speeches between states over time. Cosine similarity is well established in the natural language processing literature but has also received recent attention in political science BIBREF38 . This provides a measure of similarity between two vectors of an inner product space which measures the cosine of the angle between them and is expressed as:   where  and  are vectors of attributes (i.e. term frequency vectors), and  and  are components of vectors  and  (i.e.",
      "partitioning approach, the new Multilayer Extraction algorithm, and model performance assessments. For clusters based on speeches alone, we follow Pauls and Cranmer's ( BIBREF8 ) approach through the performance of 5-nearest neighbor clustering on the matrix of RWMD state-state similarities, which yields candidates for membership in an affinity community. As we are dealing with textual data, and in contrast to their sign-test approach, we assign ties between affinity candidate state pairs  based on relatively strict similarity thresholds between 0.50 and 0.60. The result is a square  adjacency matrix  of unweighted ties for each year which contains all states who voted and delivered a GD statement. To locate multiplex blocs, we use the voting clusters found by Pauls and Cranmer ( BIBREF8",
      "scaling, and item-response type models that are intuitive for settings where a small number of parties or ideological divisions influence outcomes. This is less intuitive for the study of state preferences, because international relations is marked by multiple dimensions that span ideological, economic, and security concerns, among others BIBREF0 . This paper introduces a new approach to estimate preference polarization in multidimensional settings using texts and votes. First, a distributional representation of textual data is utilized to retain qualities of human speech that are otherwise discarded by a bag-of-words approach. Second, community detection in multiplex networks is used to uncover preference affinity blocs across multiple layers of votes and speeches. Just as scaling and"
    ]
  },
  {
    "title": "When to reply? Context Sensitive Models to Predict Instructor Interventions in MOOC Forums",
    "answer": "The latent context used to predict instructor intervention is the series of posts that trigger an intervention. This context is inferred using attention models, which are proposed to accommodate different subject areas and instructor intervention strategies. The specific type of latent context is a sequence of posts leading up to an instructor's intervention.",
    "evidence": [
      "subject areas intervene differently. For example, on a Science, Technology, Engineering and Mathematics (STEM) MOOC, instructors may often intervene early as possible to resolve misunderstanding of the subject material and prevent confusion. However, in a Humanities MOOC, instructors allow for the students to explore open-ended discussions and debate among themselves. Such instructors may prefer to intervene later in the discussion to encourage further discussion or resolve conflicts among students. We therefore propose attention models to infer the latent context, i.e., the series of posts that trigger an intervention. Earlier studies on MOOC forum intervention either model the entire context or require the context size to be specified explicitly."
    ]
  },
  {
    "title": "A Hierarchical Model for Data-to-Text Generation",
    "answer": "To accurately answer the question, we need to identify the quantitative improvement of the proposed method's best variant relative to the best variant of the baseline. \n\nThe context provided mentions \"Comparison w.r.t. baselines\" and \"Results,\" indicating that there should be a comparison between the proposed method and the baseline methods. However, the actual results or numbers are not provided in the given context.\n\nTo follow the instructions precisely, we need to directly extract or infer the answer from the given context. Since the specific results or numbers comparing the proposed method to the baseline are not provided, we cannot directly state the quantitative improvement.\n\nTherefore, the answer cannot be determined based on the information given. If the actual results were available, we would calculate the quantitative improvement by comparing the performance metrics (e.g., accuracy, F1 score, etc.) of the best variant of the proposed method to that of the best baseline variant.\n\nFor example, if the context included a statement like \"The proposed method achieved an accuracy of 90%, while the best baseline achieved 85%,\" we could answer: \"The quantitative improvement of the proposed method's best variant over the best baseline variant is 5 percentage points in terms of accuracy.\" \n\nSince the necessary details are missing, a precise numerical answer cannot be",
    "evidence": [
      "Results\nComparison w.r.t. baselines."
    ]
  },
  {
    "title": "Fully Convolutional Speech Recognition",
    "answer": "The current state-of-the-art model on WSJ is an HMM-based system that uses a combination of convolutional, recurrent, and fully connected layers, as well as speaker adaptation. It achieves a certain Word Error Rate (WER) on nov92, although the exact WER value is not specified in the given context. However, it is noted that among end-to-end systems trained only on WSJ, the state-of-the-art uses lattice-free MMI on augmented data and achieves a certain WER, which is the most comparable to the system being discussed.",
    "evidence": [
      "Table  shows Word Error Rates (WER) on WSJ for the current state-of-the-art and our models. The current best model trained on this dataset is an HMM-based system which uses a combination of convolutional, recurrent and fully connected layers, as well as speaker adaptation, and reaches  WER on nov92. DeepSpeech 2 shows a WER of  but uses 150 times more training data for the acoustic model and huge text datasets for LM training. Finally, the state-of-the-art among end-to-end systems trained only on WSJ, and hence the most comparable to our system, uses lattice-free MMI on augmented data (with speed perturbation) and gets  WER. Our baseline system, trained on mel-filterbanks, and decoded with a n-gram language model has a  WER. Replacing the n-gram LM by a convolutional one reduces the WER"
    ]
  },
  {
    "title": "Massively Multilingual Neural Grapheme-to-Phoneme Conversion",
    "answer": "The datasets used were three subsets of the Wiktionary data: LangID-High/NoLangID-High, LangID-Adapted/NoLangID-Adapted, and LangID-All/NoLangID-All, which were trained on data from 85, 157, and 311 languages, respectively.",
    "evidence": [
      "We train the LangID and NoLangID versions of our model each on three subsets of the Wiktionary data: LangID-High and NoLangID-High: Trained on data from the 85 languages for which BIBREF13 used non-adapted wFST models. LangID-Adapted and NoLangID-Adapted: Trained on data from any of the 229 languages for which they built adapted models. Because many of these languages had no training data at all, the model is actually only trained on data in 157 languages. As is noted above, the Adapted set omits 23 languages which are in the High test set. LangID-All and NoLangID-All: Trained on data in all 311 languages in the Wiktionary training corpus. In order to ease comparison to Deri and Knight's system, we limited our use of the training corpus to 10,000 words per language. We set aside 10"
    ]
  },
  {
    "title": "A Neural Approach to Irony Generation",
    "answer": "Four annotators who are proficient in English judged the irony accuracy, sentiment preservation, and content preservation by ranking the output sentences of different models from best to worst.",
    "evidence": [
      "In order to evaluate sentiment preservation, we use the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. We call the value as sentiment delta (senti delta). Besides, we report the sentiment accuracy (Senti ACC) which measures whether the output sentence has the same sentiment polarity as the input sentence based on our standardized sentiment classifiers. The BLEU score BIBREF25 between the input sentences and the output sentences is calculated to evaluate the content preservation performance. In order to evaluate the overall performance of different models, we also report the geometric mean (G2) and harmonic mean (H2) of the sentiment accuracy and the BLEU score. As for the irony accuracy, we only report it",
      "In this section, we describe some additional experiments on the transformation from ironic sentences to non-ironic sentences. Sometimes ironies are hard to understand and may cause misunderstanding, for which our task also explores the transformation from ironic sentences to non-ironic sentences. As shown in Table  , we also conduct automatic evaluations and the conclusions are similar to those of the transformation from non-ironic sentences to ironic sentences. As for human evaluation results in Table  , our model still can achieve the second-best results in sentiment and content preservation. Nevertheless, DualRL system and ours get poor performances in irony accuracy. The reason may be that the other four baselines tend to generate common and even not fluent sentences which are",
      "sentences and their corresponding output sentences of different models. Then, we ask four annotators who are proficient in English to evaluate the qualities of the generated sentences of different models. They are required to rank the output sentences of our model and baselines from the best to the worst in terms of irony accuracy (Irony), Sentiment preservation (Senti) and content preservation (Content). The best output is ranked with 1 and the worst output is ranked with 6. That means that the smaller our human evaluation value is, the better the corresponding model is."
    ]
  },
  {
    "title": "Visual Question: Predicting If a Crowd Will Agree on the Answer",
    "answer": "The model architecture used is a VQA deep learning architecture, which combines visual and textual features. Specifically, it employs a Convolutional Neural Network (CNN), VGG16, to describe the image and a 1024-dimensional LSTM model to encode the question. The image and question features are then combined using element-wise multiplication after a linear transformation of the image features.",
    "evidence": [
      "vote prediction from the ensemble of decision trees. The system returns the final prediction along with a probability indicating the system's confidence in that prediction. We employ the Matlab implementation of random forests, using 25 trees and the default parameters. We next adapt a VQA deep learning architecture BIBREF24 to learn the predictive combination of visual and textual features. The question is encoded with a 1024-dimensional LSTM model that takes in a one-hot descriptor of each word in the question. The image is described with the 4096-dimensional output from the last fully connected layer of the Convolutional Neural Network (CNN), VGG16 BIBREF25 . The system performs an element-wise multiplication of the image and question features, after linearly transforming the image"
    ]
  },
  {
    "title": "From Textual Information Sources to Linked Data in the Agatha Project",
    "answer": "The effectiveness of the pipeline approach is evaluated by comparing its performance with existing systems, such as Rembrandt BIBREF22 and LinguaKit. The comparison shows that the proposed framework outperforms LinguaKit, and Rembrandt BIBREF22 only has the initial steps of the proposed pipeline, until Named Entity Recognition (NER). This suggests that the proposed framework has a more comprehensive and effective processing pipeline for the Portuguese language.",
    "evidence": [
      "We have defined a major design principle for our architecture: it should be modular and not rely on human made rules allowing, as much as possible, its independence from a specific language. In this way, its potential application to another language would be easier, simply by changing the modules or the models of specific modules. In fact, we have explored the use of already existing modules and adopted and integrated several of these tools into our pipeline. It is important to point out that, as far as we know, there is no integrated architecture supporting the full processing pipeline for the Portuguese language. We evaluated several systems like Rembrandt BIBREF22 or LinguaKit: the former only has the initial steps of our proposal (until NER) and the later performed worse than our",
      "Conclusions and Future Work",
      "Acknowledgments\nThe authors would like to thank COMPETE 2020, PORTUGAL 2020 Program, the European Union, and ALENTEJO 2020 for supporting this research as part of Agatha Project SI & IDT number 18022 (Intelligent analysis system of open of sources information for surveillance/crime control). The authors would also like to thank LISP - Laboratory of Informatics, Systems and Parallelism.",
      "to point out that, as far as we know, there is no integrated architecture supporting the full processing pipeline for the Portuguese language. We evaluated several systems like Rembrandt BIBREF22 or LinguaKit: the former only has the initial steps of our proposal (until NER) and the later performed worse than our system. This framework, developed within the context of the Agatha project (described in Section ) has the full processing pipeline for Portuguese texts: it receives sentences as input and outputs ontological information: a) first performs all NLP typical tasks until semantic role labelling; b) then, it extracts subject-verb-object triples; c) and, then, it performs ontology matching procedures. As a final result, the obtained output is inserted into a specialized ontology. We",
      "complex queries and inferences, which can detect patterns of typical criminal actions. Another axe of innovation in this research is the development, for the Portuguese language, of a pipeline of Natural Language Processing (NLP) processes, that allows us to fully process sentences and represent their content in an ontology. Although there are several tools for the processing of the Portuguese language, the combination of all these steps in a integrated tool is a new contribution. Moreover, we have already explored other related research path, namely author profiling BIBREF2, aggression identification BIBREF3 and hate-speech detection BIBREF4 over social media, plus statute law retrieval and entailment for Japanese BIBREF5. The remainder of this paper is organized as follows: Section"
    ]
  },
  {
    "title": "Sparse and Constrained Attention for Neural Machine Translation",
    "answer": "The language pairs explored in this paper are De-En (German-English), Ja-En (Japanese-English), and Ro-En (Romanian-English).",
    "evidence": [
      "We evaluated our attention transformations on three language pairs. We focused on small datasets, as they are the most affected by coverage mistakes. We use the IWSLT 2014 corpus for De-En, the KFTT corpus for Ja-En BIBREF19 , and the WMT 2016 dataset for Ro-En. The training sets have 153,326, 329,882, and 560,767 parallel sentences, respectively. Our reason to prefer smaller datasets is that this regime is what brings more adequacy issues and demands more structural biases, hence it is a good test bed for our methods. We tokenized the data using the Moses scripts and preprocessed it with subword units BIBREF20 with a joint vocabulary and 32k merge operations. Our implementation was done on a fork of the OpenNMT-py toolkit BIBREF21 with the default parameters . We used a validation set to"
    ]
  },
  {
    "title": "Dialog Context Language Modeling with Recurrent Neural Networks",
    "answer": "The dialog history captured is 2 turns previous, as indicated by the connection of the last RNN state of turn $k-2$ to the starting RNN state of turn $k$. This suggests that the model considers the previous two turns when modeling the current turn. The last RNN state of turn $k-1$ is also used as the context vector for turn $k$. Therefore, the model captures a dialog history of 2 previous turns.",
    "evidence": [
      "The previously proposed contextual language models, such as DRNNLM and CCDCLM, treat dialog history as a sequence of inputs, without modeling dialog interactions. A dialog turn from one speaker may not only be a direct response to the other speaker's query, but also likely to be a continuation of his own previous statement. Thus, when modeling turn $k$ in a dialog, we propose to connect the last RNN state of turn $k-2$ directly to the starting RNN state of turn $k$ , instead of letting it to propagate through the RNN for turn $k-1$ . The last RNN state of turn $k-1$ serves as the context vector to turn $k$ , which is fed to turn $k$ 's RNN hidden state at each time step together with the word input. The model architecture is as shown in Figure 2 . The context vector $c$ and the initial"
    ]
  },
  {
    "title": "Shallow Syntax in Deep Water",
    "answer": "The chunk label information is obtained automatically on downstream task data via BIOUL encoding of the labels, after the data is automatically chunked. Specifically, the task training and test data are automatically chunked. The chunk boundary information is then passed into the task model.",
    "evidence": [
      "Shallow Syntactic Features\nOur second approach incorporates shallow syntactic information in downstream tasks via token-level chunk label embeddings. Task training (and test) data is automatically chunked, and chunk boundary information is passed into the task model via BIOUL encoding of the labels. We add randomly initialized chunk label embeddings to task-specific input encoders, which are then fine-tuned for task-specific objectives. This approach does not require a shallow syntactic encoder or chunk annotations for pretraining cwrs, only a chunker. Hence, this can more directly measure the impact of shallow syntax for a given task."
    ]
  },
  {
    "title": "TTTTTackling WinoGrande Schemas",
    "answer": "The previous state of the art is based on RoBERTa, which is characterized as an encoder-only transformer architecture.",
    "evidence": [
      "art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture. Since T5-3B is larger than RoBERTa, it cannot be ruled out that model size alone explains the performance gain. However, when coupled with the observations of Nogueira et al. BIBREF7, T5's “generative capability”, i.e., its ability to generate fluent text, honed through pretraining, seems to play an important role. The fact that the choice of target tokens affects prediction accuracy is consistent with this observation. How and why is the subject of ongoing work.",
      "random. Another interesting finding is that the choice of target token appears to have an impact on performance, which is also consistent with the above work. Since using true/false as the target token (conditions #3 and #4) did not improve performance much over conditions with entailment/contradiction, we did not run all data size conditions given our limited computational resources. Looking at the current WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture. Since T5-3B is larger than RoBERTa, it cannot be ruled out that model size alone explains the performance gain. However, when coupled with the observations of Nogueira et al. BIBREF7, T5's “generative capability”,"
    ]
  },
  {
    "title": "A Question-Entailment Approach to Question Answering",
    "answer": "The machine learning and deep learning methods used for RQE are Logistic Regression and neural networks. Logistic Regression achieved the best performance in question-question similarity at SemEval2017, while neural networks performed well on larger datasets such as SNLI.",
    "evidence": [
      "RQE Approaches and Experiments\nThe choice of two methods for our empirical study is motivated by the best performance achieved by Logistic Regression in question-question similarity at SemEval 2017 (best system BIBREF37 and second best system BIBREF38 ), and the high performance achieved by neural networks on larger datasets such as SNLI BIBREF13 , BIBREF39 , BIBREF40 , BIBREF41 . We first define the RQE task, then present the two approaches, and evaluate their performance on five different datasets."
    ]
  },
  {
    "title": "Argumentation Mining in User-Generated Web Discourse",
    "answer": "The context does not directly address the challenges posed by different registers, but it does discuss the impact of different domains on the task of argument component identification. According to the context, the cross-domain experiments yield \"rather poor results\" for most feature combinations, indicating that domain differences pose a challenge. However, using embeddings-based features (feature set4) improves performance across domains, suggesting that these features generalize well. The main challenges mentioned in the context relate to the task's complexity, such as detecting boundaries of argument components and distinguishing between premise and backing, rather than the registers directly. \n\nSpecifically, the challenges posed by different domains are mitigated by using embeddings-based features, which convey important information when in-domain data are scarce. The primary difficulties lie in properly detecting argument component boundaries and distinguishing between certain argumentative components, such as premise and backing.",
    "evidence": [
      "Topics and registers",
      "to other domains (refer to Table  ). We suspect that embeddings-based features convey important information when not enough in-domain data are available. This observation will become apparent in the next experiment. The cross-domain experiments yield rather poor results for most of the feature combinations (Table  ). However, using only feature set 4 (embeddings), the system performance increases rapidly, so it is even comparable to numbers achieved in the in-domain scenario. These results indicate that embedding features generalize well across domains in our task of argument component identification. We leave investigating better performing vector representations, such as paragraph vectors BIBREF123 , for future work. Error analysis based on the probabilistic confusion matrix BIBREF124",
      "the rebuttal, thus having a similar function to the premise. We manually examined miss-classified examples produced the best-performing system to find out which phenomena pose biggest challenges. Properly detecting boundaries of argument components caused problems, as shown in Figure  (a). This goes in line with the granularity annotation difficulties discussed in section  . The next example in Figure  (b) shows that even if boundaries of components were detected precisely, the distinction between premise and backing fails. The example also shows that in some cases, labeling on clause level is required (left-hand side claim and premise) but the approximation in the system cannot cope with this level of detail (as explained in section  ). Confusing non-argumentative text and argument"
    ]
  },
  {
    "title": "Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model",
    "answer": "To answer this question, we need to understand the context of \"zero-shot transfer model\" and its relation to the \"unseen language dataset.\" \n\nIn the context provided, the question is asking about the learning outcome of a zero-shot transfer model. \n\nThe key information needed is what the model learns when it is applied to an \"unseen language dataset\" in a zero-shot setting.\n\nIn a zero-shot setting, the model is expected to perform a task without being explicitly trained on that specific task or dataset. \n\nTherefore, based on the given context, the model learns to generalize its knowledge to new, unseen languages or tasks.\n\nThe model learns to transfer its knowledge across languages or tasks without requiring direct training on the specific task or language it is being tested on.\n\nSo, the model learns to generalize and apply its understanding to new contexts. \n\nThus, the precise answer is: The model learns to transfer its knowledge across languages or tasks.",
    "evidence": [
      "What Does Zero-shot Transfer Model Learn?\nUnseen Language Dataset"
    ]
  },
  {
    "title": "Automated News Suggestions for Populating Wikipedia Entity Pages",
    "answer": "To answer this question, we need to identify the features used to represent the salience and relative authority of entities based on the provided context.\n\nThe context mentions that the features used include \"Salience\" and \"Relative Authority\". Specifically, it states that \"Salience: entity should be a salient entity in news article\" and defines Relative Authority as a comparison between \"the authority of (which is a measure of popularity of an entity, such as its frequency of mention in a whole corpus) with the authority of its co-occurring entities\".\n\nTherefore, the features used are: \n1. Salience, which checks if an entity is salient in a news article.\n2. Relative Authority, which compares the authority of an entity with that of its co-occurring entities.\n\nThese features are summarized in a table as mentioned in the context. \n\nThe answer is: Salience and Relative Authority.",
    "evidence": [
      "for  . We do this by comparing the authority of  (which is a measure of popularity of an entity, such as its frequency of mention in a whole corpus) with the authority of its co-occurring entities in  , leading to a feature we call relative authority. The intuition is that for an entity that has overall lower authority than its co-occurring entities, a news article is more easily of importance. Finally, if the article we are about to suggest is already covered in the entity profile  , we do not wish to suggest redundant information, hence the novelty. Therefore, the learning objective of  should fulfill the following properties. Table  shows a summary of the computed features for  . Salience: entity  should be a salient entity in news article   Relative Authority: the set of entities"
    ]
  },
  {
    "title": "Using Monolingual Data in Neural Machine Translation: a Systematic Study",
    "answer": "The data is in multiple languages, specifically English, French, and German, as the context mentions experiments on two language pairs: English-German and English-French.",
    "evidence": [
      "Properties of back-translated data",
      "In-domain and out-of-domain data",
      "- a technique that fails to fully take advantage of existing datasets. In this paper, we conduct a systematic study of back-translation, comparing alternative uses of monolingual data, as well as multiple data generation procedures. Our findings confirm that back-translation is very effective and give new explanations as to why this is the case. We also introduce new data simulation techniques that are almost as effective, yet much cheaper to implement.",
      "The literature devoted to the use of monolingual data is large, and quickly expanding. We already alluded to several possible ways to use such data: using back- or forward-translation or using a target language model. The former approach is mostly documented in BIBREF2 , and recently analyzed in BIBREF19 , which focus on fully artificial settings as well as pivot-based artificial data; and BIBREF4 , which studies the effects of increasing the size of BT data. The studies of BIBREF20 , BIBREF19 also consider forward translation and BIBREF21 expand these results to domain adaptation scenarios. Our results are complementary to these earlier studies. As shown above, many alternatives to BT exist. The most obvious is to use target LMs BIBREF3 , BIBREF29 , as we have also done here; but",
      "We are mostly interested with the following training scenario: a large out-of-domain parallel corpus, and limited monolingual in-domain data. We focus here on the Europarl domain, for which we have ample data in several languages, and use as in-domain training data the Europarl corpus BIBREF5 for two translation directions: English  German and English  French. As we study the benefits of monolingual data, most of our experiments only use the target side of this corpus. The rationale for choosing this domain is to (i) to perform large scale comparisons of synthetic and natural parallel corpora; (ii) to study the effect of BT in a well-defined domain-adaptation scenario. For both language pairs, we use the Europarl tests from 2007 and 2008 for evaluation purposes, keeping test 2006 for",
      "monolingual target texts to generate artificial parallel data via backward translation (BT). This technique has since proven effective in many subsequent studies. It is however very computationally costly, typically requiring to translate large sets of data. Determining the “right” amount (and quality) of BT data is another open issue, but we observe that experiments reported in the literature only use a subset of the available monolingual resources. This suggests that standard recipes for BT might be sub-optimal. This paper aims to better understand the strengths and weaknesses of BT and to design more principled techniques to improve its effects. More specifically, we seek to answer the following questions: since there are many ways to generate pseudo parallel corpora, how important is",
      "The new generation of Neural Machine Translation (NMT) systems is known to be extremely data hungry BIBREF0 . Yet, most existing NMT training pipelines fail to fully take advantage of the very large volume of monolingual source and/or parallel data that is often available. Making a better use of data is particularly critical in domain adaptation scenarios, where parallel adaptation data is usually assumed to be small in comparison to out-of-domain parallel data, or to in-domain monolingual texts. This situation sharply contrasts with the previous generation of statistical MT engines BIBREF1 , which could seamlessly integrate very large amounts of non-parallel documents, usually with a large positive effect on translation quality. Such observations have been made repeatedly and have led to",
      "Stupid Back-Translation \nWe now analyze the effect of using much simpler data generation schemes, which do not require the availability of a backward translation engine.",
      "BIBREF3 ? Can BT be efficiently simulated? Does BT data play the same role as a target-side language modeling, or are they complementary? BT is often used for domain adaptation: can the effect of having more in-domain data be sorted out from the mere increase of training material BIBREF2 ? For studies related to the impact of varying the size of BT data, we refer the readers to the recent work of BIBREF4 . To answer these questions, we have reimplemented several strategies to use monolingual data in NMT and have run experiments on two language pairs in a very controlled setting (see §  ). Our main results (see §  and §  ) suggest promising directions for efficient domain adaptation with cheaper techniques than conventional BT.",
      "Comparing the natural and artificial sources of our parallel data wrt. several linguistic and distributional properties, we observe that (see Fig.  -  ): artificial sources are on average shorter than natural ones: when using BT, cases where the source is shorter than the target are rarer; cases when they have the same length are more frequent. automatic word alignments between artificial sources tend to be more monotonic than when using natural sources, as measured by the average Kendall  of source-target alignments BIBREF22 : for French-English the respective numbers are 0.048 (natural) and 0.018 (artificial); for German-English 0.068 and 0.053. Using more monotonic sentence pairs turns out to be a facilitating factor for NMT, as also noted by BIBREF20 . syntactically, artificial",
      "sources are more concentrated; this also translates into both a slower increase of the number of types wrt. the number of sentences and a smaller number of rare events. The intuition is that properties (i) and (ii) should help translation as compared to natural source, while property (iv) should be detrimental. We checked (ii) by building systems with only 10M words from the natural parallel data selecting these data either randomly or based on the regularity of their word alignments. Results in Table  show that the latter is much preferable for the overall performance. This might explain that the mostly monotonic BT from Moses are almost as good as the fluid BT from NMT and that both boost the baseline.",
      "low quality BTs (§  ): BT is only worth its cost when good BT can be generated. Finally, BT seems preferable to integrating external LM - at least in our data condition (§  ). Further experiments with larger LMs are needed to confirm this observation, and also to evaluate the complementarity of both strategies. More work is needed to better understand the impact of BT on subparts of the network (§  ). In future work, we plan to investigate other cheap ways to generate artificial data. The experimental setup we proposed may also benefit from a refining of the data selection strategies to focus on the most useful monolingual sentences.",
      "the average Kendall  of source-target alignments BIBREF22 : for French-English the respective numbers are 0.048 (natural) and 0.018 (artificial); for German-English 0.068 and 0.053. Using more monotonic sentence pairs turns out to be a facilitating factor for NMT, as also noted by BIBREF20 . syntactically, artificial sources are simpler than real data; We observe significant differences in the distributions of tree depths. distributionally, plain word occurrences in artificial sources are more concentrated; this also translates into both a slower increase of the number of types wrt. the number of sentences and a smaller number of rare events. The intuition is that properties (i) and (ii) should help translation as compared to natural source, while property (iv) should be detrimental. We",
      "are learned with 50k merge operations, yielding vocabularies of about respectively 32k and 36k for English  French and 32k and 44k for English  German. Both systems use 512-dimensional word embeddings and a single hidden layer with 1024 cells. They are optimized using Adam BIBREF12 and early stopped according to the validation performance. Training lasted for about three weeks on an Nvidia K80 GPU card. Systems generating back-translated data are trained using the same out-of-domain corpus, where we simply exchange the source and target sides. They are further documented in §  . For the sake of comparison, we also train a system that has access to a large batch of in-domain parallel data following the strategy often referred to as “fine-tuning”: upon convergence of the baseline model, we",
      "The quality of Back-Translation",
      "rationale for choosing this domain is to (i) to perform large scale comparisons of synthetic and natural parallel corpora; (ii) to study the effect of BT in a well-defined domain-adaptation scenario. For both language pairs, we use the Europarl tests from 2007 and 2008 for evaluation purposes, keeping test 2006 for development. When measuring out-of-domain performance, we will use the WMT newstest 2014.",
      "BT on the in-domain tests, at least in terms of BLEU and BEER. On the News domain, we remain close to the baseline level, with slight improvements in German. A first observation is that this method brings stupid BT models closer to conventional BT, at a greatly reduced computational cost. While French still remains 0.4 to 1.0 BLEU below very good backtranslation, both approaches are in the same ballpark for German - may be because BTs are better for the former system than for the latter. Finally note that the GAN architecture has two differences with basic copy-marked: (a) a distinct encoder for real and pseudo-sentence; (b) a different training regime for these encoders. To sort out the effects of (a) and (b), we reproduce the GAN setup with BT sentences, instead of copies. Using a",
      "We observe that the copy setup has only a small impact on the English-French system, for which the baseline is already strong. This is less true for English-German where simple copies yield a significant improvement. Performance drops for both language pairs in the copy-dummies setup. We achieve our best gains with the copy-marked setup, which is the best way to use a copy of the target (although the performance on the out-of-domain tests is at most the same as the baseline). Such gains may look surprising, since the NMT model does not need to learn to translate but only to copy the source. This is indeed what happens: to confirm this, we built a fake test set having identical source and target side (in French). The average cross-entropy for this test set is 0.33, very close to 0, to be"
    ]
  },
  {
    "title": "Combining Advanced Methods in Japanese-Vietnamese Neural Machine Translation",
    "answer": "The Japanese-Vietnamese parallel dataset used is extracted from TED talks in WIT3's corpus BIBREF15, resulting in 106758 pairs of sentences after removing blank and duplicate lines.",
    "evidence": [
      "We collected Japanese-Vietnamese parallel data from TED talks extracted from WIT3's corpus BIBREF15 . After removing blank and duplicate lines we obtained 106758 pairs of sentences. The validation set used in all experiments is dev2010 and the test set is tst2010. The data augmentation methods has been applied only for the Japanese  Vietnamese direction. For Back Translation, we use Vietnamese monolingual data from VNESEcorpus of DongDu which includes 349578 sentences. We shuffle the lines of VNESEcorpus corpus and take out the first 106758 sentences (the same as the number of sentence pairs in the original parallel corpus). For Mix-Source, instead of using a subsampled monolingual corpus, we use the Vietnamese part of the Japanese-Vietnamese parallel corpus in order to learn the"
    ]
  },
  {
    "title": "Ancient-Modern Chinese Translation with a Large Training Dataset",
    "answer": "The context does not explicitly state the origin of the ancient Chinese dataset. However, based on the given information, it can be inferred that the ancient Chinese dataset is part of the \"Ancient-Modern Chinese Dataset\". The exact source or origin is not provided in the context. Therefore, a precise answer cannot be determined from the given information. \n\nHowever, a more accurate response would be: The Ancient-Modern Chinese Dataset is mentioned, but the specific origin or source of the ancient Chinese dataset is not provided in the context.",
    "evidence": [
      "Ancient-Modern Chinese Dataset"
    ]
  },
  {
    "title": "UG18 at SemEval-2018 Task 1: Generating Additional Training Data for Predicting Emotion Intensity in Spanish",
    "answer": "The context does not explicitly state the specific semi-supervised learning method applied. However, it is mentioned that \"a method was applied to automatically assign the tweets to an emotion by comparing our scraped tweets to this new data set.\" This implies that the semi-supervised learning approach involved automatically assigning emotion labels to the tweets in the DISC BIBREF0 corpus, which was then used for training. \n\nTo be more precise, the semi-supervised learning approach used involved utilizing the DISC BIBREF0 corpus, where the specific emotion of the tweets was not known, and automatically assigning emotion labels by comparing the frequencies of certain words with the scraped tweets corpus.",
    "evidence": [
      "Semi-supervised Learning",
      "Table  shows the results on the development set of all individuals models, distinguishing the three types of training: regular (r), translated (t) and semi-supervised (s). In Tables  and  , the letter behind each model (e.g. SVM-r, LSTM-r) corresponds to the type of training used. Comparing the regular and translated columns for the three algorithms, it shows that in 22 out of 30 cases, using translated instances as extra training data resulted in an improvement. For the semi-supervised learning approach, an improvement is found in 15 out of 16 cases. Moreover, our best individual model for each subtask (bolded scores in Table  ) is always either a translated or semi-supervised model. Table  also shows that, in general, our feed-forward network obtained the best results, having the",
      "One of the aims of this study was to see if using semi-supervised learning is beneficial for emotion intensity tasks. For this purpose, the DISC BIBREF0 corpus was used. This corpus was created by querying certain emotion-related words, which makes it very suitable as a semi-supervised corpus. However, the specific emotion the tweet belonged to was not made public. Therefore, a method was applied to automatically assign the tweets to an emotion by comparing our scraped tweets to this new data set. First, in an attempt to obtain the query-terms, we selected the 100 words which occurred most frequently in the DISC corpus, in comparison with their frequencies in our own scraped tweets corpus. Words that were clearly not indicators of emotion were removed. The rest was annotated per emotion"
    ]
  },
  {
    "title": "Conflict as an Inverse of Attention in Sequence Relationship",
    "answer": "The authors test their conflict method on determining whether two given sequences (or pairs of sentences) are duplicates or not, specifically on cases where the pairs are non-duplicate and have very small differences. \n\nThe examples provided in the context, such as \"What are the best ways to learn French ?\" and \"How do I learn french genders ?\", as well as \"How do I prevent breast cancer ?\" and \"Is breast cancer preventable ?\", illustrate this task. \n\nThe task is essentially a duplicate detection or sentence similarity task, where the model is expected to predict whether two sentences are duplicates (or very similar) or not.",
    "evidence": [
      "Conflict model",
      "We also show qualitative results where we can observe that our model with attention and conflict combined does better on cases where pairs are non-duplicate and has very small difference. We have observed that the conflict model is very sensitive to even minor differences and compensates in such cases where attention poses high bias towards similarities already there in the sequences. Sequence 1: What are the best ways to learn French ? Sequence 2: How do I learn french genders ? Attention only: 1 Attention+Conflict: 0 Ground Truth: 0 Sequence 1: How do I prevent breast cancer ? Sequence 2: Is breast cancer preventable ? Attention only: 1 Attention+Conflict: 0 Ground Truth: 0 We provide two examples with predictions from the models with only attention and combination of attention and"
    ]
  },
  {
    "title": "A Comprehensive Comparison of Machine Learning Based Methods Used in Bengali Question Classification",
    "answer": "The ML-based approaches compared were SGD, K-NN, MLP, and SVM. These classifiers were evaluated with and without eliminating stop words while extracting features, and their performance was measured in terms of accuracy and F1 score. The comparison involved analyzing the average results of these different classifiers.",
    "evidence": [
      "Table II shows the accuracy and F1 score for different classifiers with and without eliminating stop words while extracting features. Figure  shows the average results of different classifiers in a bar chart with and without eliminating stop words from the questions. Overall, SGD has shown the best performance on our dataset as it introduces non-linearity and uses back-propagation for updating parameter weights using loss function calculated on training set into classification. K-NN has shown the weakest performance overall, as this algorithm has a bad reputation of not working well in high dimensional data BIBREF28. MLP and SVM have shown similar performance. MLP takes advantage of multiple hidden layers in order to take non-linearly separable samples in a linearly separable condition."
    ]
  },
  {
    "title": "Extractive Summarization of Long Documents by Combining Global and Local Context",
    "answer": "According to the context, \"local context\" refers to the topic segment information of a sentence, while \"global context\" refers to the representation of the whole document. In other words, local context is the contextual information specific to the topic segment that the sentence belongs to, whereas global context is the overall representation of the entire document.",
    "evidence": [
      "correction at (  ). From these tables, we can see that on both datasets the performance significantly improves when local topic information (i.e. local context) is added. And the improvement is even greater when we only consider long documents. Rather surprisingly, this is not the case for the global context. Adding a representation of the whole document (i.e. global context) never significantly improves performance. In essence, it seems that all the benefits of our model come exclusively from modeling the local context, even for the longest documents. Further investigation of this finding is left as future work.",
      "Once we have obtained a representation for the sentence, for its topic segment (i.e., local context) and for the document (i.e., global context), these three factors are combined to make a final prediction  on whether the sentence should be included in the summary. We consider two ways in which these three factors can be combined. Concatenation We can simply concatenate the vectors of these three factors as,   where sentence  is part of the topic  , and  is the representation of sentence  with topic segment information and global context information. Attentive context As local context and global context are all contextual information of the given sentence, we use an attention mechanism to decide the weight of each context vector, represented as   where the  is the weighted context vector"
    ]
  },
  {
    "title": "Performance Comparison of Crowdworkers and NLP Tools onNamed-Entity Recognition and Sentiment Analysis of Political Tweets",
    "answer": "The measure used for evaluation is the Correct Classification Rate (CCR). The CCR is defined as the number of correctly classified instances, and it is used to evaluate the performance of the tools for sentiment analysis, a three-class problem (positive, negative, and neutral).",
    "evidence": [
      "Dataset and Analysis Methodology",
      "Acknowledgments\nPartial support of this work by the Hariri Institute for Computing and Computational Science & Engineering at Boston University (to L.G.) and a Google Faculty Research Award (to M.B. and L.G.) is gratefully acknowledged. Additionally, we would like to thank Daniel Khashabi for his help in running the CogComp-NLP Python API and Mike Thelwal for his help with TensiStrength. We are also grateful to the Stanford NLP group for clarifying some of the questions we had with regards to the Stanford NER tool.",
      "Results and Discussion",
      "sentiment of a sentence overall or a targeted sentiment. Given a list of keywords, TensiStrength (similar to SentiStrength) reports the sentiment towards selected entities in a sentence, based on five levels of relaxation and five levels of stress. Among commercial NLP toolkits (e.g., BIBREF14, BIBREF15, BIBREF16), we selected BIBREF17 and BIBREF18 for our experiments, which, to the best of our knowledge, are the only publicly accessible commercial APIs for the task of entity-level sentiment analysis that is agnostic to the text domain. We also report results of TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, and Stanford NLP NER BIBREF21.",
      "algorithms for sequence tagging use Hidden Markov Models BIBREF7, Maximum Entropy Markov Models BIBREF7, BIBREF8, or Conditional Random Fields BIBREF9. Recent works BIBREF10, BIBREF11 have used recurrent neural networks with attention modules for NER. Sentiment detection tools like SentiStrength BIBREF12 and TensiStrength BIBREF13 are rule-based tools, relying on various dictionaries of emoticons, slangs, idioms, and ironic phrases, and set of rules that can detect the sentiment of a sentence overall or a targeted sentiment. Given a list of keywords, TensiStrength (similar to SentiStrength) reports the sentiment towards selected entities in a sentence, based on five levels of relaxation and five levels of stress. Among commercial NLP toolkits (e.g., BIBREF14, BIBREF15, BIBREF16), we",
      "NLP toolkits typically have the following capabilities: tokenization, part-of-speech (PoS) tagging, chunking, named entity recognition and sentiment analysis. In a study by BIBREF3, it is shown that the well-known NLP toolkits NLTK BIBREF4, Stanford CoreNLP BIBREF5, and TwitterNLP BIBREF6 have tokenization, PoS tagging and NER modules in their pipelines. There are two main approaches for NER: (1) rule-based and (2) statistical or machine learning based. The most ubiquitous algorithms for sequence tagging use Hidden Markov Models BIBREF7, Maximum Entropy Markov Models BIBREF7, BIBREF8, or Conditional Random Fields BIBREF9. Recent works BIBREF10, BIBREF11 have used recurrent neural networks with attention modules for NER. Sentiment detection tools like SentiStrength BIBREF12 and",
      "We measured the CCR, across all tweets, to be 31.7% for Rosette Text Analytics, 43.2% for Google Cloud, 44.2% for TensiStrength, and 74.7% for the crowdworkers. This means the difference between the performance of the tools and the crowdworkers is significant – more than 30 percent points. Crowdworkers correctly identified 62% of the neutral, 85% of the positive, and 92% of the negative sentiments. Google Cloud correctly identified 88% of the neutral sentiments, but only 3% of the positive, and 19% of the negative sentiments. TensiStrength correctly identified 87.2% of the neutral sentiments, but 10.5% of the positive, and 8.1% of the negative sentiments. Rosette Text Analytics correctly identified 22.7% of neutral sentiments, 38.1% of negative sentiments and 40.9% of positive sentiments.",
      "and BIBREF26, BIBREF18 for named-entity recognition. In the experiments that we conducted with TwitterNLP for named-entity recognition, we worked with the default values of the model. Furthermore, we selected the 3-class Stanford NER model, which uses the classes “person,” “organization,” and “location” because it resulted in higher accuracy compared to the 7-class model. For CogComp-NLP NER we used Ontonotes 5.0 NER model BIBREF27. For spaCy NER we used the `en_core_web_lg' model. We report the experimental results for our two tasks in terms of the correct classification rate (CCR). For sentiment analysis, we have a three-class problem (positive, negative, and neutral), where the classes are mutually exclusive. The CCR, averaged for a set of tweets, is defined to be the number of"
    ]
  },
  {
    "title": "Creative GANs for generating poems, lyrics, and metaphors",
    "answer": "The datasets used are: \n1. A corpus of 740 classical and contemporary English poems, \n2. a corpus of 14,950 metaphor sentences retrieved from a metaphor database website, \n3. a corpus of 1,500 song lyrics ranging across genres, \nand the Gutenberg dataset.",
    "evidence": [
      "GumbelGAN model BIBREF14 across all proposed datasets. We use three creative English datasets with distinct linguistic characteristics: (1) A corpus of 740 classical and contemporary English poems, (2) a corpus of 14950 metaphor sentences retrieved from a metaphor database website and (3) a corpus of 1500 song lyrics ranging across genres. The mix of linguistic styles within this corpus offers the potential for interesting variation during the generation phase. We use the same pre-processing as in earlier work BIBREF19, BIBREF23. We reserve 10% of our data for test set and another 10% for our validation set. We first pre-train our generator on the Gutenberg dataset BIBREF24 for 20 epochs and then fine-tune BIBREF19 them to our target datasets with a language modeling objective. The"
    ]
  },
  {
    "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text",
    "answer": "The context does not explicitly state what the baselines are. However, it mentions that \"We also implement several variations to reveal the effects of features: authors, likers, comment, and commenters. In the results section we compare our model with related work.\" This suggests that the baselines are related to the \"related work\" mentioned, but the exact baselines are not specified in the given context. To provide a more accurate answer, more context or information about the specific baselines being referred to would be necessary. \n\nHowever, based on the given text, a more precise answer can be derived from the sentence: \"In the results section we compare our model with related work.\" This implies that the baselines are the \"related work\" to which the authors are comparing their model. \n\nA more direct answer is not possible with the given information.",
    "evidence": [
      "Experiment\nWe start with the experimental dataset and then describe the training process as well as the implementation of the baselines. We also implement several variations to reveal the effects of features: authors, likers, comment, and commenters. In the results section we compare our model with related work.",
      "in our approach. The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR). The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The labeling results are shown in the right-hand side of Table  . We observe that the dataset is more balanced than the FBFans dataset. In addition, there are 977 unique users in the dataset. To compare with Hasan and Ng's work, we conducted five-fold cross-validation and present the annotation results as the average number of all folds BIBREF9 , BIBREF5 . The FBFans dataset has more integrated functions than the CreateDebate dataset; thus our model",
      "standpoint on the topic, or were irrelevant. Raw agreement between annotators is 0.91, indicating high agreement. Specifically, Cohen’s Kappa for Neu and not Neu labeling is 0.58 (moderate), and for Sup or Uns labeling is 0.84 (almost perfect). Posts with inconsistent labels were filtered out, and the development and testing sets were randomly selected from what was left. Posts in the development and testing sets involved at least one user who appeared in the training set. The number of posts for each stance is shown on the left-hand side of Table  . About twenty percent of the posts were labeled with a stance, and the number of supportive (Sup) posts was much larger than that of the unsupportive (Uns) ones: this is thus highly skewed data, which complicates stance classification. On"
    ]
  },
  {
    "title": "Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual Lexical Semantic Similarity",
    "answer": "The datasets were annotated differently: some were translated from existing datasets (e.g., SimLex-999) and some were created from scratch in the target languages, with varying procedures for translation and annotation. Specifically, some datasets were translated and reannotated in target languages, while others were sampled from scratch or directly translated from English data without reannotation. The annotation process was not unified, resulting in heterogeneous datasets that are incomparable.",
    "evidence": [
      "but they did not provide details concerning the translation process and the resolution of translation disagreements. More importantly, they also did not reannotate the translated pairs in the target languages. As we discussed in §  and reiterate later in §, semantic differences among languages can have a profound impact on the annotation scores; particulary, we show in § that these differences even roughly define language clusters based on language affinity. A core issue with the current datasets concerns a lack of one unified procedure that ensures the comparability of resources in different languages. Further, concept pairs for different languages are sourced from different corpora (e.g., direct translation of the English data versus sampling from scratch in the target language).",
      "Multi-SimLex: Translation and Annotation\nData Analysis",
      "in its entirety BIBREF25, BIBREF16 or in part BIBREF28. Other datasets were created from scratch BIBREF26 and yet others sampled English concept pairs differently from SimLex-999 and then translated and reannotated them in target languages BIBREF27. This heterogeneity makes these datasets incomparable and precludes systematic cross-linguistic analyses. In this article, consolidating the lessons learned from previous dataset construction paradigms, we propose a carefully designed translation and annotation protocol for developing monolingual Multi-SimLex datasets with aligned concept pairs for typologically diverse languages. We apply this protocol to a set of 12 languages, including a mixture of major languages (e.g., Mandarin, Russian, and French) as well as several low-resource ones"
    ]
  },
  {
    "title": "Multi-attention Recurrent Network for Human Communication Comprehension",
    "answer": "The Long-short Term Hybrid Memory (LSTHM) is an extension of the Long-short Term Memory (LSTM) networks. The primary difference between LSTHM and LSTM lies in their memory mechanisms. LSTHM is designed to store both view-specific dynamics of its assigned modality and cross-view dynamics related to its assigned modality, whereas LSTM has a standard memory that stores a representation of its input through time. In other words, LSTHM has a hybrid memory that allows it to capture both modality-specific information and interactions between different modalities, making it more suitable for multimodal setups.",
    "evidence": [
      "Long-short Term Hybrid Memory",
      "Long-short Term Memory (LSTM) networks have been among the most successful models in learning from sequential data BIBREF21 . The most important component of the LSTM is a memory which stores a representation of its input through time. In the LSTHM model, we seek to build a memory mechanism for each modality which in addition to storing view-specific dynamics, is also able to store the cross-view dynamics that are important for that modality. This allows the memory to function in a hybrid manner. The Long-short Term Hybrid Memory is formulated in Algorithm 1. Given a set of $M$ modalities in the domain of the data, subsequently $M$ LSTHMs are built in the MARN pipeline. For each modality $m \\in M$ , the input to the $m$ th LSTHM is of the form $\\mathbf {X}^m=\\lbrace {x}_{1}^m, {x}_{2}^m,",
      "in a hybrid manner. The Long-short Term Hybrid Memory is formulated in Algorithm 1. Given a set of $M$ modalities in the domain of the data, subsequently $M$ LSTHMs are built in the MARN pipeline. For each modality $m \\in M$ , the input to the $m$ th LSTHM is of the form $\\mathbf {X}^m=\\lbrace {x}_{1}^m, {x}_{2}^m, {x}_{3}^m, \\cdots , {x}_{T}^m \\ ; {x}_{t}^m \\in \\mathbb {R}^{d_{in}^m} \\rbrace $ , where ${x}^m_{t}$ is the input at time $t$ and $d^m_{in}$ is the dimensionality of the input of modality $m$ . For example if $m=l \\textrm {(language)}$ , we can use word vectors with $M$0 at each time step $M$1 . $M$2 is the dimensionality of the memory for modality $M$3 . $M$4 is the (hard-)sigmoid activation function and $M$5 is the tangent hyperbolic activation function. $M$6 denotes vector",
      "and learns a neural cross-view dynamics code for LSTHMs to update their hybrid memories. Figure 1 shows the overview of the MARN. MARN is differentiable end-to-end which allows the model to be learned efficiently using gradient decent approaches. In the next subsection, we first outline the Long-short Term Hybrid Memory. We then proceed to outline the Multi-attention Block and describe how the two components are integrated in the MARN.",
      "In this section we outline our pipeline for human communication comprehension: the Multi-attention Recurrent Network (MARN). MARN has two key components: Long-short Term Hybrid Memory and Multi-attention Block. Long-short Term Hybrid Memory (LSTHM) is an extension of the Long-short Term Memory (LSTM) by reformulating the memory component to carry hybrid information. LSTHM is intrinsically designed for multimodal setups and each modality is assigned a unique LSTHM. LSTHM has a hybrid memory that stores view-specific dynamics of its assigned modality and cross-view dynamics related to its assigned modality. The component that discovers cross-view dynamics across different modalities is called the Multi-attention Block (MAB). The MAB first uses information from hidden states of all LSTHMs at",
      "output of each modality network. EF-LSTM (Early Fusion LSTM) concatenates the inputs from different modalities at each time-step and uses that as the input to a single LSTM. We also implement the Stacked, (EF-SLSTM) Bidirectional (EF-BLSTM) and Stacked Bidirectional (EF-SBLSTM) LSTMs for stronger baselines. The best performing model is reported as EF-LSTM $_{(\\star )}$ , $\\star \\in \\lbrace  \\textrm {-, s, b, sb}\\rbrace $ denoting vanilla, stacked, bidirectional and stacked bidirectional LSTMs respectively. Majority performs majority voting for classification tasks, and predicts the expected label for regression tasks. This baseline is useful as a lower bound of model performance. Human performance is calculated for CMU-MOSI dataset which offers per annotator results. This is the accuracy",
      "the $K$ attention coefficients, $K$ softmax distributions are assigned to the concatenated LSTHM memories using a deep neural network $\\mathcal {A} : \\mathbb {R}^{{d_{mem}}} \\mapsto \\mathbb {R}^{K \\times {d_{mem}}}$ with ${d_{mem}} = \\sum _{m \\in M} {d^{m}_{mem}}$ . At each timestep $t$ , the output of LSTHM is the set $\\lbrace h^m_t : m \\in M, h^m_t \\in \\mathbb {R}^{{d^m_{mem}}}\\rbrace $ . $h^m_t$0 takes the concatenation of LSTHM outputs $h^m_t$1 as input and outputs a set of $h^m_t$2 attentions $h^m_t$3 with $h^m_t$4 , $h^m_t$5 . $h^m_t$6 has a softmax layer at top of the network which takes the softmax activation along each one of the $h^m_t$7 dimensions of its output $h^m_t$8 . As a result, $h^m_t$9 which forms a probability distribution over the output dimensions. $K$0 is then",
      "hybrid memory that stores view-specific dynamics of its assigned modality and cross-view dynamics related to its assigned modality. The component that discovers cross-view dynamics across different modalities is called the Multi-attention Block (MAB). The MAB first uses information from hidden states of all LSTHMs at a timestep to regress coefficients to outline the multiple existing cross-view dynamics among them. It then weights the output dimensions based on these coefficients and learns a neural cross-view dynamics code for LSTHMs to update their hybrid memories. Figure 1 shows the overview of the MARN. MARN is differentiable end-to-end which allows the model to be learned efficiently using gradient decent approaches. In the next subsection, we first outline the Long-short Term Hybrid",
      "dynamics that it finds related to its modality. The set of weights $W^m_*$ , $U^m_*$ and $V^m_*$ respectively map the input of LSTHM $x^m_t$ , output of LSTHM $h^m_t$ , and neural cross-view dynamics code $z_{t}$ to each LSTHM memory space using affine transformations. [b!] Multi-attention Recurrent Network (MARN), Long-short Term Hybrid Memory (LSTHM) and Multi-attention Block (MAB) Formulation [1] $\\textrm {MARN}$ $\\mathbf {X}^m$ $c_0,h_0,z_0 \\leftarrow \\mathbf {0}$ $t = 1, ..., T$ : $h_t \\leftarrow \\textrm {LSTHM\\_Step} (\\bigcup _{m \\in M} \\lbrace  x^m_t\\rbrace , z_{t-1})$ $z_t \\leftarrow \\textrm {MAB\\_Step} (h_t)$ $h_T, z_T$   $\\textrm {LSTHM\\_Step}$ $\\bigcup _{m \\in M} \\lbrace {x}^m_t\\rbrace $ , $z_{t-1}$ $m \\in M$ : $\\triangleleft $ for all the $M$ modalities $i_t^m \\leftarrow",
      "paper, we present a novel neural architecture for understanding human communication called the Multi-attention Recurrent Network (MARN). The main strength of our model comes from discovering interactions between modalities through time using a neural component called the Multi-attention Block (MAB) and storing them in the hybrid memory of a recurrent component called the Long-short Term Hybrid Memory (LSTHM). We perform extensive comparisons on six publicly available datasets for multimodal sentiment analysis, speaker trait recognition and emotion recognition. MARN shows state-of-the-art performance on all the datasets.",
      "gate. $m \\in M$1 is the proposed update to the hybrid memory $m \\in M$2 at time $m \\in M$3 . $m \\in M$4 is the time distributed output of each modality. The neural cross-view dynamics code $z_{t}$ is the output of the Multi-attention Block at the previous time-step and is discussed in detail in next subsection. This neural cross-view dynamics code $z_{t}$ is passed to each of the individual LSTHMs and is the hybrid factor, allowing each individual LSTHM to carry cross-view dynamics that it finds related to its modality. The set of weights $W^m_*$ , $U^m_*$ and $V^m_*$ respectively map the input of LSTHM $x^m_t$ , output of LSTHM $h^m_t$ , and neural cross-view dynamics code $z_{t}$ to each LSTHM memory space using affine transformations. [b!] Multi-attention Recurrent Network (MARN),",
      "of the input of modality $m$ . For example if $m=l \\textrm {(language)}$ , we can use word vectors with $M$0 at each time step $M$1 . $M$2 is the dimensionality of the memory for modality $M$3 . $M$4 is the (hard-)sigmoid activation function and $M$5 is the tangent hyperbolic activation function. $M$6 denotes vector concatenation and $M$7 denotes element-wise multiplication. Similar to the LSTM, $M$8 is the input gate, $M$9 is the forget gate, and $m \\in M$0 is the output gate. $m \\in M$1 is the proposed update to the hybrid memory $m \\in M$2 at time $m \\in M$3 . $m \\in M$4 is the time distributed output of each modality. The neural cross-view dynamics code $z_{t}$ is the output of the Multi-attention Block at the previous time-step and is discussed in detail in next subsection. This",
      "Conclusion\nIn this paper we modeled multimodal human communication using a novel neural approach called the Multi-attention Recurrent Network (MARN). Our approach is designed to model both view-specific dynamics as well as cross-view dynamics continuously through time. View-specific dynamics are modeled using a Long-short Term Hybrid Memory (LSTHM) for each modality. Various cross-view dynamics are identified at each time-step using the Multi-attention Block (MAB) which outputs a multimodal neural code for the hybrid memory of LSTHM. MARN achieves state-of-the-art results in 6 publicly available datasets and across 16 different attributes related to understanding human communication.",
      "5) MV-HSSHCRF: further extends EF-HSSHCRF by performing Multi-view hierarchical sequence summary representation. The best performing early fusion model is reported as EF-HCRF $_{(\\star )}$ while the best multi-view model is reported as MV-HCRF $_{(\\star )}$ , where $\\star \\in \\lbrace  \\textrm {h, l, s}\\rbrace $ to represent HCRF, LDCRF and HSSCRF respectively. DF (Deep Fusion) BIBREF14 is a model that trains one deep model for each modality and performs decision voting on the output of each modality network. EF-LSTM (Early Fusion LSTM) concatenates the inputs from different modalities at each time-step and uses that as the input to a single LSTM. We also implement the Stacked, (EF-SLSTM) Bidirectional (EF-BLSTM) and Stacked Bidirectional (EF-SBLSTM) LSTMs for stronger baselines. The best",
      "Fields BIBREF16 , BIBREF17 have been proposed for learning from multiple different views (modalities) BIBREF18 , BIBREF19 . Extensions of LSTMs have also been proposed in a multi-view setting BIBREF20 . MARN is different from the first category since we model both view-specific and cross-view dynamics. It is differs from the second and third category since we explicitly model view-specific dynamics using a LSTHM for each modality as well as cross-view dynamics using the MAB. Finally, MARN is different from the fourth category since it explicitly models view-specific dynamics and proposes more advanced temporal modeling of cross-view dynamics.",
      "from previous approaches in that it explicitly accounts for both view-specific and cross-view dynamics in the network architecture and continuously models both dynamics through time. In MARN, view-specific dynamics within each modality are modeled using a Long-short Term Hybrid Memory (LSTHM) assigned to that modality. The hybrid memory allows each modality's LSTHM to store important cross-view dynamics related to that modality. Cross-view dynamics are discovered at each recurrence time-step using a specific neural component called the Multi-attention Block (MAB). The MAB is capable of simultaneously finding multiple cross-view dynamics in each recurrence timestep. The MARN resembles the mechanism of our brains for understanding communication, where different regions independently process",
      "We compare the performance of our MARN to the following state-of-the-art models in multimodal sentiment analysis, speaker trait recognition, and emotion recognition. All baselines are trained for datasets for complete comparison. TFN (Tensor Fusion Network) BIBREF1 explicitly models view-specific and cross-view dynamics by creating a multi-dimensional tensor that captures unimodal, bimodal and trimodal interactions across three modalities. It is the current state of the art for CMU-MOSI dataset. BC-LSTM (Bidirectional Contextual LSTM) BIBREF5 is a model for context-dependent sentiment analysis and emotion recognition, currently state of the art on the IEMOCAP and MOUD datasets. MV-LSTM (Multi-View LSTM) BIBREF20 is a recurrent model that designates special regions inside one LSTM to",
      "they have shortcomings for cross-view dynamics since these inter-modality dynamics are normally more complex than a decision vote. As an example of this shortcoming, if a model is trained for sentiment analysis using the vision modality and predicts negative sentiment, late fusion models have no access to whether this negative sentiment was due to a frowning face or a disgusted face. Multi-view Learning: Extensions of Hidden Markov Models BIBREF15 and Hidden Conditional Random Fields BIBREF16 , BIBREF17 have been proposed for learning from multiple different views (modalities) BIBREF18 , BIBREF19 . Extensions of LSTMs have also been proposed in a multi-view setting BIBREF20 . MARN is different from the first category since we model both view-specific and cross-view dynamics. It is differs",
      "LSTMs respectively. Majority performs majority voting for classification tasks, and predicts the expected label for regression tasks. This baseline is useful as a lower bound of model performance. Human performance is calculated for CMU-MOSI dataset which offers per annotator results. This is the accuracy of human performance in a one-vs-rest classification/regression. Finally, MARN indicates our proposed model. Additionally, the modified baseline MARN (no MAB) removes the MAB and learns no dense cross-view dynamics code $z$ . This model can be seen as three disjoint LSTMs and is used to investigate the importance of modeling temporal cross-view dynamics. The next modified baseline MARN (no $\\mathcal {A}$ ) removes the $\\mathcal {A}$ deep network and sets all $K$ attention coefficients",
      "art for CMU-MOSI dataset. BC-LSTM (Bidirectional Contextual LSTM) BIBREF5 is a model for context-dependent sentiment analysis and emotion recognition, currently state of the art on the IEMOCAP and MOUD datasets. MV-LSTM (Multi-View LSTM) BIBREF20 is a recurrent model that designates special regions inside one LSTM to different views of the data. C-MKL (Convolutional Neural Network (CNN) with Multiple Kernel Learning) BIBREF29 is a model which uses a CNN for visual feature extraction and multiple kernel learning for prediction. THMM (Tri-modal Hidden Markov Model) BIBREF0 performs early fusion of the modalities by concatenation and uses a HMM for classification. SVM (Support Vector Machine) BIBREF30 a SVM is trained on the concatenated multimodal features for classification or regression",
      "MAB and learns no dense cross-view dynamics code $z$ . This model can be seen as three disjoint LSTMs and is used to investigate the importance of modeling temporal cross-view dynamics. The next modified baseline MARN (no $\\mathcal {A}$ ) removes the $\\mathcal {A}$ deep network and sets all $K$ attention coefficients $a^k_t = 1$ ( $h^k_t = \\tilde{h}^k_t$ ). This comparison shows whether explicitly outlining the cross-view dynamics using the attention coefficients is required. For MARN and MARN (no $\\mathcal {A}$ ), $K$ is treated as a hyperparamter and the best value of $K$ is indicated in parenthesis next to the best reported result.",
      "Human face-to-face communication is a complex multimodal signal. We use words (language modality), gestures (vision modality) and changes in tone (acoustic modality) to convey our intentions. Humans easily process and understand face-to-face communication, however, comprehending this form of communication remains a significant challenge for Artificial Intelligence (AI). AI must understand each modality and the interactions between them that shape human communication. In this paper, we present a novel neural architecture for understanding human communication called the Multi-attention Recurrent Network (MARN). The main strength of our model comes from discovering interactions between modalities through time using a neural component called the Multi-attention Block (MAB) and storing them in",
      "or not they form cross-view dynamics. These attention coefficients will be high if the dimension contributes to formation of a cross-view dynamics and low if they are irrelevant. The coefficient assignment is performed multiple times due to the existence of possibly multiple such cross-view dynamics across the outputs of LSTHM. The Multi-attention Block is formulated in Algorithm 1. We assume a maximum of $K$ cross-view dynamics to be present at each timestamp $t$ . To obtain the $K$ attention coefficients, $K$ softmax distributions are assigned to the concatenated LSTHM memories using a deep neural network $\\mathcal {A} : \\mathbb {R}^{{d_{mem}}} \\mapsto \\mathbb {R}^{K \\times {d_{mem}}}$ with ${d_{mem}} = \\sum _{m \\in M} {d^{m}_{mem}}$ . At each timestep $t$ , the output of LSTHM is the",
      "is the simultaneous co-occurrence of a smile with a positive sentence and an example of asynchronous cross-view dynamics is the delayed occurrence of a laughter after the end of sentence. For machines to understand human communication, they must be able to understand these view-specific and cross-view dynamics. To model these dual dynamics in human communication, we propose a novel deep recurrent neural model called the Multi-attention Recurrent Network (MARN). MARN is distinguishable from previous approaches in that it explicitly accounts for both view-specific and cross-view dynamics in the network architecture and continuously models both dynamics through time. In MARN, view-specific dynamics within each modality are modeled using a Long-short Term Hybrid Memory (LSTHM) assigned to",
      "view-specific and cross-view dynamics explicitly. In other words, these approaches rely on generic models (such as Support Vector Machines or deep neural networks) to learn both view-specific and cross-view dynamics without any specific model design. This concatenation technique is known as early fusion BIBREF9 , BIBREF10 . Often, these early fusion approaches remove the time factor as well BIBREF11 , BIBREF0 . We additionally compare to a stronger recurrent baseline that uses early fusion while maintaining the factor of time. A shortcoming of these models is the lack of detailed modeling for view-specific dynamics, which in turn affects the modeling of cross-view dynamics, as well as causing overfitting on input data BIBREF12 . Late Fusion: Late fusion methods learn different models for",
      ". While these models are successful in understanding co-occurrences, the lack of temporal modeling is a major flaw as these models cannot deal with multiple contradictory evidences, eg. if a smile and frown happen together in an utterance. Furthermore, these approaches cannot accurately model long sequences since the representation over long periods of time become less informative. Early Fusion: Approaches have used multimodal input feature concatenation instead of modeling view-specific and cross-view dynamics explicitly. In other words, these approaches rely on generic models (such as Support Vector Machines or deep neural networks) to learn both view-specific and cross-view dynamics without any specific model design. This concatenation technique is known as early fusion BIBREF9 ,",
      "and multiple kernel learning for prediction. THMM (Tri-modal Hidden Markov Model) BIBREF0 performs early fusion of the modalities by concatenation and uses a HMM for classification. SVM (Support Vector Machine) BIBREF30 a SVM is trained on the concatenated multimodal features for classification or regression BIBREF11 , BIBREF22 , BIBREF23 . To compare to another strong non-neural baseline we use RF (Random Forest) BIBREF31 using similar multimodal inputs. SAL-CNN (Selective Additive Learning CNN) BIBREF9 is a model that attempts to prevent identity-dependent information from being learned by using Gaussian corruption introduced to the neuron outputs. EF-HCRF: (Hidden Conditional Random Field) BIBREF16 uses a HCRF to learn a set of latent variables conditioned on the concatenated input at",
      "In this paper we benchmark MARN's understanding of human communication on three tasks: 1) multimodal sentiment analysis, 2) multimodal speaker traits recognition and 3) multimodal emotion recognition. We perform experimentations on six publicly available datasets and compare the performance of MARN with the performance of state-of-the-art approaches on the same datasets. To ensure generalization of the model, all the datasets are split into train, validation and test sets that include no identical speakers between sets, i.e. all the speakers in the test set are different from the train and validation sets. All models are re-trained on the same train/validation/test splits. To train the MARN for different tasks, the final outputs $h_T$ and neural cross-view dynamics code $z_T$ are the",
      "among modalities such as decision voting or simple co-occurrence rules such as Tensor Fusion baseline. Across all datasets, MARN (no MAB) is outperformed by MARN. This indicates that continuous modeling of cross-view dynamics is crucial in understanding human communication. RQ2: Whether or not the presence of the coefficients $a_t$ are crucial is an important research question. From the results tables, we notice that the MARN (no $\\mathcal {A}$ ) baseline severely under-performs compared to MARN. This supports the importance of the attentions in the MAB. Without these attentions, MARN is not able to accurately model the cross-view dynamics. RQ3: In our experiments the MARN with only one attention (like conventional attention models) under-performs compared to the models with multiple",
      "We visually display how each attention is sensitive to different dimensions of LSTHM outputs in Figure 3 . Each column of the figure denoted by $a^k$ shows the behavior of the $k$ th attention on a sample video from CMU-MOSI. The left side of $a^k$ is $t=1$ and the right side is $t=20$ , since the sequence has 20 words. The $y$ axis shows what modality the dimension belongs to. Dark blue means high coefficients and red means low coefficients. Our observations (O) are detailed below: O1: By comparing each of the attentions together, they show diversity on which dimensions they are sensitive to, indicating that each attention is sensitive to different cross-view dynamics. O2: Some attention coefficients are not active (always red) throughout time. These dimensions carry only view-specific"
    ]
  },
  {
    "title": "Construction of a Japanese Word Similarity Dataset",
    "answer": "The data used to construct the Japanese word similarity dataset comes from the Evaluation Dataset of Japanese Lexical Simplification. This dataset targeted content words, including nouns, verbs, adjectives, and adverbs, and contained 10 contexts about target words annotated with their lexical substitutions and rankings.",
    "evidence": [
      "Comparison to Other Datasets",
      "Table  shows how several resources vary. WordSim353 comprises high-frequency words and so the variance tends to be low. In contrast, RW includes low-frequency words, unknown words, and complex words composed of several morphemes; thus, the variance is large. VSD has many polysemous words, which increase the variance. Despite the fact that our dataset, similar to the VSD and RW datasets, contains low-frequency and ambiguous words, its variance is 3.00. The variance level is low compared with the other corpora. We considered that the examples of the similarity in the task request reduced the variance level. We did not expect SCWS to have the largest variance in the datasets shown in Table  because it gave the context to annotators during annotation. At the beginning, we thought the context",
      "Error analysis revealed that the notion of similarity should be carefully defined when constructing a similarity dataset. As a future work, we plan to construct a word analogy dataset in Japanese by translating an English dataset to Japanese. We hope that a Japanese database will facilitate research in Japanese distributed representations.",
      "compared with the other corpora. We considered that the examples of the similarity in the task request reduced the variance level. We did not expect SCWS to have the largest variance in the datasets shown in Table  because it gave the context to annotators during annotation. At the beginning, we thought the context would serve to remove the ambiguity and clarify the meaning of word; however after looking into the dataset, we determined that the construction procedure used several extraordinary annotators. It is crucial to filter insincere annotators and provide straightforward instructions to improve the quality of the similarity annotation like we did. To gain better similarity, each dataset should utilize the reliability score to exclude extraordinary annotators. For example, for SCWS,",
      "Traditionally, a word is represented as a sparse vector indicating the word itself (one-hot vector) or the context of the word (distributional vector). However, both the one-hot notation and distributional notation suffer from data sparseness since dimensions of the word vector do not interact with each other. Distributed word representation addresses the data sparseness problem by constructing a dense vector of a fixed length, wherein contexts are shared (or distributed) across dimensions. Distributed word representation is known to improve the performance of many NLP applications such as machine translation BIBREF0 and sentiment analysis BIBREF1 to name a few. The task to learn a distributed representation is called representation learning. However, evaluating the quality of learned",
      "Related Work",
      "What makes a pair of words similar? Most of the previous datasets do not concretely define the similarity of word pairs. The difference in the similarity of word pairs originates from each annotator's mind, resulting in different scales of a word. Thus, we propose to use an example-based approach (Table  ) to control the variance of the similarity ratings. We remove the context of word when we extracted the word. So, we consider that an ambiguous word has high variance of the similarity, but we can get low variance of the similarity when the word is monosemous. For this study, we constructed a Japanese word similarity dataset. We followed the procedure used to construct the Stanford Rare Word Similarity Dataset (RW) Luong-etal:conll13:morpho. We extracted Japanese word pairs from the",
      "extraordinary annotators. It is crucial to filter insincere annotators and provide straightforward instructions to improve the quality of the similarity annotation like we did. To gain better similarity, each dataset should utilize the reliability score to exclude extraordinary annotators. For example, for SCWS, an annotator rating the similarity of pair of “CD” and “aglow” assigned a rating of 10. We assumed it was a typo or misunderstanding regarding the words. To address this problem, such an annotation should be removed before calculating the true similarity. All the datasets except for RW simply calculated the average of the similarity, but datasets created using crowdsourcing should consider the reliability of the annotator.",
      "In this study, we constructed the first Japanese word similarity dataset. It contains various parts of speech and includes rare words in addition to common words. Crowdsourced annotators assigned similarity to word pairs during the word similarity task. We gave examples of similarity in the task request sent to annotators, so that we reduced the variance of each word pair. However, we did not restrict the attributes of words, such as the level of feeling, during annotation. Error analysis revealed that the notion of similarity should be carefully defined when constructing a similarity dataset. As a future work, we plan to construct a word analogy dataset in Japanese by translating an English dataset to Japanese. We hope that a Japanese database will facilitate research in Japanese",
      "across dimensions. Distributed word representation is known to improve the performance of many NLP applications such as machine translation BIBREF0 and sentiment analysis BIBREF1 to name a few. The task to learn a distributed representation is called representation learning. However, evaluating the quality of learned distributed word representation itself is not straightforward. In language modeling, perplexity or cross-entropy is widely accepted as a de facto standard for intrinsic evaluation. In contrast, distributed word representations include the additive (or compositional) property of the vectors, which cannot be assessed by perplexity. Moreover, perplexity makes little use of infrequent words; thus, it is not appropriate for evaluating distributed presentations that try to",
      "words annotated with their lexical substitutions and rankings. Figure  shows an example of the dataset. A word in square brackets in the text is represented as a target word of simplification. A target word is not only recorded in the lemma form but also in the conjugated form. We built a Japanese similarity dataset from this dataset using the following procedure.",
      "SimLex-999 has also been translated and rescored in Hebrew and Croatian Mrksic:2017:TACL. SimLex-999 explicitly targets at similarity rather than relatedness and includes adjective, noun and verb pairs. However, this dataset contains only frequent words. In addition, the distributed representation of words is generally learned using only word-level information. Consequently, the distributed representation for low-frequency words and unknown words cannot be learned well with conventional models. However, low-frequency words and unknown words are often comprise high-frequency morphemes (e.g., unkingly  un + king + ly). Some previous studies take advantage of the morphological information to provide a suitable representation for low-frequency words and unknown words BIBREF4 , BIBREF5 .",
      "The former judges whether distributed word representations improve modeling contexts, and the latter estimates how well the learned representations achieve the additive property. However, such resources other than for English (e.g., Japanese) seldom exist. In addition, most of these datasets comprise high-frequency nouns so that they tend not to include other parts of speech. Hence, previous data fail to evaluate word representations of other parts of speech, including content words such as verbs and adjectives. To address the problem of the lack of a dataset for evaluating Japanese distributed word representations, we propose to build a Japanese dataset for the word similarity task. The main contributions of our work are as follows:",
      "human understanding of verb meaning Gerz:2016:EMNLP. It provides human ratings for the similarity of 3,500 verb pairs so that it enables robust evaluation of distributed representation for verbs. However, most of these datasets include English words only. There has been no Japanese dataset for the word-similarity task. Apart from English, WordSim353 and SimLex-999 Hill:2015:CL have been translated and rescored in other languages: German, Italian and Russian Leviant:2015:arXiv. SimLex-999 has also been translated and rescored in Hebrew and Croatian Mrksic:2017:TACL. SimLex-999 explicitly targets at similarity rather than relatedness and includes adjective, noun and verb pairs. However, this dataset contains only frequent words. In addition, the distributed representation of words is",
      "In general, distributed word representations are evaluated using a word similarity task. For instance, WordSim353 2002:PSC:503104.503110, MC BIBREF2 , RG BIBREF3 , and SCWS Huang:2012:IWR:2390524.2390645 have been used to evaluate word similarities in English. Moreover, baker-reichart-korhonen:2014:EMNLP2014 built a verb similarity dataset (VSD) based on WordSim353 because there was no dataset of verbs in the word-similarity task. Recently, SimVerb-3500 was introduced to evaluate human understanding of verb meaning Gerz:2016:EMNLP. It provides human ratings for the similarity of 3,500 verb pairs so that it enables robust evaluation of distributed representation for verbs. However, most of these datasets include English words only. There has been no Japanese dataset for the word-similarity",
      "Abstract\nAn evaluation of distributed word representation is generally conducted using a word similarity task and/or a word analogy task. There are many datasets readily available for these tasks in English. However, evaluating distributed representation in languages that do not have such resources (e.g., Japanese) is difficult. Therefore, as a first step toward evaluating distributed representations in Japanese, we constructed a Japanese word similarity dataset. To the best of our knowledge, our dataset is the first resource that can be used to evaluate distributed representations in Japanese. Moreover, our dataset contains various parts of speech and includes rare words in addition to common words.",
      "the similarity, but we can get low variance of the similarity when the word is monosemous. For this study, we constructed a Japanese word similarity dataset. We followed the procedure used to construct the Stanford Rare Word Similarity Dataset (RW) Luong-etal:conll13:morpho. We extracted Japanese word pairs from the Evaluation Dataset of Japanese Lexical Simplification kodaira. It targeted content words (nouns, verbs, adjectives, adverbs). It included 10 contexts about target words annotated with their lexical substitutions and rankings. Figure  shows an example of the dataset. A word in square brackets in the text is represented as a target word of simplification. A target word is not only recorded in the lemma form but also in the conjugated form. We built a Japanese similarity dataset",
      "represent only the pronunciation of a word, annotators might think of different words. In this case, Annotator A assigned a rating of 8, but Annotator B assigned a rating of 0. Similarly, we confirmed the same thing in other parts of speech. Especially, nouns can have several word pairs with different spellings, which results in their IAA became too low compared to other parts of speech. (e.g., a pairing of “oftenUTF8min（しばしば）” and “frequentlyUTF8min（しきりに）”.) We confirmed that the variance becomes larger among adverbs expressing frequency. This is due to the difference in the frequency of words that annotators imagines. In this case, Annotator A assigned a rating of 9, but Annotator B assigned a rating of 0. Similarly, we confirmed the same thing among adverbs expressing time.",
      "for intrinsic evaluation. In contrast, distributed word representations include the additive (or compositional) property of the vectors, which cannot be assessed by perplexity. Moreover, perplexity makes little use of infrequent words; thus, it is not appropriate for evaluating distributed presentations that try to represent them. Therefore, a word similarity task and/or a word analogy task are generally used to evaluate distributed word representations in the NLP literature. The former judges whether distributed word representations improve modeling contexts, and the latter estimates how well the learned representations achieve the additive property. However, such resources other than for English (e.g., Japanese) seldom exist. In addition, most of these datasets comprise high-frequency",
      "Language Resource References\nlrec lrec2018",
      "pairs based on semantic similarity, it was not straightforward to put paraphrase candidates onto a single scale considering all the attributes of the words. This limitation might be relaxed if we would ask annotators to refer to a thesaurus or an ontology such as Japanese Lexicon GoiTaikei:1997. (e.g., a pairing of “sloganUTF8min（スローガン）” and “sloganUTF8min（標語）”.) In Japanese, we can write a word using hiragana, katakana, or kanji characters; however because hiragana and katakana represent only the pronunciation of a word, annotators might think of different words. In this case, Annotator A assigned a rating of 8, but Annotator B assigned a rating of 0. Similarly, we confirmed the same thing in other parts of speech. Especially, nouns can have several word pairs with different spellings,",
      "with conventional models. However, low-frequency words and unknown words are often comprise high-frequency morphemes (e.g., unkingly  un + king + ly). Some previous studies take advantage of the morphological information to provide a suitable representation for low-frequency words and unknown words BIBREF4 , BIBREF5 . Morphological information is particularly important for Japanese since Japanese is an agglutinative language.",
      "are some cases where they express different degrees of feeling. Compared with “request”, “eager” indicates a stronger feeling. There were two annotators who emphasized the similarity of the act itself rather than the different degrees of feeling, and vice versa. In this case, Annotator A assigned a rating of 9, but Annotator B assigned a rating of 2. Although it was necessary to distinguish similarity and semantic relatedness BIBREF7 and we asked annotators to rate the pairs based on semantic similarity, it was not straightforward to put paraphrase candidates onto a single scale considering all the attributes of the words. This limitation might be relaxed if we would ask annotators to refer to a thesaurus or an ontology such as Japanese Lexicon GoiTaikei:1997. (e.g., a pairing of",
      "We present examples of a pair with high variance of similarity as shown below: (e.g., a pairing of “fastUTF8min（速い）” and “earlyUTF8min（早い）”.) Although they are similar in meaning with respect to the time, they have nothing in common with respect to speed; Annotator A assigned a rating of 10, but Annotator B assigned a rating of 1. Another example, the pairing of “be eagerUTF8min（懇願する）” and “requestUTF8min（頼む）”. Even though the act indicated by the two verbs is the same, there are some cases where they express different degrees of feeling. Compared with “request”, “eager” indicates a stronger feeling. There were two annotators who emphasized the similarity of the act itself rather than the different degrees of feeling, and vice versa. In this case, Annotator A assigned a rating of 9, but",
      "Construction of a Japanese Word Similarity Dataset"
    ]
  },
  {
    "title": "Assessing the Applicability of Authorship Verification Methods",
    "answer": "A self-compiled corpus refers to a dataset or collection of texts that has been compiled by the researchers themselves, rather than using an existing publicly available corpus. In the context provided, the researchers compiled their own corpora based on English documents crawled from different publicly accessible sources. Three such corpora were constructed, each representing a different challenge in the field of authorship verification (AV).",
    "evidence": [
      "Methodology\nIn the following, we introduce our three self-compiled corpora, where each corpus represents a different challenge. Next, we describe which authorship verification approaches we considered for the experiments and classify each AV method according to the properties introduced in Section  . Afterwards, we explain which performance measures were selected with respect to the conclusion made in Section  . Finally, we describe our experiments, present the results and highlight a number of observations.",
      "this, we hope to contribute to the further development of this young research field. Based on the proposed properties, we investigate the applicability of 12 existing AV approaches on three self-compiled corpora, where each corpus involves a specific challenge. The rest of this paper is structured as follows. Section  discusses the related work that served as an inspiration for our analysis. Section  comprises the proposed criteria and properties to characterize AV methods. Section  describes the methodology, consisting of the used corpora, examined AV methods, selected performance measures and experiments. Finally, Section  concludes the work and outlines future work.",
      "A serious challenge in the field of AV is the lack of publicly available (and suitable) corpora, which are required to train and evaluate AV methods. Among the few publicly available corpora are those that were released by the organizers of the well-known PAN-AV competitions BIBREF11 , BIBREF5 , BIBREF12 . In regard to our experiments, however, we cannot use these corpora, due to the absence of relevant meta-data such as the precise time spans where the documents have been written as well as the topic category of the texts. Therefore, we decided to compile our own corpora based on English documents, which we crawled from different publicly accessible sources. In what follows, we describe our three constructed corpora, which are listed together with their statistics in Table  . Note that"
    ]
  },
  {
    "title": "Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach",
    "answer": "The question is asking for the baseline models used in the experiments. To answer this, we need to look at the \"Baselines\" section of the context. Unfortunately, the specific details of the \"Baselines\" section are not provided. \n\n Normally, the answer would directly reference the baseline models mentioned in the context, such as specific model names or architectures. \n\nSince the actual content of \"Baselines\" is not given, a precise answer cannot be formulated. \n\nIf the content were available, the answer would be structured as follows: \"The baseline models used are [list specific models or architectures mentioned in the Baselines section].\"",
    "evidence": [
      "Experiments\nLabel-partially-unseen evaluation\nBaselines."
    ]
  },
  {
    "title": "Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU",
    "answer": "The baseline decoder used is a standard beam search decoder.",
    "evidence": [
      "Baseline Decoder\nOur baseline decoder is a standard beam search decoder BIBREF5 with several straightforward performance optimizations:"
    ]
  },
  {
    "title": "A Stable Variational Autoencoder for Text Modelling",
    "answer": "They evaluate the generated text quality through both quantitative and qualitative evaluation. The quantitative evaluation includes metrics such as negative log likelihood and perplexity.",
    "evidence": [
      "(E2E) text generation dataset BIBREF10. Experimental results show that our HR-VAE model not only can effectively mitigate the latent variable collapse issue with a stable training process, but also can give better predictive performance than the baselines, as evidenced by both quantitative (e.g., negative log likelihood and perplexity) and qualitative evaluation. The code for our model is available online."
    ]
  },
  {
    "title": "Natural Language Interactions in Autonomous Vehicles: Intent Detection and Slot Filling from Passenger Utterances",
    "answer": "The Joint-2 model is used for the second level of the hierarchy, which involves jointly training slots, intent keywords, and utterance-level intents. Therefore, what is shared in the joint model are slots, intent keywords, and utterance-level intents. \n\nTo be more precise, the Joint-2 Seq2seq models are used with slots, intent keywords, and utterance-level intents at Level-2.",
    "evidence": [
      "the ones that are predicted as ‘None/O’) are fed as input to the joint sequence-to-sequence models (described above). See Fig.  (b) for sample network architecture. After the filtering or summarization of sequence at level-1, <BOU> and <EOU> tokens are appended to the shorter input sequence before level-2 for joint learning. Note that in this case, using Joint-1 model (jointly training annotated slots & utterance-level intents) for the second level of the hierarchy would not make much sense (without intent keywords). Hence, Joint-2 model is used for the second level as described below: Hierarchical & Joint-2: Level-1 (Seq2seq Bi-LSTM for intent keywords & slots extraction) + Level-2 (Joint-2 Seq2seq models with slots & intent keywords & utterance-level intents) Table  summarizes the",
      "compared models. After the ASR pipeline described above is completed for all 20 sessions of AMIE in-cabin dataset (ALL with 1331 utterances), we repeated all our experiments with the subsets for 10 sessions having single passenger (Singletons with 600 utterances) and remaining 10 sessions having two passengers (Dyads with 731 utterances). We observed overall WER of 13.5% and 13.7% for Singletons and Dyads, respectively. The overlapping speech cases with slightly more conversations going on (longer transcriptions) in Dyad sessions compared to the Singleton sessions may affect the ASR performance, which may also affect the intent/slots models performances. As shown in Table  , although we have more samples with Dyads, the performance drops between the models trained on transcriptions vs."
    ]
  },
  {
    "title": "Is there Gender bias and stereotype in Portuguese Word Embeddings?",
    "answer": "The word embeddings analyzed are those in Portuguese, specifically using the Word2Vec model with two training strategies: Skip-Gram and Continuous Bag-of-Words (CBOW).",
    "evidence": [
      "In BIBREF13 , the quality of the representation of words through vectors in several models is discussed. According to the authors, the ability to train high-quality models using simplified architectures is useful in models composed of predictive methods that try to predict neighboring words with one or more context words, such as Word2Vec. Word embeddings have been used to provide meaningful representations for words in an efficient way. In BIBREF14 , several word embedding models trained in a large Portuguese corpus are evaluated. Within the Word2Vec model, two training strategies were used. In the first, namely Skip-Gram, the model is given the word and attempts to predict its neighboring words. The second, Continuous Bag-of-Words (CBOW), the model is given the sequence of words without",
      "a task through the experience BIBREF1 . Thus, BIBREF2 presents this as a method of inferences of functions or hypotheses capable of solving a problem algorithmically from data representing instances of the problem. This is an important way to solve different types of problems that permeate computer science and other areas. One of the main uses of ML is in text processing, where the analysis of the content the entry point for various learning algorithms. However, the use of this content can represent the insertion of different types of bias in training and may vary with the context worked. This work aims to analyze and remove gender stereotypes from word embedding in Portuguese, analogous to what was done in BIBREF3 for the English language. Hence, we propose to employ a public word2vec",
      "This paper presents an analysis of the presence of gender bias in Portuguese word embeddings. Even though it is a work in progress, the proposal showed promising results in analyzing predicting models. A possible extension of the work involves deepening the analysis of the results obtained, seeking to achieve higher accuracy rates and fairer models to be used in machine learning techniques. Thus, these studies can involve tests with different methods of pre-processing the data to the use of different models, as well as other factors that may influence the results generated. This deepening is necessary since the model's accuracy is not high. To conclude, we believe that the presence of gender bias and stereotypes in the Portuguese language is found in different spheres of language, and it"
    ]
  },
  {
    "title": "An analysis of the utility of explicit negative examples to improve the syntactic abilities of neural language models",
    "answer": "The neural language models explored are those trained on a set of sentences, typically only on positive examples, and the utilities of explicit negative examples in training these models are being examined. Specifically, the context mentions that neural language models are \"commonly trained only on positive examples, a set of sentences in the training data\". The models in question are related to handling complex syntactic constructions, such as long-distance agreement, and are being trained on English data.",
    "evidence": [
      "We explore the utilities of explicit negative examples in training neural language models. Negative examples here are incorrect words in a sentence, such as\"barks\"in\"*The dogs barks\". Neural language models are commonly trained only on positive examples, a set of sentences in the training data, but recent studies suggest that the models trained in this way are not capable of robustly handling complex syntactic constructions, such as long-distance agreement. In this paper, using English data, we first demonstrate that appropriately using negative examples about particular constructions (e.g., subject-verb agreement) will boost the model's robustness on them, with a negligible loss of perplexity. The key to our success is an additional margin loss between the log-likelihoods of a correct"
    ]
  },
  {
    "title": "Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset",
    "answer": "The six domains covered in the Taskmaster-1 dataset are: \n1. Ordering pizza, \n2. Creating auto repair appointments, \n3. Setting up ride service, \n4. Ordering movie tickets, \n5. Ordering coffee drinks, and \n6. Making restaurant reservations.",
    "evidence": [
      "progress in this area BIBREF9, BIBREF11. To help solve the data problem we present Taskmaster-1, a dataset consisting of 13,215 dialogs, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures. Each conversation falls into one of six domains: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations. For the spoken dialogs, we created a “Wizard of Oz” (WOz) system BIBREF12 to collect two-person, spoken conversations. Crowdsourced workers playing the “user\" interacted with human operators playing the “digital assistant” using a web-based interface. In this way, users were led to believe they were interacting with an automated system while it was in fact a human,"
    ]
  },
  {
    "title": "Open-Vocabulary Semantic Parsing with both Distributional Statistics and Formal Knowledge",
    "answer": "The knowledge base used is initially a formal Knowledge Base (KB), specifically Freebase, but recent work has proposed replacing it with a probabilistic database learned from a text corpus for open vocabulary semantic parsing. \n\nKey information: \n- Initial knowledge base: Freebase\n- Alternative knowledge base for open vocabulary semantic parsing: a probabilistic database learned from a text corpus. \n\nTherefore, the answer is: Freebase, and alternatively, a probabilistic database learned from a text corpus.",
    "evidence": [
      "statements, semantic parsers can leverage large, curated sources of knowledge to answer questions BIBREF3 . This benefit comes with an inherent limitation, however—semantic parsers can only produce executable statements within their manually-produced schema. There is no query against Freebase that can answer questions like “Who are the Democratic front-runners in the US election?”, as Freebase does not encode information about front-runners. Semantic parsers trained for Freebase fail on these kinds of questions. To overcome this limitation, recent work has proposed methods for open vocabulary semantic parsing, which replace a formal KB with a probabilistic database learned from a text corpus. In these methods, language is mapped onto queries with predicates derived directly from the text"
    ]
  },
  {
    "title": "SimplerVoice: A Key Message&Visual Description Generator System for Illiteracy",
    "answer": "The model used to generate key messages is the \"S-V-O\" (Subject-Verb-Object) query model, specifically in the canonical representation: [Subject] + [Verb-ing] + (with) + [Object Type/Category].",
    "evidence": [
      "This section discusses the process of generating key message from the object's input. Based on the retrieved input, we can easily obtain the object's title through searching in database, or using search engine; hence, we assume that the input of object2text is the object's title. The workflow of object2text is provided in Figure  . S-V-O query is constructed by the 3 steps below. In order to find the object type, SimplerVoice, first, builds an ontology-based knowledge tree. Then, the system maps the object with a tree's leaf node based on the object's title. For instance, given the object's title as “Thomas' Plain Mini Bagels\", SimplerVoice automatically defines that the object category is “bagel\". Note that both the knowledge tree, and the mapping between object and object category are",
      "Given an object as the target, SimplerVoice, first, retrieves the target input in either of 3 representations: (1) object's title as text, (2) object's shape as image, or (3) other forms, e.g. object's information from scanned barcode, speech from users, etc. Based on the captured input, the system, then, generates a query string/sequence of text which is the key message describing the object's usage. Due to low-literates' lack of reading capability, the generated text requires not only informativeness, but also simplicity, and clarity. Therefore, we propose to use the \"S-V-O\" query's canonical representation as below: [Subject] + [Verb-ing] + (with) + [Object Type/Category] The intuition of this query representation is that the generated key message should be able to describe the action",
      "not only informativeness, but also simplicity, and clarity. Therefore, we propose to use the \"S-V-O\" query's canonical representation as below: [Subject] + [Verb-ing] + (with) + [Object Type/Category] The intuition of this query representation is that the generated key message should be able to describe the action of a person using, or interacting with the target object. Moreover, the simple \"S-V-O\" model has been proposed to use in other studies BIBREF11 , BIBREF12 , BIBREF13 since it is able to provide adequate semantics meaning. The detail of generating the S-V-O query is provided in Section  . Once the query is constructed, SimplerVoice converts the query text into visual forms. There is a variety of visual formats to provide users: photos, icons, pictographs, etc. These visual"
    ]
  },
  {
    "title": "A Parallel Corpus of Theses and Dissertations Abstracts",
    "answer": "The NMT models experimented with are OpenNMT and Moses, in addition to comparing with Google Translate (GT). \n\nKey information extracted: \n- NMT models: OpenNMT, Moses \n- Other translation system mentioned: Google Translate (GT) \n\nThe answer directly addresses the question based on the context provided, focusing on the specific NMT models mentioned.",
    "evidence": [
      "One can notice that in fact NMT model tend to produce more fluent results, specially regarding verbal regency. Human translation: this paper presents a study of efficiency and power management in a packaging industry and plastic films. OpenNMT: this work presents a study of efficiency and electricity management in a packaging industry and plastic films. Moses: in this work presents a study of efficiency and power management in a packaging industry and plastic films. GT: this paper presents a study of the efficiency and management of electric power in a packaging and plastic film industry. Human translation: this fact corroborates the difficulty in modeling human behavior. OpenNMT: this fact corroborates the difficulty in modeling human behavior. Moses: this fact corroborated the"
    ]
  },
  {
    "title": "Data Innovation for International Development: An overview of natural language processing for qualitative data analysis",
    "answer": "The application of natural language processing (NLP) and machine learning (ML) is proposed to analyze qualitative data on a large scale. Specifically, the main approaches to NLP, which can be augmented by human coding, are suggested for the speedy analysis of qualitative data. \n\nKey elements include: \n1. Natural Language Processing (NLP) \n2. Machine Learning (ML) \n3. Human coding to augment NLP approaches.",
    "evidence": [
      "to inform quick decision-making poses challenges, in terms of high costs, time, or expertise required. The application of natural language processing (NLP) and machine learning (ML) can make feasible the speedy analysis of qualitative data on a large scale. We start with a brief description of the main approaches to NLP and how they can be augmented by human coding. We then move on to the issue of working with multiple languages and different document formats. We then provide an overview of the recent application of these techniques in a United Nations Development Program (UNDP) study."
    ]
  },
  {
    "title": "UniSent: Universal Adaptable Sentiment Lexica for 1000+ Languages",
    "answer": "The sentiment sources they compare UniSent with are manually or semi-manually created sentiment resources, specifically for languages such as Macedonian, Czech, German, Spanish, and French.",
    "evidence": [
      "Conclusion\nUsing our novel Adapted Sentiment Pivot method, we created UniSent, a sentiment lexicon covering over 1000 (including many low-resource) languages in several domains. The only necessary resources to create UniSent are a sentiment lexicon in any language and a massively parallel corpus that can be small and domain specific. Our evaluation showed that the quality of UniSent is closed to manually annotated resources.",
      "domain adaptation. We evaluate the quality of UniSent for Macedonian, Czech, German, Spanish, and French and show that its quality is comparable to manually or semi-manually created sentiment resources. With the publication of this paper, we release UniSent lexica as well as Adapted Sentiment Pivot related codes. method."
    ]
  },
  {
    "title": "TutorialVQA: Question Answering Dataset for Tutorial Videos",
    "answer": "The triples, consisting of videos, questions, and answer spans, were manually collected from screencast tutorial videos with spoken narratives for a photo-editing software.",
    "evidence": [
      "Baselines\nResults",
      "a single entity. Differing from video transcripts, the structures of these data sources, namely paragraphs from Wikipedia and news sources, are typically straightforward since they are meant to be read. In contrast, video transcripts originate from spoken dialogue, which can verbose, unstructured, and disconnected. Furthermore, the answers in instructional video transcripts can be longer, spanning multiple sentences if the process is multi-step or even fragmented into multiple segments throughout the video. Visual corpora in particular have proven extremely valuable to visual questions-answering tasks BIBREF9, the most similar being MovieQA BIBREF1 and VideoQA BIBREF0. Similar to how our data is generated from video tutorials, the MovieQA and VideoQA corpus is generated from movie scripts",
      "video question answering causes some practical limitations for the following reasons. First, factoid questions are just a small part of what people actually want to ask on video contents. Especially if a short video is given to users, most fragmentary facts within the scope of previous tasks can be easily perceived by themselves even before asking questions. Thus, video question answering is expected to provide answers to more complicated non-factoid questions beyond the simple facts. For example, those questions could be the ones asking about a how procedure as shown in Fig. , and the answers should contain all necessary steps to complete the task. Accordingly, the answer format needs to also be improved towards more flexible ways than multiple choice BIBREF1, BIBREF2 or",
      "beyond the simple facts. For example, those questions could be the ones asking about a how procedure as shown in Fig. , and the answers should contain all necessary steps to complete the task. Accordingly, the answer format needs to also be improved towards more flexible ways than multiple choice BIBREF1, BIBREF2 or fill-in-the-blank questions BIBREF3, BIBREF4. Although open-ended video question answering BIBREF0, BIBREF2, BIBREF5 has been explored, it still aims to generate just a short word or phrase-level answer, which is not enough to cover various granularities of non-factoid question answering. The other issue is that most videos with sufficient amount of information, which are likely to be asked, have much longer lengths than the video clips in the existing datasets. Therefore, the",
      "Video is the fastest growing medium to create and deliver information today. Consequentially, videos have been increasingly used as main data sources in many question answering problems BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF2, BIBREF5. These previous studies have mostly focused on factoid questions, each of which can be answered in a few words or phrases generated by understanding multimodal contents in a short video clip. However, this problem definition of video question answering causes some practical limitations for the following reasons. First, factoid questions are just a small part of what people actually want to ask on video contents. Especially if a short video is given to users, most fragmentary facts within the scope of previous tasks can be easily perceived by",
      "Table  presents some extracted sample questions from our dataset. The first column corresponds to an AMT generated question, while the second column corresponds to the video ID where the segment can be found. As can be seen in the first two rows, multiple types of questions can be answered within the same video (but different segments). The last two rows display questions which belong to the same segment but correspond to different properties of the same entity, 'crop tool'. Here we observe different types of questions, such as \"why\", \"how\", \"what\", and \"where\", and can see why the answers may involve multiple steps. Some questions that the worked paraphrased were in the \"yes/no\" style, however our answer segments then provide an explanation to these questions. Each answer segment was",
      "were generated after each paragraph in a movie's plot synopsis BIBREF1. The MovieQA dataset also contains specific annotated answers with incorrect examples for each question. In the VideoQA dataset, questions focus on a single entity, contrary to our instructional video dataset. Although not necessarily a visual question-answering task, the work proposed by BIBREF10 involved answering questions over transcript data. Contrary to our work, Gupta's dataset is not publically available and their examples only showcase factoid-style questions involving single entity answers. BIBREF11 focus on aligning a set of instructions to a video of someone carrying out those instructions. In their task, they use the video transcript to represent the video, which they later augment with a visual cue",
      "the differences between our data collection process and the the query generation process employed in the Search and Hyperlinking Task at MediaEval BIBREF12. In the Search and Hyperlinking Task, 30 users were tasked to first browse the collection of videos, select interesting segments with start and end times, and then asked to conjecture questions that they would use on a search query to find the interesting video segments. This was done in order to emulate their thought-proces mechanism. While the nature of their task involves queries relating to the overall videos themselves, hence coming from a video's interestingness, our task involves users already being given a video and formulating questions where the answers themselves come from within a video. By presenting the same video segment",
      "Most relevant to our proposed work is the reading comprehension task, which is a question answering task involving a piece of text such as a paragraph or article. Such datasets for the reading comprehension task, such as SQuAD BIBREF6 based on Wikipedia, TriviaQA BIBREF7 constructed from trivia questions with answer evidence from Wikipedia, or those from Hermann et al. based on CNN and Daily Mail articles BIBREF8 are factoid-based, meaning the answers typically involve a single entity. Differing from video transcripts, the structures of these data sources, namely paragraphs from Wikipedia and news sources, are typically straightforward since they are meant to be read. In contrast, video transcripts originate from spoken dialogue, which can verbose, unstructured, and disconnected.",
      "Here we observe different types of questions, such as \"why\", \"how\", \"what\", and \"where\", and can see why the answers may involve multiple steps. Some questions that the worked paraphrased were in the \"yes/no\" style, however our answer segments then provide an explanation to these questions. Each answer segment was extracted from an image editing tutorial video that involved multiple steps and procedures to produce a final image, which can partially be seen in . The average number of sentences per video was approximately 52, with the maximum number of sentences contained in a video being 187. The sub-tasks in the tutorial include segments (and thus answers) on editing parts of images, instructions on using certain tools, possible actions that can be performed on an image, and identifying",
      "Related Work",
      "vector, $p_i$, which represents the latent meaning of each sentence. Then, the model combines each pair of sentence embeddings ($p_i$, $p_j$) to generate a span embedding. where [$\\cdot $,$\\cdot $] indicates the concatenation. Finally, we use a one-layer feed forward network to compute a score between each span and a question. In training, we use cross-entropy as an objective function. In testing, the span with the highest score is picked as an answer. Metrics. We use tolerance accuracy BIBREF16, which measures how far away the predicted span is from the gold standard span, as a metric. The rationale behind the metric is that, in practice, it suffices to recommend a rough span which contains the answer – a difference of a few seconds would not matter much to the user. Specifically, the",
      "mechanism. While the nature of their task involves queries relating to the overall videos themselves, hence coming from a video's interestingness, our task involves users already being given a video and formulating questions where the answers themselves come from within a video. By presenting the same video segment to many users, we maintain a consistent set of video segments and extend the possibility to generate a diverse set of question for the same segment.",
      "segments throughout the video. Visual corpora in particular have proven extremely valuable to visual questions-answering tasks BIBREF9, the most similar being MovieQA BIBREF1 and VideoQA BIBREF0. Similar to how our data is generated from video tutorials, the MovieQA and VideoQA corpus is generated from movie scripts and news transcipts, respectively. MovieQA's answers have a shorter span than the answers collected in our corpus, because questions and answer pairs were generated after each paragraph in a movie's plot synopsis BIBREF1. The MovieQA dataset also contains specific annotated answers with incorrect examples for each question. In the VideoQA dataset, questions focus on a single entity, contrary to our instructional video dataset. Although not necessarily a visual",
      "was solved by providing them with an unrelated example. The second part of the question collection framework consisted of a paraphrasing task. In this task we presented workers with the questions generated by the first task and asked them to write the questions differently while keeping the semantics the same. In this way, we expanded our question dataset. After filtering out the questions with low quality, we collected a total of 6,195 questions. It is important to note the differences between our data collection process and the the query generation process employed in the Search and Hyperlinking Task at MediaEval BIBREF12. In the Search and Hyperlinking Task, 30 users were tasked to first browse the collection of videos, select interesting segments with start and end times, and then",
      "just a short word or phrase-level answer, which is not enough to cover various granularities of non-factoid question answering. The other issue is that most videos with sufficient amount of information, which are likely to be asked, have much longer lengths than the video clips in the existing datasets. Therefore, the most relevant part of a whole video needs to be determined prior to each answer generation in practice. However, this localization task has been out of scope for previous studies. In this work, we propose a new question answering problem for non-factoid questions on instructional videos. According to the nature of the media created for educational purposes, we assume that many answers already exist within the given video contents. Under this assumption, we formulate the",
      "available and their examples only showcase factoid-style questions involving single entity answers. BIBREF11 focus on aligning a set of instructions to a video of someone carrying out those instructions. In their task, they use the video transcript to represent the video, which they later augment with a visual cue detector on food entities. Their task focuses on procedure-based cooking videos, and contrary to our task is primarily a text alignment task. In our task we aim to answer questions-using the transcripts-on instructional-style videos, in which the answer can involve steps not mentioned in the question.",
      "We downloaded 76 videos from a tutorial website about an image editing program . Each video is pre-processed to provide the transcripts and the time-stamp information for each sentence in the transcript. We then used Amazon Mechanical Turk to collect the question-answer pairs . One naive way of collecting the data is to prepare a question list and then, for each question, ask the workers to find the relevant parts in the video. However, this approach is not feasible and error-prone because the videos are typically long and finding a relevant part from a long video is difficult. Doing so might also cause us to miss questions which were relevant to the video segment. Instead, we took a reversed approach. First, for each video, we manually identified the sentence spans that can serve as",
      "set of questions which the span can answer. Along with the questions, the workers were also required to provide a justification as to why they made their questions. We manually checked this justification to filter out the questions with poor quality by removing those questions which were unrelated to the video. One initial challenge worth mentioning is that at first some workers input questions they had about the video and not questions which the video could answer. This was solved by providing them with an unrelated example. The second part of the question collection framework consisted of a paraphrasing task. In this task we presented workers with the questions generated by the first task and asked them to write the questions differently while keeping the semantics the same. In this",
      "We chose videos pertaining to an image editing software because of the complexity and variety of tasks involved. In these videos, a narrator is communicating an overall goal by utilizing an example. For example, in  the video pertains to combining multiple layers into one image. However, throughout the videos multiple subtasks are achieved, such as the opening of multiple images, the masking of images, and the placement of two images side-by-side. These subtasks involve multiple steps and are of interest to us in segmenting the videos. Each segment can be seen as a subtask within a larger video dictating an example. We thus chose these videos because of the amount of procedural information stored in each video for which the user may ask. Though there is only one domain, each video",
      "We performed an error analysis on Baseline1's results. We first observe that, in 92% of the errors, the predicted span and the ground-truth overlap. Furthermore, in 56% of the errors, the predicted spans are a subset or superset of the ground-truth spans. This indicates that the model finds the rough answer regions but fails to locate the precise boundaries. To address this issue, we plan on exploring the Pointer-network BIBREF19, which finds an answer span by selecting the boundary sentences. Unlike Baseline1 which avoids an explicit segmentation step, the Pointer-network can explicitly model which sentences are likely to be a boundary sentence. Moreover, the search space of the spans in the Pointer-network is $2n$ where $n$ is the number of sentences, because it selects only two",
      "for previous studies. In this work, we propose a new question answering problem for non-factoid questions on instructional videos. According to the nature of the media created for educational purposes, we assume that many answers already exist within the given video contents. Under this assumption, we formulate the problem as a localization task to specify the span of a video segment as the direct answer to a given video and a question, as illustrated in Figure . The remainder of this paper is structured as follows. Section  introduces TutorialVQA dataset as a case study of our proposed problem. The dataset includes about 6,000 triples, comprised of videos, questions, and answer spans manually collected from screencast tutorial videos with spoken narratives for a photo-editing software.",
      "of this paper is structured as follows. Section  introduces TutorialVQA dataset as a case study of our proposed problem. The dataset includes about 6,000 triples, comprised of videos, questions, and answer spans manually collected from screencast tutorial videos with spoken narratives for a photo-editing software. Section  presents the baseline models and their experiment details on the sentence-level prediction and video segment retrieval tasks on our dataset. Then, we discuss the experimental results in Section  and conclude the paper in Section .",
      "the Baseline1 model. Model. The model takes two inputs, a transcript, $\\lbrace s_1, s_2, ... s_n\\rbrace $ where $s_i$ are individual sentences and a question, $q$. The output is the span scores, $y$, the scores over all possible spans. GLoVe BIBREF14 is used for the word representations in the transcript and the questions. We use two bi-LSTMs BIBREF15 to encode the transcript. where n is the number of sentences . The output of Passage-level Encoding, $p$, is a sequence of vector, $p_i$, which represents the latent meaning of each sentence. Then, the model combines each pair of sentence embeddings ($p_i$, $p_j$) to generate a span embedding. where [$\\cdot $,$\\cdot $] indicates the concatenation. Finally, we use a one-layer feed forward network to compute a score between each span and a",
      "an answer, given a question and video clip, our task aims to identify a span of a video segment as an answer which contains instructional details with various granularities. This work focuses on screencast tutorial videos pertaining to an image editing program. We introduce a dataset, TutorialVQA, consisting of about 6,000manually collected triples of (video, question, answer span). We also provide experimental results with several baselines algorithms using the video transcripts. The results indicate that the task is challenging and call for the investigation of new algorithms."
    ]
  },
  {
    "title": "Transductive Learning with String Kernels for Cross-Domain Text Classification",
    "answer": "The context mentions several domain adaptation techniques used in transfer learning, which can be categorized into: graph-based methods, probabilistic models, knowledge-based models, and joint optimization frameworks. These categories represent the machine learning algorithms used in transfer learning. Specifically, the techniques include graph-based methods, probabilistic models, knowledge-based models, and joint optimization frameworks.",
    "evidence": [
      "Transfer learning (or domain adaptation) aims at building effective classifiers for a target domain when the only available labeled training data belongs to a different (source) domain. Domain adaptation techniques can be roughly divided into graph-based methods BIBREF1 , BIBREF29 , BIBREF9 , BIBREF30 , probabilistic models BIBREF3 , BIBREF4 , knowledge-based models BIBREF14 , BIBREF31 , BIBREF11 and joint optimization frameworks BIBREF12 . The transfer learning methods from the literature show promising results in a variety of real-world applications, such as image classification BIBREF12 , text classification BIBREF13 , BIBREF16 , BIBREF3 , polarity classification BIBREF1 , BIBREF29 , BIBREF4 , BIBREF6 , BIBREF30 and others BIBREF32 . General transfer learning approaches. Long et al."
    ]
  },
  {
    "title": "Discriminative Acoustic Word Embeddings: Recurrent Neural Network-Based Approaches",
    "answer": "The dataset used is drawn from the Switchboard conversational English corpus.",
    "evidence": [
      "approximately 11k segments (corresponding to about 60M pairs for computing the dev/test AP). As in BIBREF13 , when training the classification-based embeddings, we use a subset of the training set containing all word types with a minimum of 3 occurrences, reducing the training set size to approximately 9k segments. When training the Siamese networks, the training data consists of all of the same-word pairs in the full training set (approximately 100k pairs). For each such training pair, we randomly sample a third example belonging to a different word type, as required for the  loss.",
      "Classification network details",
      "are examples of the same class (a pair from the 100k same-word pair set) and  is a randomly sampled example from a different class. Then, for each of these  triplets  , an additional triplet  is added to the mini-batch to allow all segments to serve as anchors. This is a slight departure from earlier work BIBREF13 , which we found to improve stability in training and performance on the development set. In preliminary experiments, we compared two methods for choosing the negative examples  during training, a uniform sampling approach and a non-uniform one. In the case of uniform sampling, we sample  uniformly at random from the full set of training examples with labels different from  . This sampling method requires only word-pair supervision. In the case of non-uniform sampling,  is",
      "For experiments with Siamese networks, we initialize (warm-start) the networks with the tuned classification network, removing the final log-softmax layer and replacing it with a linear layer of size equal to the desired embedding dimensionality. We explored embeddings with dimensionalities between 8 and 2048. We use a margin of 0.4 in the cos-hinge loss. In training the Siamese networks, each training mini-batch consists of  triplets.  triplets are of the form  where  and  are examples of the same class (a pair from the 100k same-word pair set) and  is a randomly sampled example from a different class. Then, for each of these  triplets  , an additional triplet  is added to the mini-batch to allow all segments to serve as anchors. This is a slight departure from earlier work BIBREF13 ,",
      "BIBREF18 , and unsupervised speech recognition BIBREF19 . Voinea et al. BIBREF15 developed a representation also based on templates, in their case phone templates, designed to be invariant to specific transformations, and showed their robustness on digit classification. Kamper et al. BIBREF13 compared several types of acoustic word embeddings for a word discrimination task related to query-by-example search, finding that embeddings based on convolutional neural networks (CNNs) trained with a contrastive loss outperformed the reference vector approach of Levin et al. BIBREF11 as well as several other CNN and DNN embeddings and DTW using several feature types. There have now been a number of approaches compared on this same task and data BIBREF11 , BIBREF20 , BIBREF21 , BIBREF22 . For a",
      "the training data. For words not seen in training, the embeddings should correspond to some measure of similarity of the word to the training words, measured via the posterior probabilities of the previously seen words. In the experiments below, we examine this assumption by analyzing performance on words that appear in the training data compared to those that do not. The second training approach, based on earlier work of Kamper et al. BIBREF13 , is to train \"Siamese\" networks BIBREF30 . In this approach, full supervision is not needed; rather, we use weak supervision in the form of pairs of segments labeled as same or different. The base model remains the same as before—an RNN followed by a set of fully connected layers—but the final layer is no longer a softmax but rather a linear",
      "of determining, for each pair of evaluation segments, whether they are examples of the same or different words, and measuring performance via the average precision (AP). We do this by measuring the cosine similarity between their acoustic word embeddings and declaring them to be the same if the distance is below a threshold. By sweeping the threshold, we obtain a precision-recall curve from which we compute the AP. The data used for this task is drawn from the Switchboard conversational English corpus BIBREF31 . The word segments range from 50 to 200 frames in length. The acoustic features in each frame (the input to the word embedding models  ) are 39-dimensional MFCCs+  +  . We use the same train, development, and test partitions as in prior work BIBREF13 , BIBREF11 , and the same"
    ]
  },
  {
    "title": "Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference",
    "answer": "The ESIM model has two LSTM layers. Specifically, it consists of the first (input) LSTM layer that performs input encoding and the second (inference) LSTM layer that generates the representation for inference. Therefore, there are 2 LSTM layers in the ESIM model.",
    "evidence": [
      "Task and Model",
      "We propose new visualization and interpretation strategies for neural models to understand how and why they work. We demonstrate the effectiveness of the proposed strategies on a complex task (NLI). Our strategies are able to provide interesting insights not achievable by previous explanation techniques. Our future work will extend our study to consider other NLP tasks and models with the goal of producing useful insights for further improving these models. Model In this section we describe the ESIM model. We divide ESIM to three main parts: 1) input encoding, 2) attention, and 3) inference. Figure 4 demonstrates a high-level view of the ESIM framework. Let $u=[u_1, \\cdots , u_n]$ and $v=[v_1, \\cdots , v_m]$ be the given premise with length $n$ and hypothesis with length $m$ respectively,",
      "of considering individual dimensions of the gating signals, we aggregate them to consider their norm, both for the signal and for its saliency. Note that ESIM models have two LSTM layers, the first (input) LSTM performs the input encoding and the second (inference) LSTM generates the representation for inference. In Fig. 3 we plot the normalized signal and saliency norms for different gates (input, forget, output) of the Forward input (bottom three rows) and inference (top three rows) LSTMs. These results are produced by the ESIM-50 model for the three examples of Section 3.1, one for each column. From the figure, we first note that the saliency tends to be somewhat consistent across different gates within the same LSTM, suggesting that we can interpret them jointly to identify parts of"
    ]
  },
  {
    "title": "\"What is Relevant in a Text Document?\": An Interpretable Machine Learning Approach",
    "answer": "The authors state that the CNN model outperforms the BoW/SVM classifier in terms of explanatory power, indicating a higher level of explainability. Specifically, the results of the experiments, including the deletion experiment and the evaluation using an external KNN classifier on document summary vectors, quantitatively confirm this observation. The learned filters in the CNN models (CNN2 and CNN3) consider not only isolated words but also bigrams or trigrams of words, which contributes to the higher explanatory power.",
    "evidence": [
      "TFIDF weighting, this indicates that the explanations produced by LRP are semantically more meaningful than the latter. Finally, we confirm quantitatively the observations made before, namely that (1) the LRP decomposition method provides better explanations than SA and that (2) the CNN model outperforms the BoW/SVM classifier in terms of explanatory power.",
      "enough for the considered classification problem), the learned filters in CNN2 and CNN3 do not only focus on isolated words but additionally consider bigrams or trigrams of words, as their results differ a lot from the CNN1 model in the first deletion experiment. In order to quantitatively evaluate and compare the ML models in combination with a relevance decomposition or explanation technique, we apply the evaluation method described in section \"Measuring Model Explanatory Power through Extrinsic Validation\" . That is, we compute the accuracy of an external classifier (here KNN) on the classification of document summary vectors (obtained with the ML model's predicted class). For these experiments we remove test documents which are empty or contain only one word after preprocessing (this"
    ]
  },
  {
    "title": "Same Representation, Different Attentions: Shareable Sentence Representation Learning from Multiple Tasks",
    "answer": "The researchers conducted experiments on text classification tasks. Specifically, they experimented with 16 different text classification tasks.",
    "evidence": [
      "Experiment\nIn this section, we investigate the empirical performances of our proposed architectures on three experiments.",
      "learning. More specifically, all tasks share the same sentence representation and each task can select the task-specific information from the shared sentence representation with attention mechanism. The query vector of each task's attention could be either static parameters or generated dynamically. We conduct extensive experiments on 16 different text classification tasks, which demonstrate the benefits of our architecture."
    ]
  },
  {
    "title": "Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation",
    "answer": "The model \"Ours with Mask and Ordered Triplets\" outperformed the Baseline model by 35% in Exact Match and by 25% is mentioned in comparison to the Ablation model, but the comparison to the Baseline for Goal Match is not directly stated. However, it is stated that the model outperformed the Baseline by 35% in Exact Match. \n\nTo directly answer the question: The model outperformed the Baseline by 35% in Exact Match.",
    "evidence": [
      "in new environments. For instance, the difference between our model and the second best model in the Test-New set is about  EM and GM. Note that the average number of actions in the ground truth output sequences is 7.07 for the Test-New set. Our model's predictions are just  edits off on average from the correct navigation plans.",
      "particular, we obtain over  increase in EM and GM between our model and the next best two models. The previous section evaluated model performance on new instructions (and corresponding navigation plans) for environments that were previously seen at training time. Here, we examine whether the trained models succeed on environments that are completely new. The evaluation on the Test-New Set helps understand the generalization capabilities of the models under consideration. This experiment is more challenging than the one in the previous section, as can be seen in performance drops in Table  for the new environments. Nonetheless, the insights from the previous section still hold: masking in the output layer and reordering the graph triplets tend to increase performance. Even though the",
      "Models Used in the Evaluation",
      "Table  shows the performance of the models considered in our evaluation on both test sets. The next two sections discuss the results in detail. First, we can observe that the final model “Ours with Mask and Ordered Triplets” outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively. These results suggest that providing the behavioral navigation graph to the model and allowing it to process this information as a knowledge base in an end-to-end fashion is beneficial. We can also observe from Table  that the masking function of Eq. ( EQREF12 ) tends"
    ]
  },
  {
    "title": "Sentiment Analysis of Citations Using Word2vec",
    "answer": "The metrics considered are F-score, micro-F, macro-F, and weighted-F scores. These metrics are used to evaluate the performance of a multi-class classification task. Specifically, they include: \n\n1. F-score: a weighted average of precision and recall.\n2. Micro-F: calculates metrics globally by counting total true positives, false negatives, and false positives.\n3. Macro-F: calculates metrics for each label and finds their unweighted mean.\n4. Weighted-F: calculates metrics for each label and finds their average weighted by support (the number of true instances for each label).",
    "evidence": [
      "positives. Macro-F calculates metrics for each label, and find their unweighted mean. Macro-F does not take label imbalance into account. Weighted-F calculates metrics for each label, and find their average, weighted by support (the number of true instances for each label). Weighted-F alters macro-F to account for label imbalance.",
      "One-Vs-The-Rest strategy was adopted for the task of multi-class classification and I reported F-score, micro-F, macro-F and weighted-F scores using 10-fold cross-validation. The F1 score is a weighted average of the precision and recall. In the multi-class case, this is the weighted average of the F1 score of each class. There are several types of averaging performed on the data: Micro-F calculates metrics globally by counting the total true positives, false negatives and false positives. Macro-F calculates metrics for each label, and find their unweighted mean. Macro-F does not take label imbalance into account. Weighted-F calculates metrics for each label, and find their average, weighted by support (the number of true instances for each label). Weighted-F alters macro-F to account for"
    ]
  },
  {
    "title": "Neural Factor Graph Models for Cross-lingual Morphological Tagging",
    "answer": "The languages explored are Russian (Ru) and Bulgarian (Bg).",
    "evidence": [
      "Language-Specific Weights\nAs an enhancement to the information encoded in the transition and pairwise factors, we experiment with training general and language-specific parameters for the transition and the pairwise weights. We define the weight matrix  to learn the general trends that hold across both languages, and the weights  to learn the exceptions to these trends. In our model, we sum both these parameter matrices before calculating the transition and pairwise factors. For instance, the transition weights  are calculated as  .",
      "One of the major advantages of our model is the ability to interpret what the model has learned by looking at the trained parameter weights. We investigated both language-generic and language-specific patterns learned by our parameters: Language-Generic: We found evidence for several syntactic properties learned by the model parameters. For instance, in Figure  , we visualize the generic (  ) transition weights of the POS tags in Ru/Bg. Several universal trends such as determiners and adjectives followed by nouns can be seen. In Figure  , we also observed that infinitive has a strong correlation for NULL tense, which follows the universal phenomena that infinitives don't have tense. Language Specific Trends: We visualized the learnt language-specific weights and looked for evidence of"
    ]
  },
  {
    "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
    "answer": "The context does not directly list the specific state-of-the-art general-purpose pretrained models available under the unified API. However, it mentions \"Auto classes\" that provide a unified API, and gives an example using \"bert-base-cased,\" indicating that models like BERT are available. It implies that various models, including potentially others beyond BERT, can be used through this unified API, as it mentions \"XXXForMaskedLM where XXX is the name of the model.\" Therefore, the models made available include at least those compatible with the Auto classes and the masked language modeling task, such as BERT. \n\nMore precisely, the context suggests that models like BERT are available under the unified API provided by Auto classes.",
    "evidence": [
      "or XXXForMaskedLM where XXX is the name of the model and can be used for adaptation (fine-tuning) or pre-training. All models are available both in PyTorch and TensorFlow (starting 2.0) and offer deep inter-operability between both frameworks. For instance, a model trained in one of frameworks can be saved on drive for the standard library serialization practice and then be reloaded from the saved files in the other framework seamlessly, making it particularly easy to switch from one framework to the other one along the model life-time (training, serving, etc.). Auto classes - In many cases, the architecture to use can be automatically guessed from the shortcut name of the pretrained weights (e.g. `bert-base-cased`). A set of Auto classes provides a unified API that enable very fast"
    ]
  },
  {
    "title": "Multichannel Variable-Size Convolution for Sentence Classification",
    "answer": "MVCNN is a type of CNN architecture specifically designed for sentence classification. Compared to standard CNN, MVCNN combines multichannel initialization, using diverse versions of pretrained word embeddings, and extracts features of multigranular phrases with variable-size convolution filters. This allows MVCNN to have richer feature maps as input and output of each convolution layer, making its convolution operation more flexible to extract features of variable-range phrases and model dependency among all dimensions of representations.",
    "evidence": [
      "sets are available. MVCNN has a large number of parameters, so that overfitting is a danger when they are trained on small training sets. We address this problem by pretraining MVCNN on unlabeled data. These pretrained weights can then be fine-tuned for the specific classification task. In sum, we attribute the success of MVCNN to: (i) designing variable-size convolution filters to extract variable-range features of sentences and (ii) exploring the combination of multiple public embedding versions to initialize words in sentences. We also employ two “tricks” to further enhance system performance: mutual learning and pretraining. In remaining parts, Section \"Related Work\" presents related work. Section \"Model Description\" gives details of our classification model. Section \"Model",
      "We now describe the architecture of our model MVCNN, illustrated in Figure 1 . Multichannel Input. The input of MVCNN includes multichannel feature maps of a considered sentence, each is a matrix initialized by a different embedding version. Let $s$ be sentence length, $d$ dimension of word embeddings and $c$ the total number of different embedding versions (i.e., channels). Hence, the whole initialized input is a three-dimensional array of size $c\\times d\\times s$ . Figure 1 depicts a sentence with $s=12$ words. Each word is initialized by $c=5$ embeddings, each coming from a different channel. In implementation, sentences in a mini-batch will be padded to the same length, and unknown words for corresponding channel are randomly initialized or can acquire good initialization from the",
      "Layer. Finally, sentence representation is forwarded into logistic regression layer for classification. In brief, our MVCNN model learns from BIBREF4 to use dynamic k-max pooling to stack multiple convolution layers, and gets insight from BIBREF5 to investigate variable-size filters in a convolution layer. Compared to BIBREF4 , MVCNN has rich feature maps as input and as output of each convolution layer. Its convolution operation is not only more flexible to extract features of variable-range phrases, but also able to model dependency among all dimensions of representations. MVCNN extends the network in BIBREF5 by hierarchical convolution architecture and further exploration of multichannel and variable-size feature detectors.",
      "This part introduces two training tricks that enhance the performance of MVCNN in practice. Mutual-Learning of Embedding Versions. One observation in using multiple embedding versions is that they have different vocabulary coverage. An unknown word in an embedding version may be a known word in another version. Thus, there exists a proportion of words that can only be partially initialized by certain versions of word embeddings, which means these words lack the description from other versions. To alleviate this problem, we design a mutual-learning regime to predict representations of unknown words for each embedding version by learning projections between versions. As a result, all embedding versions have the same vocabulary. This processing ensures that more words in each embedding",
      "Conclusion\nThis work presented MVCNN, a novel CNN architecture for sentence classification. It combines multichannel initialization – diverse versions of pretrained word embeddings are used – and variable-size filters – features of multigranular phrases are extracted with variable-size convolution filters. We demonstrated that multichannel initialization and variable-size filters enhance system performance on sentiment classification and subjectivity classification tasks.",
      "Abstract\nWe propose MVCNN, a convolution neural network (CNN) architecture for sentence classification. It (i) combines diverse versions of pretrained word embeddings and (ii) extracts features of multigranular phrases with variable-size convolution filters. We also show that pretraining MVCNN is critical for good performance. MVCNN achieves state-of-the-art performance on four tasks: on small-scale binary, small-scale multi-class and largescale Twitter sentiment prediction and on subjectivity classification.",
      "versions to extract higher quality sentence features and thereby improve sentence classification performance. The letters “M” and “V” in the name “MVCNN” of our architecture denote the multichannel and variable-size convolution filters, respectively. “Multichannel” employs language from computer vision where a color image has red, green and blue channels. Here, a channel is a description by an embedding version. For many sentence classification tasks, only relatively small training sets are available. MVCNN has a large number of parameters, so that overfitting is a danger when they are trained on small training sets. We address this problem by pretraining MVCNN on unlabeled data. These pretrained weights can then be fine-tuned for the specific classification task. In sum, we attribute the",
      "one specific configurational aspect of the system. All results in the five blocks are with respect to row 34, “MVCNN (overall)”; e.g., row 19 shows what happens when HLBL is removed from row 34, row 28 shows what happens when mutual learning is removed from row 34 etc. The block “baselines” (1–18) lists some systems representative of previous work on the corresponding datasets, including the state-of-the-art systems (marked as italic). The block “versions” (19–23) shows the results of our system when one of the embedding versions was not used during training. We want to explore to what extend different embedding versions contribute to performance. The block “filters” (24–27) gives the results when individual filter width is discarded. It also tells us how much a filter with specific size",
      "initialized embeddings and our mutual-learning is able to initialize the missing embeddings through projections. Pretraining is expected to learn good representations for all words, but pretraining is especially important for words without initialization (“no hit”); a particularly clear example for this is the Senti140 task: 236,484 of 387,877 words or 61% are in the “no hit” category. Table 3 compares results on test of MVCNN and its variants with other baselines in the four sentence classification tasks. Row 34, “MVCNN (overall)”, shows performance of the best configuration of MVCNN, optimized on dev. This version uses five versions of word embeddings, four filter sizes (3, 5, 7, 9), both mutual-learning and pretraining, three convolution layers for Senti140 task and two convolution",
      "papers study the comparative performance of different versions of word embeddings, usually learned by different neural network (NN) architectures. For example, chen2013expressive compared HLBL BIBREF9 , SENNA BIBREF2 , Turian BIBREF13 and Huang BIBREF14 , showing great variance in quality and characteristics of the semantics captured by the tested embedding versions. hill2014not showed that embeddings learned by neural machine translation models outperform three representative monolingual embedding versions: skip-gram BIBREF15 , GloVe BIBREF16 and C&W BIBREF3 in some cases. These prior studies motivate us to explore combining multiple versions of word embeddings, treating each of them as a distinct description of words. Our expectation is that the combination of these embedding versions,",
      "the system performs when it has different numbers of convolution layers. From the “layers” block, we can see that our system performs best with two layers of convolution in Standard Sentiment Treebank and Subjectivity Classification tasks (row 31), but with three layers of convolution in Sentiment140 (row 32). This is probably due to Sentiment140 being a much larger dataset; in such a case deeper neural networks are beneficial. The block “tricks” demonstrates the effect of mutual-learning and pretraining. Apparently, pretraining has a bigger impact on performance than mutual-learning. We speculate that it is because pretraining can influence more words and all learned word embeddings are tuned on the dataset after pretraining. The block “filters” indicates the contribution of each filter",
      "four sentence classification tasks. Row 34, “MVCNN (overall)”, shows performance of the best configuration of MVCNN, optimized on dev. This version uses five versions of word embeddings, four filter sizes (3, 5, 7, 9), both mutual-learning and pretraining, three convolution layers for Senti140 task and two convolution layers for the other tasks. Overall, our system gets the best results, beating all baselines. The table contains five blocks from top to bottom. Each block investigates one specific configurational aspect of the system. All results in the five blocks are with respect to row 34, “MVCNN (overall)”; e.g., row 19 shows what happens when HLBL is removed from row 34, row 28 shows what happens when mutual learning is removed from row 34 etc. The block “baselines” (1–18) lists some",
      "kernels. This means this convolution layer can detect three kinds of features of phrases with length 3 and 5, respectively. DCNN in BIBREF4 used one-dimensional convolution: each higher-order feature is produced from values of a single dimension in the lower-layer feature map. Even though that work proposed folding operation to model the dependencies between adjacent dimensions, this type of dependency modeling is still limited. Differently, convolution in present work is able to model dependency across dimensions as well as adjacent words, which obviates the need for a folding step. This change also means our model has substantially fewer parameters than the DCNN since the output of each convolution layer is smaller by a factor of $d$ . Dynamic k-max Pooling. blunsom2014convolutional"
    ]
  },
  {
    "title": "A Lightweight Front-end Tool for Interactive Entity Population",
    "answer": "The tool, LUWAK, is written in pure JavaScript code.",
    "evidence": [
      "based on the CommonCrawl corpus and the Twitter corpus. Note that the specification of the expansion algorithm is not limited to the algorithm described in this paper, as LUWAK considers the Expansion API as an external function. Moreover, we also utilize the category-based expansion module, in which we used is-a relationship between the ontological category and each entity and expanded seeds via category-level. For example, if most of the entities already inserted in the dictionary share the same category, such as Programming Languages, the system suggests that \"Programming Language\" entities should be inserted in the dictionary when we develop a job skill name dictionary. Category-based entity expansion is helpful to avoid the candidate entity one by one. We used Yago BIBREF3 as an",
      "and fastest solution to collecting entities of a particular entity type. User interaction cost is a dominant factor in the entire workload of an interactive tool. Thus, we carefully design the user interface for users to give feedbacks to the tool intuitively. Furthermore, we also consider the end-to-end user cost reduction. We adhere to the concept of developing installation-free software to distribute the tool among a wide variety of users, including nontechnical clusters. This lightweight design of LUWAK might speed up the procedure of the whole interactive entity population workflow. Furthermore, this advantage might be beneficial to continuously improve the whole pipeline of interactive entity population system.",
      "in which feedback is given to expanded candidates, as essential to maintaining the quality of an entity set. LUWAK implements fundamental functions for entity population, including (a) importing an initial entity set, (b) generating entity candidates, (c) obtaining user feedback, and (d) publishing populated entity dictionary. We aim to reduce the user’s total workload as a key metric of an entity population tool. That is, an entity population tool should provide the easiest and fastest solution to collecting entities of a particular entity type. User interaction cost is a dominant factor in the entire workload of an interactive tool. Thus, we carefully design the user interface for users to give feedbacks to the tool intuitively. Furthermore, we also consider the end-to-end user cost",
      "Implementation\nLUWAK is implemented in pure JavaScript code, and it uses the LocalStorage of a web browser. A user does not have to install any packages except for a web browser. The only thing the user must do is to download the LUWAK software and put it in a local directory. We believe the cost of installing the tool will keep away a good amount of potential users. This philosophy follows our empirical feeling of the curse of installing required packages/libraries. Moreover, LUWAK does not need users to consider the usage and maintenance of these additional packages. That is why we are deeply committed to making LUWAK a pure client-side tool in the off-the-shelf style."
    ]
  },
  {
    "title": "Modeling Trolling in Social Media Conversations",
    "answer": "The size of the dataset is 1000 conversations, which consists of 6833 sentences and 88047 tokens.",
    "evidence": [
      "Feature Sets",
      "In particular, we observe that GloVe vectors are the most powerful feature set, accuracy-wise, even better than the experiments with all features for all tasks except interpretation. The overall Total Accuracy score reported in table  using the entire feature set is 549. This result is what makes this dataset interesting: there is still lots of room for research on this task. Again, the primary goal of this experiment is to help identify the difficult-to-classify instances for analysis in the next section.",
      "trolling attempt, and all of the direct responses to the suspected trolling attempt. We had two human annotators who were trained on snippets (i.e., (suspected trolling attempt, responses) pairs) taken from 200 conversations and were allowed to discuss their findings. After this training stage, we asked them to independently label the four aspects for each snippet. We recognize that this limited amount of information is not always sufficient to recover the four aspects we are interested in, so we give the annotators the option to discard instances for which they couldn't determine the labels confidently. The final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens. The distribution over the classes per trolling aspect is shown in the table  in the"
    ]
  },
  {
    "title": "Rotations and Interpretability of Word Embeddings: the Case of the Russian Language",
    "answer": "The paper evaluates interpretability using a measure inspired by vector space models, where word vectors are normalized, and the word matrix $W$ is considered. Specifically, the authors suggest estimating the interpretability of a representation, although the exact formula is not provided in the given context. The traditional measure of interpretability involves selecting $n$ most probable words for each component, calculating a co-occurrence measure such as PMI for each pair of selected words, and averaging these values over all pairs and components.",
    "evidence": [
      "Interpretability measures",
      "One of traditional measures of interpretability in topic modeling looks as follows BIBREF29 , BIBREF19 . For each component, $n$ most probable words are selected. Then for each pair of selected words some co-occurrence measure such as PMI is calculated. These values are averaged over all pairs of selected words and all components. The other approaches use human markup. Such measures need additional data, and it is difficult to study them algebraically. Also, unlike topic modeling, word embeddings are not probabilistic: both positive and negative values of coordinates should be considered. Let all word vectors be normalized and $W$ be the word matrix. Inspired by BIBREF21 , where vector space models are used for evaluating topic coherence, we suggest to estimate the interpretability of $k$"
    ]
  },
  {
    "title": "Revisiting Low-Resource Neural Machine Translation: A Case Study",
    "answer": "The pitfalls mentioned in the paper include: \n1. PBSMT systems defaulting to copying unknown words, \n2. NMT systems producing suboptimal translations for unknown words on a subword-level, \n3. ultra low-resource NMT systems ignoring unknown words in favour of a more-or-less fluent but semantically inadequate translation, \nand 4. PBSMT producing less grammatical translations compared to NMT systems.",
    "evidence": [
      "Data and Preprocessing",
      "Table  shows some sample translations that represent typical errors of our PBSMT and NMT systems, trained with ultra-low (100k words) and low (3.2M words) amounts of data. For unknown words such as blutbefleckten (`bloodstained') or Spaniern (`Spaniards', `Spanish'), PBSMT systems default to copying, while NMT systems produce translations on a subword-level, with varying success (blue-flect, bleed; spaniers, Spanians). NMT systems learn some syntactic disambiguation even with very little data, for example the translation of das and die as relative pronouns ('that', 'which', 'who'), while PBSMT produces less grammatical translation. On the flip side, the ultra low-resource NMT system ignores some unknown words in favour of a more-or-less fluent, but semantically inadequate translation:"
    ]
  },
  {
    "title": "Attention Optimization for Abstractive Document Summarization",
    "answer": "The evaluation metrics used are relevance and readability. Relevance measures how much salient information the summary contains, and readability measures how fluent and grammatical the summary is.",
    "evidence": [
      "Experiments\nAutomatic Evaluation Result",
      "We also conduct human evaluation on the generated summaries. Similar to the previous work BIBREF3, BIBREF20, we randomly select 100 samples from the test set of CNN/Daily Mail dataset and ask 3 human testers to measure relevance and readability of each summary. Relevance is based on how much salient information does the summary contain, and readability is based on how fluent and grammatical the summary is. Given an article, different people may have different understandings of the main content of the article, the ideal situation is that more than one reference is paired with the articles. However, most of summarization datasets contain the pairs of article with a single reference summary due to the cost of annotating multi-references. Since we use the reference summaries as target"
    ]
  },
  {
    "title": "Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling",
    "answer": "The question is asking about the architectures explored to improve the sequence-to-sequence (seq2seq) model. According to the context, different architectures are explored for improving the prior multilingual seq2seq model. \n\nThe answer is: Different architectures. \n\nMore precisely, the context states: \"We also explore different architectures for improving the prior multilingual seq2seq model.\" Therefore, the correct answer is that the paper explores \"different architectures\" to improve the seq2seq model, specifically the prior multilingual seq2seq model.",
    "evidence": [
      "Sequence-to-Sequence Model",
      "The sequence-to-sequence (seq2seq) model proposed in BIBREF0 , BIBREF1 , BIBREF2 is a neural architecture for performing sequence classification and later adopted to perform speech recognition in BIBREF3 , BIBREF4 , BIBREF5 . The model allows to integrate the main blocks of ASR such as acoustic model, alignment model and language model into a single framework. The recent ASR advancements in connectionist temporal classification (CTC) BIBREF5 , BIBREF4 and attention BIBREF3 , BIBREF6 based approaches has created larger interest in speech community to use seq2seq models. To leverage performance gains from this model as similar or better to conventional hybrid RNN/DNN-HMM models requires a huge amount of data BIBREF7 . Intuitively, this is due to the wide-range role of the model in",
      "learning approach. We also explore different architectures for improving the prior multilingual seq2seq model. The paper also discusses the effect of integrating a recurrent neural network language model (RNNLM) with a seq2seq model during decoding. Experimental results show that the transfer learning approach from the multilingual model shows substantial gains over monolingual models across all 4 BABEL languages. Incorporating an RNNLM also brings significant improvements in terms of %WER, and achieves recognition performance comparable to the models trained with twice more training data."
    ]
  },
  {
    "title": "Leveraging Discourse Information Effectively for Authorship Attribution",
    "answer": "The previous state-of-the-art was achieved by CNN models that relied only on character-level n-grams, as reported by BIBREF3 and BIBREF4. Specifically, a convolutional neural network formulation for AA tasks was proposed by Shrestha et al. (2017), which reported state-of-the-art performance on a corpus of Twitter data.",
    "evidence": [
      "the discourse information, and (ii) integrate discourse features into the best text classifier (i.e., CNN-based models), in the expectation of achieving state-of-the-art results in AA.  BIBREF1 (henceforth F&H14) made the first comprehensive attempt at using discourse information for AA. They employ an entity-grid model, an approach introduced by BIBREF6 for the task of ordering sentences. This model tracks how the grammatical relations of salient entities (e.g., subj, obj, etc.) change between pairs of sentences in a document, thus capturing a form of discourse coherence. The grid is summarized into a vector of transition probabilities. However, because the model only records the transition between two consecutive sentences at a time, the coherence is local. BIBREF2 (henceforth F15)",
      "the envelope of the current performance ceiling by a large margin. Admittedly, in using the RST features with entity-grids, we lose the valuable RST tree structure. In future work, we intend to adopt more sophisticated methods such as RecNN, as per Ji:17, to retain more information from the RST trees while reducing the parameter size. Further, we aim to understand how discourse embeddings contribute to AA tasks, and find alternatives to coreference chains for shorter texts.",
      "Baseline-dataset experiments. All the baseline-dataset experiments are evaluated on novel-9. As a comparison to previous work (F15), we evaluate our models using a pairwise classification task with GR discourse features. In her model, novels are partitioned into 1000-word chunks, and the model is evaluated with accuracy. Surpassing F15's SVM model by a large margin, we then further evaluate the more difficult multi-class task, i.e., all-class prediction simultaneously, with both GR and RST discourse features and the more robust F1 evaluation. In this multi-class task, we implement two SVMs to extend F15's SVM models: (i) SVM2: a linear-kernel SVM which takes char-bigrams as input, as our CNNs, and (ii) SVM2-PV: an updated SVM2 which takes also probability vector features. Further, we are",
      "evidenced in the CNN2-DE (global) model (3.5 improvement in F1). On IMDB62, as expected with short text inputs (mean=349 words/review), the discourse features in general do not add further contribution. Even the best model CNN2-DE brings only marginal improvement, confirming our findings from varying the chunk size on novel-9, where discourse features did not help at this input size. Equipped with discourse features, SVM2-PV performs slightly better than SVM2 on novel-50 (by 0.4 with GR, 0.9 with RST features). On IMDB62, the same pattern persists for the SVMs: discourse features do not make noticeable improvements (by 0.0 and 0.5 with GR and RST respectively).",
      "in Table  . However, it is unclear how this pattern helps improve classification performance. We intend to investigate this question in future work. Global vs. Local featurization. As described in Section  , the global reading processes all the discourse features for one entity at a time, while the local approach reads one sentence (or one sentence pair) at a time. In all the relevant experiments, global featurization showed a clear performance advantage (on average 1 point gain in F1). Recall that the creation of the grids (both GR and RST) depend on coreference chains of entities (Section  ), and only the global reading scheme takes advantage of the coreference pattern whereas the local reading breaks the chains. To find out whether coreference pattern is the key to the performance",
      "etc.) change between pairs of sentences in a document, thus capturing a form of discourse coherence. The grid is summarized into a vector of transition probabilities. However, because the model only records the transition between two consecutive sentences at a time, the coherence is local. BIBREF2 (henceforth F15) further extends the entity-grid model by replacing grammatical relations with discourse relations from Rhetorical Structure Theory BIBREF7 . Their study uses a linear-kernel SVM to perform pairwise author classifications, where a non-discourse model captures lexical and syntactic features. They find that adding the entity-grid with grammatical relations enhances the non-discourse model by almost 1% in accuracy, and using RST relations provides an improvement of 3%. The study,",
      "a tree as in Figure  . F15 finds that RST relations are more effective for AA than grammatical relations. In our paper, we populate the entity-grid in the same way as F15's “Shallow RST-style” encoding, but use fine-grained instead of coarse-grained RST relations, and do not distinguish between intra-sentential and multi-sentential RST relations, or salient and non-salient entities. We explore various featurization techniques using the coding scheme. CNN model. shrestha2017 propose a convolutional neural network formulation for AA tasks (detailed in Section  ). They report state-of-the-art performance on a corpus of Twitter data BIBREF11 , and compare their models with alternative architectures proposed in the literature: (i) SCH: an SVM that also uses character n-grams, among other",
      "gain in F1). Recall that the creation of the grids (both GR and RST) depend on coreference chains of entities (Section  ), and only the global reading scheme takes advantage of the coreference pattern whereas the local reading breaks the chains. To find out whether coreference pattern is the key to the performance difference, we further ran a probe experiment where we read RST discourse relations in the order in which EDUs are arranged in the RST tree (i.e., left-to-right), and evaluated this model on novel-50 and IMDB62 with the same hyperparameter setting. The F1 scores turned out to be very close to the CNN2-DE (local) model, at 97.5 and 90.9. Based on this finding, we tentatively confirm the importance of the coreference pattern, and intend to further investigate how exactly it",
      "F15 extends the notion of tracking salient entities to RST. Instead of using grammatical relations in the grid, RST discourse relations are specified. An RST discourse relation defines the relationship between two or more elementary discourse units (EDUs), which are spans of text that typically correspond to syntactic clauses. In a relation, an EDU can function as a nucleus (e.g., result.N) or as a satellite (e.g., summary.S). All the relations in a document then form a tree as in Figure  . F15 finds that RST relations are more effective for AA than grammatical relations. In our paper, we populate the entity-grid in the same way as F15's “Shallow RST-style” encoding, but use fine-grained instead of coarse-grained RST relations, and do not distinguish between intra-sentential and",
      "tree of discourse relations for the input text, thus introducing a “global view.” The GR features, on the other hand, are more restricted to a “local view” on entities between consecutive sentences. While a deeper empirical investigation is needed, one can intuitively imagine that identifying authorship by focusing on the local transitions between grammatical relations (as in GR) is more difficult than observing how the entire text is organized (as in RST).",
      "the discourse-enhanced SVM2-PVs increase F1 by about 1 point, with overall much lower scores in comparison to the CNNs. In general, RST features work better than GR features. The results of the varying-sizes experiments are plotted in Figure  . Again, we observe the overall pattern that discourse features improve the F1 score, and RST features procure superior performance. Crucially, however, we note there is no performance boost below the chunk size of 1000 for GR features, and below 600 for RST features. Where discourse features do help, the GR-based models achieve, on average, 1 extra point on F1, and the RST-based models around 2. Generalization-dataset experiments. Table  summarizes the results of the generalization-dataset experiments. On novel-50, most discourse-enhanced models",
      "and below 600 for RST features. Where discourse features do help, the GR-based models achieve, on average, 1 extra point on F1, and the RST-based models around 2. Generalization-dataset experiments. Table  summarizes the results of the generalization-dataset experiments. On novel-50, most discourse-enhanced models improve the performance of the baseline non-discourse CNN2 to varying degrees. The clear pattern again emerges that RST features work better, with the best F1 score evidenced in the CNN2-DE (global) model (3.5 improvement in F1). On IMDB62, as expected with short text inputs (mean=349 words/review), the discourse features in general do not add further contribution. Even the best model CNN2-DE brings only marginal improvement, confirming our findings from varying the chunk size",
      "neural networks (CNNs) have demonstrated considerable success on AA relying only on character-level  -grams BIBREF3 , BIBREF4 . The strength of these models is evidenced by findings that traditional stylometric features such as word  -grams and POS-tags do not improve, and can sometimes even hurt performance BIBREF3 , BIBREF5 . However, none of these CNN models make use of discourse. Our work builds upon these prior studies by exploring an effective method to (i) featurize the discourse information, and (ii) integrate discourse features into the best text classifier (i.e., CNN-based models), in the expectation of achieving state-of-the-art results in AA.  BIBREF1 (henceforth F&H14) made the first comprehensive attempt at using discourse information for AA. They employ an entity-grid",
      "an input-length threshold for the discourse features to help (Section  ). Not surprisingly, discourse does not contribute on shorter texts. Many of the feature grids are empty for these shorter texts– either there are no coreference chains or they are not correctly resolved. Currently we only have empirical results on short novel chunks and movie reviews, but believe the finding would generalize to Twitter or blog posts. Discourse embeddings. It does not come as a surprise that discourse embedding-based models perform better than their relation probability-based peers. The former (i) leverages the weight learning of the entire computational graph of the CNN rather than only the softmax layer, as the PV models do, and (ii) provides a more fine-grained featurization of the discourse",
      "both GR and RST discourse features and the more robust F1 evaluation. In this multi-class task, we implement two SVMs to extend F15's SVM models: (i) SVM2: a linear-kernel SVM which takes char-bigrams as input, as our CNNs, and (ii) SVM2-PV: an updated SVM2 which takes also probability vector features. Further, we are interested in finding a performance threshold on the minimally-required input text length for discourse information to “kick in”. To this end, we chunk the novels into different sizes: 200-2000 words, at 200-word intervals, and evaluate our CNNs in the multi-class condition. Generalization-dataset experiments. To confirm that our models generalize, we pick the best models from the baseline-dataset experiments and evaluate on the novel-50 and IMDB62 datasets. For novel-50,",
      "The statistics for the three datasets used in the experiments are summarized in Table  . novel-9. This dataset was compiled by F&H14: a collection of 19 novels by 9 nineteenth century British and American authors in the Project Gutenberg. To compare to F&H14, we apply the same resampling method (F&H14, Section 4.2) to correct the imbalance in authors by oversampling the texts of less-represented authors. novel-50. This dataset extends novel-9, compiling the works of 50 randomly selected authors of the same period. For each author, we randomly select 5 novels for a total 250 novels. IMDB62. IMDB62 consists of 62K movie reviews from 62 users (1,000 each) from the Internet Movie dataset, compiled by Seroussi:11. Unlike the novel datasets, the reviews are considerably shorter, with a mean of",
      "SVM to perform pairwise author classifications, where a non-discourse model captures lexical and syntactic features. They find that adding the entity-grid with grammatical relations enhances the non-discourse model by almost 1% in accuracy, and using RST relations provides an improvement of 3%. The study, however, works with only one small dataset and their models produce overall unremarkable performance (  85%). BIBREF8 propose an advanced Recursive Neural Network (RecNN) architecture to work with RST in the more general area of text categorization and present impressive results. However, we suspect that the massive number of parameters of RecNNs would likely cause overfitting when working with smaller datasets, as is often the case in AA tasks. In our paper, we opt for a",
      "As described in Section  , in both the GR and RST variants, from each input entry we start by obtaining an entity grid. CNN2-PV. We collect the probabilities of entity role transitions (in GR) or discourse relations (in RST) for the entries. Each entry corresponds to a probability distribution vector. CNN2-DE. We employ two schema for creating discourse feature sequences from an entity grid. While we always read the grid by column (by a salient entity), we vary whether we track the entity across a number of sentences (n rows at a time) or across the entire document (one entire column at a time), denoted as local and global reading respectively. For the GR discourse features, in the case of local reading, we process the entity roles one sentence pair at a time (Figure  , left). For",
      "and evaluated this model on novel-50 and IMDB62 with the same hyperparameter setting. The F1 scores turned out to be very close to the CNN2-DE (local) model, at 97.5 and 90.9. Based on this finding, we tentatively confirm the importance of the coreference pattern, and intend to further investigate how exactly it matters for the classification performance. GR vs. RST. RST features in general effect higher performance gains than GR features (Table  ). The RST parser produces a tree of discourse relations for the input text, thus introducing a “global view.” The GR features, on the other hand, are more restricted to a “local view” on entities between consecutive sentences. While a deeper empirical investigation is needed, one can intuitively imagine that identifying authorship by focusing on",
      "General analysis. Overall, we have shown that discourse information can improve authorship attribution, but only when properly encoded. This result is critical in demonstrating the particular value of discourse information, because typical stylometric features such as word  -grams and POS tags do not add additional performance improvements BIBREF3 , BIBREF5 . In addition, the type of discourse information and the way in which it is featurized are tantamount to this performance improvement: RST features provide overall stronger improvement, and the global reading scheme for discourse embedding works better than the local one. The discourse embedding proves to be a superior featurization technique, as evidenced by the generally higher performance of CNN2-DE models over CNN2-PV models. With",
      "CNN. Specifically, We explore these questions using two approaches to represent salient entities: grammatical relations, and RST discourse relations. We apply these models to datasets of varying sizes and genres, and find that adding any discourse information improves AA consistently on longer documents, but has mixed results on shorter documents. Further, embedding the discourse features in a parallel CNN at the input end yields better performance than concatenating them to the output layer as a feature vector (Section  ). The global featurization is more effective than the local one. We also show that SVMs, which can only use discourse probability vectors, neither produce a competitive performance (even with fine-tuning), nor generalize in using the discourse information effectively.",
      "Baseline-dataset experiments. The results of the baseline-dataset experiments are reported in Table  ,  and Figure  . In Table  , Baseline denotes the dumb baseline model which always predicts the more-represented author of the pair. Both SVMs are from F15, and we report her results. SVM (LexSyn) takes character and word bi/trigrams and POS tags. SVM (LexSyn-PV) additionally includes probability vectors, similar to our CNN2-PV. In this part of the experiment, while the CNNs clear a large margin over SVMs, adding discourse in CNN2-PV brings only a small performance gain. Table  reports the results from the multi-class classification task, the more difficult task. Here, probability vector features (i.e., PV) again fail to contribute much. The discourse embedding features, on the other hand,"
    ]
  },
  {
    "title": "Rapid Adaptation of BERT for Information Extraction on Domain-Specific Business Documents",
    "answer": "The evaluation metrics used for presenting the results were F$_1$, precision, and recall. These metrics were computed for all content elements (micro-averaged) as well as broken down by types.",
    "evidence": [
      "Approach\nInference and Evaluation",
      "Our main results are presented in Table  on the test set of the regulatory filings and in Table  on the test set of the property lease agreements; F$_1$, precision, and recall are computed in the manner described above. We show metrics across all content elements (micro-averaged) as well as broken down by types. For the property lease agreements, we show results on all documents (left) and only over those with unseen templates (right). Examining these results, we see that although there is some degradation in effectiveness between all documents and only unseen templates, it appears that BERT is able to generalize to previously-unseen expressions of the content elements. Specifically, it is not the case that the model is simply memorizing fixed patterns or key phrases—otherwise, we could"
    ]
  },
  {
    "title": "Learning Twitter User Sentiments on Climate Change with Limited Labeled Data",
    "answer": "The five natural disasters examined were: \n1. The East Coast Bomb Cyclone (January 2-6, 2018)\n2. The Mendocino, California wildfires (July 27 - September 18, 2018)\n3. Hurricane Florence (August 31 - September 19, 2018)\n4. Hurricane Michael (October 7-16, 2018)\n5. The California Camp Fires (November 8-25, 2018).",
    "evidence": [
      "yielding a total of 16,360 influential tweets. The second data batch consists of event-related tweets for five natural disasters occurring in the U.S. in 2018. These are: the East Coast Bomb Cyclone (Jan. 2 - 6); the Mendocino, California wildfires (Jul. 27 - Sept. 18); Hurricane Florence (Aug. 31 - Sept. 19); Hurricane Michael (Oct. 7 - 16); and the California Camp Fires (Nov. 8 - 25). For each disaster, we scraped tweets starting from two weeks prior to the beginning of the event, and continuing through two weeks after the end of the event. Summary statistics on the downloaded event-specific tweets are provided in Table  . Note that the number of tweets occurring prior to the two 2018 sets of California fires are relatively small. This is because the magnitudes of these wildfires were"
    ]
  },
  {
    "title": "Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding",
    "answer": "The knowledge graph completion tasks experimented with are link prediction and triplet classification. These two tasks are specifically mentioned as the typical knowledge graph completion tasks used to evaluate the effectiveness of the LAN model.",
    "evidence": [
      "Experimental Configurations\nWe evaluate the effectiveness of our LAN model on two typical knowledge graph completion tasks, i.e., link prediction and triplet classification. We compare our LAN with two baseline aggregators, MEAN and LSTM, as described in the Encoder section. MEAN is used on behalf of pooling functions since it leads to the best performance in BIBREF9 ijcai2017-250. LSTM is used due to its large expressive capability BIBREF26 ."
    ]
  },
  {
    "title": "The First Evaluation of Chinese Human-Computer Dialogue Technology",
    "answer": "The evaluation metrics used for Task 2 are: \n1. Task completion ratio, \n2. User satisfaction degree, and \n3. Response fluency. \nThere are five evaluation metrics in total for Task 2.",
    "evidence": [
      "Evaluation Data",
      "user intent, the initial sentence, which is used to start the dialogue, is the same. The tester then begin to converse to each system. A dialogue is finished if the system successfully returns the information which the user inquires or the number of dialogue turns is larger than 30 for a single task. For building the dialogue systems of participants, we release an example set of complete user intent and three data files of flight, train and hotel in JSON format. There are five evaluation metrics for task 2 as following. Task completion ratio: The number of completed tasks divided by the number of total tasks. User satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively. Response fluency:"
    ]
  },
  {
    "title": "BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization",
    "answer": "The templates are discovered through a multi-stage process for automatic retrieval from the training corpus. This process is proposed due to the limitations in obtaining handcrafted templates. The approach is used to retrieve high-quality templates from the training corpus.",
    "evidence": [
      "summary generation. Due to the limitations in obtaining handcrafted templates, we further propose a multi-stage process for automatic retrieval of high-quality templates from training corpus. Extensive experiments were conducted on the Gigaword dataset BIBREF0 , a public dataset widely used for abstractive sentence summarization, and the results appear to be quite promising. Merely using the templates selected by our approach as the final summaries, our model can already achieve superior performance to some baseline models, demonstrating the effect of our templates. This may also indicate the availability of many quality templates in the corpus. Secondly, the template-equipped summarization model, BiSET, outperforms all the state-of-the-art models significantly. To evaluate the importance"
    ]
  },
  {
    "title": "AttSum: Joint Learning of Focusing and Summarization with Neural Attention",
    "answer": "The models compared to AttSum are: \n1. ISOLATION \n2. LEAD \n3. QUERY_SIM \n4. DocEmb \n5. MultiMR \n6. SVR \n\nThese models are compared in terms of their ROUGE scores, as presented in Table 2.",
    "evidence": [
      "Model Setting",
      "can answer the query to a large extent. For instance, when asked to tell the advantages of drug legalization, AttSum catches the sentences about drug trafficking prevention, the control of marijuana use, and the economic effectiveness, etc. All these aspects are mentioned in reference summaries. The sentences with the high TF-IDF similarity, however, are usually short and simply repeat the key words in the query. The advantage of AttSum over TF-IDF similarity is apparent in query relevance ranking. When there are multiple sentences in a query, AttSum may only focus on a part of them. Take the second query in Table 3 as an example. Although the responses to all the four query sentences are involved more or less, we can see that AttSum tends to describe the steps of wetland preservation",
      "The ROUGE scores of the different summarization methods are presented in Table 2 . We consider ROUGE-2 as the main evaluation metrics, and also provide the ROUGE-1 results as the common practice. As can be seen, AttSum always enjoys a reasonable increase over ISOLATION, indicating that the joint model indeed takes effects. With respect to other methods, AttSum largely outperforms two baselines (LEAD and QUERY_SIM) and the unsupervised neural network model DocEmb. Although AttSum is totally data-driven, its performance is better than the widely-used summarization systems MultiMR and SVR. It is noted that SVR heavily depends on hand-crafted features. Nevertheless, AttSum almost outperforms SVR all the time. The only exception is DUC 2005 where AttSum is slightly inferior to SVR in terms of"
    ]
  },
  {
    "title": "A Simple Approach to Multilingual Polarity Classification in Twitter",
    "answer": "The multilingual framework consists of cross-language features. These features are a set of features that can be applied to most similar languages, including features such as punctuation, diacritics, symbol duplication, and case sensitivity. The combination of these features is explored to find the best configuration for a given classifier. Specifically, the parameters considered as spelling features include punctuation (del-punc). The framework is designed to be simple to implement and easy to use, serving as a baseline for sentiment analysis contests and a starting point to build new sentiment analysis systems.",
    "evidence": [
      "Cross-language Features",
      "Language Dependent Features",
      "techniques that optimize some performance measures where the chosen techniques are robust to typical writing errors. In this context, we propose a robust multilingual sentiment analysis method, tested in eight different languages: Spanish, English, Italian, Arabic, German, Portuguese, Russian and Swedish. We compare our approach ranking in three international contests: TASS'15, SemEval'15-16 and SENTIPOLC'14, for Spanish, English and Italian respectively; the remaining languages are compared directly with the results reported in the literature. The experimental results locate our approach in good positions for all considered competitions; and excellent results in the other five languages tested. Finally, even when our method is almost cross-language, it can be extended to take advantage",
      "aim is to provide a simple to implement and easy to use multilingual framework, that can serve as a baseline for sentiment analysis contests, and as starting point to build new sentiment analysis systems. We compare our approach in eight different languages, three of them have important international contests, namely, SemEval (English), TASS (Spanish), and SENTIPOLC (Italian). Within the competitions our approach reaches from medium to high positions in the rankings; whereas in the remaining languages our approach outperforms the reported results.",
      "We defined cross-language features as a set of features that could be applied in most similar languages, not only related language families such as Germanic languages (English, German, etc.), Romance languages (Spanish, Italian, etc.), among others; but also similar surface features such as punctuation, diacritics, symbol duplication, case sensitivity, etc. Later, the combination of these features will be explored to find the best configuration for a given classifier. Generally, Twitter messages are full of slang, misspelling, typographical and grammatical errors among others; in order to tackle these aspects we consider different parameters to study this effect. The following points are the parameters to be considered as spelling features. Punctuation (del-punc) considers the use of",
      "Recently, sentiment analysis has received a lot of attention due to the interest in mining opinions of social media users. Sentiment analysis consists in determining the polarity of a given text, i.e., its degree of positiveness or negativeness. Traditionally, Sentiment Analysis algorithms have been tailored to a specific language given the complexity of having a number of lexical variations and errors introduced by the people generating content. In this contribution, our aim is to provide a simple to implement and easy to use multilingual framework, that can serve as a baseline for sentiment analysis contests, and as starting point to build new sentiment analysis systems. We compare our approach in eight different languages, three of them have important international contests, namely,",
      "Figure  shows the performance on four contests, corresponding to three different languages. The performance corresponds to the multilingual set of features, i.e., we do not used language-dependent techniques. Figures  -  illustrates the results on each challenge, all competitors are ordered in score's descending order (higher is better). The achieved performance of our approach is marked with a horizontal line on each figure. Figure  briefly describes each challenge and summarizes our performance on each contest; also, we added three standard measures to simplify the insight's creation of the reader. The winner method in SENTIPOLC'14 (Italian) is reported in BIBREF22 . This method uses three groups of features: keyword and micro-blogging characteristics, Sentiment Lexicons, SentiWordNet"
    ]
  },
  {
    "title": "Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing",
    "answer": "The baseline used is GraphParser without paraphrases. Additionally, a monolingual phrase-based machine translation (MT) system, specifically Moses trained on the Paralex corpus, is used as another baseline for comparison with the paraphrasing models.",
    "evidence": [
      "Baselines\nWe use GraphParser without paraphrases as our baseline. This gives an idea about the impact of using paraphrases. We compare our paraphrasing models with monolingual machine translation based model for paraphrase generation BIBREF24 , BIBREF36 . In particular, we use Moses BIBREF37 to train a monolingual phrase-based MT system on the Paralex corpus. Finally, we use Moses decoder to generate 10-best distinct paraphrases for the test questions."
    ]
  },
  {
    "title": "Story Ending Generation with Incremental Encoding and Commonsense Knowledge",
    "answer": "The baselines used are Seq2Seq, HLSTM, and HLSTM+Copy. These three models are mentioned as the baselines in the provided context.",
    "evidence": [
      "We presented an example of generated story endings in Table 3 . Our model generates more natural and reasonable endings than the baselines. In this example, the baselines predicted wrong events in the ending. Baselines (Seq2Seq, HLSTM, and HLSTM+Copy) have predicted improper entities (cake), generated repetitive contents (her family), or copied wrong words (eat). The models equipped with incremental encoding or knowledge through MSA(GA/CA) perform better in this example. The ending by IE+MSA is more coherent in logic, and fluent in grammar. We can see that there may exist multiple reasonable endings for the same story context. In order to verify the ability of our model to utilize the context clues and implicit knowledge when planning the story plot, we visualized the attention weights of"
    ]
  },
  {
    "title": "Multi-Source Syntactic Neural Machine Translation",
    "answer": "The question is asking about the performance drop of the model when there is no parsed input. According to the context, the parse2seq system fails when given only sequential source data, implying a significant performance drop. In contrast, both multi-source systems perform reasonably well without parsed data, although the BLEU scores are lower. \n\nThe exact performance drop is not directly stated, but it can be inferred that the parse2seq system experiences a substantial decline, while the multi-source systems have a relatively smaller drop in BLEU scores.",
    "evidence": [
      "The parse2seq and multi-source systems require parsed source data at inference time. However, the parser may fail on an input sentence. Therefore, we examine how well these systems do when given only unparsed source sentences at test time. Table  displays the results of these experiments. For the parse2seq baseline, we use only sequential (seq) data as input. For the lexicalized and unlexicalized multi-source systems, two options are considered: seq + seq uses identical sequential data as input to both encoders, while seq + null uses null input for the parsed encoder, where every source sentence is “( )”. The parse2seq system fails when given only sequential source data. On the other hand, both multi-source systems perform reasonably well without parsed data, although the BLEU scores are"
    ]
  },
  {
    "title": "Domain Adaptation for Neural Networks by Parameter Augmentation",
    "answer": "The question is referring to the domain adaptation from MS COCO to Flickr30K or vice versa. According to the context, the number of samples in the target domain depends on the direction of adaptation. \n\nFor adaptation from MS COCO to Flickr30K, Flickr30K is the target domain with 30K images. \nFor adaptation from Flickr30K to MS COCO, MS COCO is the target domain. MS COCO contains approximately 80K images for training and 40K images for validation.\n\nTherefore, the number of examples in the target domain is either 30K (Flickr30K) or 80K (MS COCO training set) or 120K (total MS COCO training and validation sets). To be precise, we need to know the direction of adaptation. \n\nIf we consider the total number of images (training + validation), for MS COCO to Flickr30K, the target domain (Flickr30K) has 30K images. For Flickr30K to MS COCO, the target domain has 80K (training) + 40K (validation) = 120K images. \n\nSo, the answer is either 30K or 120K depending on the direction of adaptation. More specifically",
    "evidence": [
      "This experiment highlights a typical scenario in which domain adaptation is useful. Suppose that we have a large dataset of captioned images, which are taken from daily lives, but we would like to generate high quality captions for more specialized domain images such as minor sports and exotic food. However, captioned images for those domains are quite limited due to the annotation cost. We use domain adaptation methods to improve the captions of the target domain. To simulate the scenario, we split the Microsoft COCO dataset into food and non-food domain datasets. The MS COCO dataset contains approximately 80K images for training and 40K images for validation; each image has 5 captions BIBREF12 . The dataset contains images of diverse categories, including animals, indoor scenes, sports,",
      "In this experiment, we explore the benefit of adaptation from both sides of the domains. Flickr30K is another captioning dataset, consisting of 30K images, and each image has five captions BIBREF16 . Although the formats of the datasets are almost the same, the model trained by the MS COCO dataset does not work well for the Flickr 30K dataset and vice versa. The word distributions of the captions are considerably different. If we ignore words with less than 30 counts, MS COCO has 3,655 words and Flicker30K has 2732 words; and only 1,486 words are shared. Also, the average lengths of captions are different. The average length of captions in Flickr30K is 12.3 while that of MS COCO is 10.5. The first result is the domain adaptation from MS COCO to Flickr30K, summarized in Table 5 . Again, we",
      "the scenario, we split the Microsoft COCO dataset into food and non-food domain datasets. The MS COCO dataset contains approximately 80K images for training and 40K images for validation; each image has 5 captions BIBREF12 . The dataset contains images of diverse categories, including animals, indoor scenes, sports, and foods. We selected the “food category” data by scoring the captions according to how much those are related to the food category. The score is computed based on wordnet similarities BIBREF13 . The training and validation datasets are split by the score with the same threshold. Consequently, the food dataset has 3,806 images for training and 1,775 for validation. The non-food dataset has 78,976 images for training and 38,749 for validation. The selected pictures from the",
      "has 3,655 words and Flicker30K has 2732 words; and only 1,486 words are shared. Also, the average lengths of captions are different. The average length of captions in Flickr30K is 12.3 while that of MS COCO is 10.5. The first result is the domain adaptation from MS COCO to Flickr30K, summarized in Table 5 . Again, we observe that the proposed method achieves the best score among the other methods. The difference between All and FineTune is bigger than in the previous setting because two datasets have different captions even for similar images. The scores of FineTune and Dual are at almost the same level. The second result is the domain adaptation from Flickr30K to MS COCO shown in Table 6 . This may not be a typical situation because the number of samples in the target domain is larger",
      "is not necessarily true that there is a correspondence between the source and target input distributions, and therefore we cannot expect their method to work well. wen2016multi have proposed a procedure to generate natural language for multiple domains of spoken dialogue systems. They improve the fine tuning method by pre-trainig with synthesized data. However, the synthesis protocol is only applicable to the spoken dialogue system. In this paper, we focus on domain adaptation methods which can be applied without dataset-specific tricks. yang2016multitask have conducted a series of experiments to investigate the transferability of neural networks for NLP. They compare the performance of two transfer methods called INIT and MULT, which correspond to the fine tuning and dual outputs methods",
      "on wordnet similarities BIBREF13 . The training and validation datasets are split by the score with the same threshold. Consequently, the food dataset has 3,806 images for training and 1,775 for validation. The non-food dataset has 78,976 images for training and 38,749 for validation. The selected pictures from the food domain are typically a close-up of foods or people eating some foods. Table 1 shows some captions from the food and non-food domain datasets. Table 2 shows the top twenty frequent words in the two datasets except for the stop words. We observe that the frequent words are largely different, but still there are some words common in both datasets. To model the image captaining, we use LSTMs as described in the previous section. The image features are computed by the trained"
    ]
  },
  {
    "title": "Interactive Machine Comprehension with Information Seeking Agents",
    "answer": "The setup provides models with actions such as \"Ctrl+F\" (search for token) and \"stop\" for searching through partially observed documents. These commands enable the models to iteratively search for relevant information in the withheld sentences, conditioning each command on the input question and the sentences they have observed.",
    "evidence": [
      "models with sufficient information in fully observable, static documents. Instead, we propose interactive versions of existing MRC tasks, whereby the information needed to answer a question must be gathered sequentially. The key idea behind our proposed interactive MRC (iMRC) is to restrict the document context that a model observes at one time. Concretely, we split a supporting document into its component sentences and withhold these sentences from the model. Given a question, the model must issue commands to observe sentences in the withheld set; we equip models with actions such as Ctrl+F (search for token) and stop for searching through partially observed documents. A model searches iteratively, conditioning each command on the input question and the sentences it has observed"
    ]
  },
  {
    "title": "Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models",
    "answer": "The authors use various combinations of RNNs and HMMs to reach the conclusion that LSTMs and HMMs learn complementary information. Specifically, they explore three methods: (1) training an HMM on LSTM states, (2) a hybrid model where an HMM is trained first, and then a small LSTM is trained on HMM state distributions to fill in gaps in the HMM's performance, and (3) a jointly trained hybrid model. These combinations allow them to assess the complementary information learned by LSTMs and HMMs.",
    "evidence": [
      "model. We explore various combinations of RNNs and HMMs: an HMM trained on LSTM states; a hybrid model where an HMM is trained first, then a small LSTM is given HMM state distributions and trained to fill in gaps in the HMM's performance; and a jointly trained hybrid model. We find that the LSTM and HMM learn complementary information about the features in the text."
    ]
  },
  {
    "title": "BERT-Based Arabic Social Media Author Profiling",
    "answer": "The in-house data employed are two datasets: \n\n1. A manually labeled dataset of 1,100 users with gender tags, comprising 550 female and 550 male users, along with 162,829 tweets obtained by crawling their timelines, used for the gender classification task.\n\n2. A gold-labeled dataset with 298,929 tweets across 15 classes, used for the dialect identification task, with 20,000 tweets sampled for each class, except for Sudan which has 18,929 tweets.",
    "evidence": [
      "To further improve the performance of our models, we introduce in-house labeled data that we use to fine-tune BERT. For the gender classification task, we manually label an in-house dataset of 1,100 users with gender tags, including 550 female users, 550 male users. We obtain 162,829 tweets by crawling the 1,100 users' timelines. We combine this new gender dataset with the gender TRAIN data (from shared task) to obtain an extended dataset, to which we refer as EXTENDED_Gender. For the dialect identification task, we randomly sample 20,000 tweets for each class from an in-house dataset gold labeled with the same 15 classes as the shared task. In this way, we obtain 298,929 tweets (Sudan only has 18,929 tweets). We combine this new dialect data with the shared task dialect TRAIN data to"
    ]
  },
  {
    "title": "WiSeBE: Window-based Sentence Boundary Evaluation",
    "answer": "The two SBD (Segment Boundary Detection) systems compared were S1 and S2.",
    "evidence": [
      "We ran both systems (S1 & S2) over the manually transcribed videos obtaining the number of boundaries shown in Table  . In general, it can be seen that S1 predicts  more segments than S2. This difference can affect the performance of S1, increasing its probabilities of false positives. Table  condenses the performance of both systems evaluated against each one of the references independently. If we focus on F1 scores, performance of both systems varies depending of the reference. For  , S1 was better in 5 occasions with respect of S2; S1 was better in 2 occasions only for  ; S1 overperformed S2 in 3 occasions concerning  and in 4 occasions for  (bold). Also from Table  we can observe that  has a bigger similarity to S1 in 5 occasions compared to other two references, while  is more"
    ]
  },
  {
    "title": "Recognizing Musical Entities in User-generated Content",
    "answer": "The authors explored the following machine learning algorithms: \n\n1. John Platt's sequential minimal optimization algorithm for training a support vector classifier (SVM).\n2. Decision Trees.\n3. Naive Bayes is not explicitly mentioned but \"Naive\" likely refers to Naive Bayes.\n\nTherefore, the machine learning algorithms explored are: Support Vector Classifier (SVM), Decision Trees, and likely Naive Bayes.",
    "evidence": [
      "\"minor\", \"m\"). Finally, we complete the tokens' description including as token's features the surface form, the POS and the chunk tag of the previous and the following two tokens (12 features). We consider two sets of GloVe word embeddings BIBREF20 for training the neural architecture, one pre-trained with 2B of tweets, publicy downloadable, one trained with a corpora of 300K tweets collected during the 2014-2017 BBC Proms Festivals and disjoint from the data used in our experiments. The first model considered for this task has been the John Platt's sequential minimal optimization algorithm for training a support vector classifier BIBREF21 , implemented in WEKA BIBREF22 . Indeed, in BIBREF23 results shown that SVM outperforms other machine learning models, such as Decision Trees and Naive"
    ]
  },
  {
    "title": "Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and NER",
    "answer": "The context does not provide a direct comparison of the performance gains of adversarial learning for text classification versus NER. However, it mentions that improvements were reported on both the multilingual MLDoc text classification and CoNLL2002/2003 named entity recognition (NER) tasks. To determine whether adversarial learning has stronger performance gains for one task over the other, specific numerical results or a direct comparison would be needed, but this information is not provided in the given context. \n\nTherefore, based on the provided context, it is not possible to conclude whether adversarial learning has stronger performance gains for text classification or for NER.",
    "evidence": [
      "Abstract\nContextual word embeddings (e.g. GPT, BERT, ELMo, etc.) have demonstrated state-of-the-art performance on various NLP tasks. Recent work with the multilingual version of BERT has shown that the model performs very well in zero-shot and zero-resource cross-lingual settings, where only labeled English data is used to finetune the model. We improve upon multilingual BERT's zero-resource cross-lingual performance via adversarial learning. We report the magnitude of the improvement on the multilingual MLDoc text classification and CoNLL 2002/2003 named entity recognition tasks. Furthermore, we show that language-adversarial training encourages BERT to align the embeddings of English documents and their translations, which may be the cause of the observed performance gains."
    ]
  }
]