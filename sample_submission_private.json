[
  {
    "title": "How Document Pre-processing affects Keyphrase Extraction Performance",
    "answer": "***244*** \n\nThis number directly answers the question based on the context provided, which specifies that the SemEval-2010 benchmark dataset BIBREF0 is composed of 244 scientific articles.",
    "evidence": [
      ". This dataset is composed of documents (scientific articles) that were automatically converted from PDF format to plain text. As a result, most documents contain irrelevant pieces of text (e.g. muddled sentences, tables, equations, footnotes) that require special handling, so as to not hinder the performance of keyphrase extraction systems. In previous work, these are usually removed at the",
      "The SemEval-2010 benchmark dataset BIBREF0 is composed of 244 scientific articles collected from the ACM Digital Library (conference and workshop papers). The input papers ranged from 6 to 8 pages and were converted from PDF format to plain text using an off-the-shelf tool. The only preprocessing applied is a systematic dehyphenation at line breaks and removal of author-assigned keyphrases."
    ]
  },
  {
    "title": "Comparative Studies of Detecting Abusive Language on Twitter",
    "answer": "A message can be regarded as harmless on its own, but when taking previous threads into account it may be seen as abusive, and vice versa.",
    "evidence": [
      "The context-dependent nature of online aggression makes annotating large collections of data extremely difficult. Previously studied datasets in abusive language detection have been insufficient in size to efficiently train deep learning models. Recently, Hate and Abusive Speech on Twitter, a dataset much greater in size and reliability, has been released. However, this dataset has not been",
      "issue BIBREF7 , BIBREF8 . The major reason of the failure in abusive language detection comes from its subjectivity and context-dependent characteristics BIBREF9 . For instance, a message can be regarded as harmless on its own, but when taking previous threads into account it may be seen as abusive, and vice versa. This aspect makes detecting abusive language extremely laborious even for human"
    ]
  },
  {
    "title": "Prepositional Attachment Disambiguation Using Bilingual Parsing and Alignments",
    "answer": "The DD algorithm enforces agreement between two parse trees by ensuring that if two words in the English sentence (denoted by i and i') are aligned to words in a parallel Hindi sentence (denoted by j and j'), then a dependency edge between i and i' in the English parse tree corresponds to a similar dependency edge between j and j' in the Hindi parse tree, subject to the given alignments. \n\nHowever, to give a more ***concise*** and precise answer based on the provided context:\n\nIt works by ensuring dependency edges between aligned words in the English and Hindi parse trees are consistent.",
    "evidence": [
      "to enforce agreement between the two parse trees subject to the given alignments. Let us take a closer look at what we mean by agreement between the two parse trees. Essentially, if we have two words in the English sentence denoted by i and i', aligned to words j and j' in the parallel Hindi sentence respectively, we can expect a dependency edge between i and i' in the English parse tree to",
      "trees for the English and Hindi sentences from the respective trained parser models as an initialsiation step. The DD algorithm then tries to enforce agreement between the two parse trees subject to the given alignments. Let us take a closer look at what we mean by agreement between the two parse trees. Essentially, if we have two words in the English sentence denoted by i and i', aligned to"
    ]
  },
  {
    "title": "NeuronBlocks: Building Your NLP DNN Models Like Playing Lego",
    "answer": "The commonly used components of deep neural networks. \n\nMore specifically, they are categorized into several groups according to their functions, and within each category, there are several alternative components encapsulated into standard and reusable blocks. \n\nHowever, based on the context provided, a more concise answer can be: \n**deep neural network components**. \n\nOr, alternatively, \n**neural network blocks**. \n\nA more detailed and precise answer based on the context is: \n**Blocks, categorized into groups by function**. \n\nHowever, to provide a ***concise*** and precise answer directly: \n**Blocks**.",
    "evidence": [
      "The Neuronblocks is built on PyTorch. The overall framework is illustrated in Figure  . It consists of two layers: the Block Zoo and the Model Zoo. In Block Zoo, the most commonly used components of deep neural networks are categorized into several groups according to their functions. Within each category, several alternative components are encapsulated into standard and reusable blocks with a"
    ]
  },
  {
    "title": "Evaluating Tag Recommendations for E-Book Annotation Using a Semantic Similarity Metric",
    "answer": "19 different algorithms.",
    "evidence": [
      "vocabularies and then using it to compare how semantically similar the recommended tags are to the expected review keywords. For this, we first extract the textual content in the form of the description text, title, editor tags and Amazon search terms of e-books from our dataset. We then train a Doc2Vec BIBREF7 model on the content. Then, we use the model to infer the latent representation for",
      "Figure  shows the results of the accuracy experiment for the (i) popularity-based, (ii) similarity-based, and (iii) hybrid tag recommendation approaches.  Popularity-based approaches. In Figure  , we see that popularity-based approaches based on editor tags tend to perform better than if trained on Amazon search terms. If we take into account contextual information like BISAC or author, we can",
      "which we group into (i) popularity-based, (ii) similarity-based (i.e., using content information), and (iii) hybrid approaches. We evaluate our approaches in terms of accuracy, semantic similarity and diversity on the review content of Amazon users, which reflects the readers' vocabulary. With semantic similarity, we measure how semantically similar (based on learned Doc2Vec BIBREF7 embeddings)",
      "similarity to validate semantic matches of tag recommendations. Approach and findings. We exploit editor tags and user-generated search terms as input for tag recommendation approaches. Our evaluation comprises of a rich set of 19 different algorithms to recommend tags for e-books, which we group into (i) popularity-based, (ii) similarity-based (i.e., using content information), and (iii) hybrid"
    ]
  },
  {
    "title": "Harnessing Cognitive Features for Sarcasm Detection",
    "answer": "joshi2015harnessing, riloff2013sarcasm",
    "evidence": [
      "dataset through a stratified 10-fold cross validation. We also compare the classification accuracy of our system and the best available systems proposed by riloff2013sarcasm and joshi2015harnessing on our dataset. Using Weka BIBREF18 and LibSVM BIBREF19 APIs, we implement the following classifiers:",
      "How Effective are the Cognitive Features",
      "70% of the training data with our whole feature set, and the feature combination from joshi2015harnessing. The goodness of our system is demonstrated by improvements in F-score and Kappa statistics, shown in Figure  . We further analyze the importance of features by ranking the features based on (a) Chi squared test, and (b) Information Gain test, using Weka's attribute selection module. Figure",
      "by improvements in F-score and Kappa statistics, shown in Figure  . We further analyze the importance of features by ranking the features based on (a) Chi squared test, and (b) Information Gain test, using Weka's attribute selection module. Figure  shows the top 20 ranked features produced by both the tests. For both the cases, we observe 16 out of top 20 features to be gaze features. Further, in",
      "on (a) Chi squared test, and (b) Information Gain test, using Weka's attribute selection module. Figure  shows the top 20 ranked features produced by both the tests. For both the cases, we observe 16 out of top 20 features to be gaze features. Further, in each of the cases, Average Fixation Duration per Word and Largest Regression Position are seen to be the two most significant features.",
      "readability ease-score BIBREF15 ), number of words in a sentence and average character per word along with the sarcasm label as the predictors of average fixation duration following a linear mixed effect model BIBREF16 , sarcasm label turned out to be the most significant predictor with a maximum slope. This indicates that average fixation duration per word has a strong connection with the text",
      "Variation in the Average Fixation Duration per Word",
      "Example Cases",
      "Table  shows the classification results considering various feature combinations for different classifiers and other systems. These are: Unigram (with principal components of unigram feature vectors), Sarcasm (the feature-set reported by joshi2015harnessing subsuming unigram features and features from other reported systems) Gaze (the simple and complex cognitive features we introduce, along with",
      "we create a stratified (keeping the class ratio constant) random train-test split of 80%:20%. We train our classifier with 100%, 90%, 80% and 70% of the training data with our whole feature set, and the feature combination from joshi2015harnessing. The goodness of our system is demonstrated by improvements in F-score and Kappa statistics, shown in Figure  . We further analyze the importance of",
      "classifiers, our feature combination outperforms the baselines (considering only unigram features) as well as BIBREF3 , with the MILR classifier getting an F-score improvement of 3.7% and Kappa difference of 0.08. We also achieve an improvement of 2% over the baseline, using SVM classifier, when we employ our feature set. We also observe that the gaze features alone, also capture the differences",
      "of average fixation duration following a linear mixed effect model BIBREF16 , sarcasm label turned out to be the most significant predictor with a maximum slope. This indicates that average fixation duration per word has a strong connection with the text being sarcastic, at least in our dataset. We now analyze scanpaths to gain more insights into the sarcasm comprehension process.",
      "Related Work",
      "3.7%, over the best available system. Using cognitive features in an NLP Processing system like ours is the first proposal of its kind. Our general approach may be useful in other NLP sub-areas like sentiment and emotion analysis, text summarization and question answering, where considering textual clues alone does not prove to be sufficient. We propose to augment this work in future by exploring",
      "or occulomotor control problems forcing readers to spend time adjusting eye-muscles. So, an elevated average fixation duration per word may not sufficiently indicate the presence of sarcasm. But we would also like to share that, for our dataset, when we considered readability (Flesch readability ease-score BIBREF15 ), number of words in a sentence and average character per word along with the",
      "a problem with traditional NLP tools and techniques. This is apparent from the results reported by the survey from DBLP:journals/corr/JoshiBC16. The following discussion brings more insights into this. Consider a scenario where an online reviewer gives a negative opinion about a movie through sarcasm: “This is the kind of movie you see because the theater has air conditioning”. It is difficult",
      "fixation duration per word for sarcastic and non-sarcastic texts. The hypothesized mean difference is set to 0 and the error tolerance limit (  ) is set to 0.05. The t-test analysis, presented in Table  , shows that for all participants, a statistically significant difference exists between the average fixation duration per word for sarcasm (higher average fixation duration) and non-sarcasm",
      "getting an F-score improvement of 3.7% and Kappa difference of 0.08. We also achieve an improvement of 2% over the baseline, using SVM classifier, when we employ our feature set. We also observe that the gaze features alone, also capture the differences between sarcasm and non-sarcasm classes with a high-precision but a low recall. To see if the improvement obtained is statistically significant",
      "an SR-Research Eyelink-1000 eye-tracker (monocular remote mode, sampling rate 500Hz) records several eye-movement parameters like fixations (a long stay of gaze) and saccade (quick jumping of gaze between two positions of rest) and pupil size. The accuracy of polarity annotation varies between 72%-91% for sarcastic texts and 75%-91% for non-sarcastic text, showing the inherent difficulty of",
      "and features from other reported systems) Gaze (the simple and complex cognitive features we introduce, along with readability and word count features), and Gaze+Sarcasm (the complete set of features). For all regular classifiers, the gaze features are averaged across participants and augmented with linguistic and sarcasm related features. For the MILR classifier, the gaze features derived from",
      "for sarcasm detection. To check the effectiveness of our feature set, we observe the performance of multiple classification techniques on our dataset through a stratified 10-fold cross validation. We also compare the classification accuracy of our system and the best available systems proposed by riloff2013sarcasm and joshi2015harnessing on our dataset. Using Weka BIBREF18 and LibSVM BIBREF19",
      "the only cognitive feature, augmented with the textual features. The F-scores of all the classifiers turn out to be close to that of the classifiers considering sarcasm feature alone and the difference in the improvement is not statistically significant (  ). One the other hand, F-scores with gaze features are superior to the F-scores when reading time is considered as a cognitive feature.",
      "Simple Gaze Based Features"
    ]
  },
  {
    "title": "Arabic Offensive Language on Twitter: Analysis and Experiments",
    "answer": "it does not use a seed list of offensive words.",
    "evidence": [
      "dataset to determine which topics, dialects, and gender are most associated with offensive tweets and how Arabic speakers use offensive language. Lastly, we conduct a large battery of experiments to produce strong results (F1 = 79.7) on the dataset using Support Vector Machine techniques.",
      "Our target was to build a large Arabic offensive language dataset that is representative of their appearance on Twitter and is hopefully not biased to specific dialects, topics, or targets. One of the main challenges is that offensive tweets constitute a very small portion of overall tweets. To quantify their proportion, we took 3 random samples of tweets from different days, with each sample",
      "we focus on building effective Arabic offensive tweet detection. We introduce a method for building an offensive dataset that is not biased by topic, dialect, or target. We produce the largest Arabic dataset to date with special tags for vulgarity and hate speech. Next, we analyze the dataset to determine which topics, dialects, and gender are most associated with offensive tweets and how Arabic",
      "topic, dialect, or target. We produce the largest Arabic dataset to date with special tags for vulgarity and hate speech. Next, we analyze the dataset to determine which topics, dialects, and gender are most associated with offensive tweets and how Arabic speakers use offensive language. Lastly, we conduct a large battery of experiments to produce strong results (F1 = 79.7) on the dataset using",
      "or genre, and it appears in all Arabic dialects. Though the use of offensive language does not necessitate the appearance of the vocative particle, the particle does not favor any specific offensive expressions and greatly improves our chances of finding offensive tweets. It is clear, the dataset is more biased toward positive class. Using the dataset for real-life application may require",
      "To date, this is the largest available dataset, which we plan to make publicly available along with annotation guidelines. We use this dataset to characterize Arabic offensive language to ascertain the topics, dialects, and users' gender that are most associated with the use of offensive language. Though we suspect that there are common features that span different languages and cultures, some",
      "offensive language is language and culture specific. Thus, we conduct a thorough analysis of how Arabic users use offensive language. Next, we use the dataset to train strong Arabic offensive language classifiers using state-of-the-art representations and classification techniques. Specifically, we experiment with static and contextualized embeddings for representation along with a variety of",
      "We characterized the offensive tweets in the dataset to determine the topics that illicit such language, the dialects that are most often used, the common modes of offensiveness, and the gender distribution of their authors. We performed this breakdown for offensive tweets in general and for vulgar and hate speech tweets separately. We believe that this is the first detailed analysis of its kind.",
      "particle, the particle does not favor any specific offensive expressions and greatly improves our chances of finding offensive tweets. It is clear, the dataset is more biased toward positive class. Using the dataset for real-life application may require de-biasing it by boosting negative class or random sampling additional data from Twitter BIBREF22.Using the Twitter API, we collected 660k Arabic",
      "dataset for offensiveness, where offensive tweets account for roughly 19% of the tweets. Further, we labeled tweets as vulgar or hate speech. To date, this is the largest available dataset, which we plan to make publicly available along with annotation guidelines. We use this dataset to characterize Arabic offensive language to ascertain the topics, dialects, and users' gender that are most",
      "we use the dataset to train strong Arabic offensive language classifiers using state-of-the-art representations and classification techniques. Specifically, we experiment with static and contextualized embeddings for representation along with a variety of classifiers such as a deep neural network classifier and Support Vector Machine (SVM). The contributions of this paper are as follows: We built",
      "does not use a seed list of offensive words, it is not biased by topic, target, or dialect. Using our methodology, we tagged 10,000 Arabic tweet dataset for offensiveness, where offensive tweets account for roughly 19% of the tweets. Further, we labeled tweets as vulgar or hate speech. To date, this is the largest available dataset, which we plan to make publicly available along with annotation"
    ]
  },
  {
    "title": "A Bayesian Model of Multilingual Unsupervised Semantic Role Induction",
    "answer": "a Bayesian model \n\nor \nindividual models for each language plus additional latent variables that capture alignments between roles across languages.",
    "evidence": [
      "Multilingual Model",
      "Monolingual Model",
      ",  refers to the variables in all the training instances except the current one, and  refers to all the model parameters. The above integral has a closed form solution due to Dirichlet-multinomial conjugacy. For sampling roles in the multilingual model, we also need to consider the probabilities of roles being generated by the CLVs:    For sampling CLVs, we need to consider three factors: two",
      "For the monolingual model, the role at a given position is sampled as:     where the subscript  refers to all the variables except at position  ,  refers to the variables in all the training instances except the current one, and  refers to all the model parameters. The above integral has a closed form solution due to Dirichlet-multinomial conjugacy. For sampling roles in the multilingual model,",
      "(model parameters are integrated out). The sample counts and the priors are then used to calculate the MAP estimate of the model parameters. For the monolingual model, the role at a given position is sampled as:     where the subscript  refers to all the variables except at position  ,  refers to the variables in all the training instances except the current one, and  refers to all the model",
      "This formulation models the global role ordering and repetition preferences using PRs, and limited context for SRs using intervals. Ordering and repetition information was found to be helpful in supervised SRL as well BIBREF9 , BIBREF8 , BIBREF10 . More details, including the motivations behind this model, are in BIBREF3 .",
      "of all the identified arguments (the visible variables). We use a collapsed Gibbs-sampling based approach to generate samples for the hidden variables (model parameters are integrated out). The sample counts and the priors are then used to calculate the MAP estimate of the model parameters. For the monolingual model, the role at a given position is sampled as:     where the subscript  refers to",
      "We use the Bayesian model of garg2012unsupervised as our base monolingual model. The semantic roles are predicate-specific. To model the role ordering and repetition preferences, the role inventory for each predicate is divided into Primary and Secondary roles as follows:  For example, the complete role sequence in a frame could be:   ,  ,  ,  ,  ,  ,  ,   . The ordering is defined as the",
      "corpora for this task. Our joint Bayesian model consists of individual models for each language plus additional latent variables that capture alignments between roles across languages. Because it is a generative Bayesian model, we can do evaluations in a variety of scenarios just by varying the inference procedure, without changing the model, thereby comparing the scenarios directly. We compare",
      "model is deficient, since the aligned roles are being generated twice. Ideally, we would like to add the CLV as additional conditioning variables in the monolingual models. The new joint probability can be written as equation  (Figure  ), which can be further decomposed following the decomposition of the monolingual model in Figure  . However, having this additional conditioning variable breaks",
      "has a closed form solution due to Dirichlet-multinomial conjugacy. For sampling roles in the multilingual model, we also need to consider the probabilities of roles being generated by the CLVs:    For sampling CLVs, we need to consider three factors: two corresponding to probabilities of generating the aligned roles, and the third one corresponding to selecting the CLV according to CRP.     where"
    ]
  },
  {
    "title": "Hierarchical Transformers for Long Document Classification",
    "answer": "The transformer layer works better, as ToBERT, which uses another Transformer on top of BERT, is mentioned to have exploited pre-trained BERT features better compared to RoBERT, which uses a recurrent LSTM network.",
    "evidence": [
      "Because our work builds heavily upon BERT, we provide a brief summary of its features. BERT is built upon the Transformer architecture BIBREF0, which uses self-attention, feed-forward layers, residual connections and layer normalization as the main building blocks. It has two pre-training objectives: Masked language modelling - some of the words in a sentence are being masked and the model has to",
      "Method\nTransformer over BERT",
      "parameters such as number of transformer blocks, number of self-attention heads. Total number of parameters in BERT-Base are 110M and 340M in BERT-Large. BERT suffers from major limitations in terms of handling long sequences. Firstly, the self-attention layer has a quadratic complexity $O(n^2)$ in terms of the sequence length $n$ BIBREF0. Secondly, BERT uses a learned positional embeddings",
      "the best of our knowledge, no attempt has been done before to use the Transformer architecture for classification of such long sequences. Our novel contributions are: Two extensions - RoBERT and ToBERT - to the BERT model, which enable its application in classification of long texts by performing segmentation and using another layer on top of the segment representations. State-of-the-art results",
      "Bidirectional Encoder Representations from Transformers (BERT) is a novel Transformer BIBREF0 model, which recently achieved state-of-the-art performance in several language understanding tasks, such as question answering, natural language inference, semantic similarity, sentiment analysis, and others BIBREF1. While well-suited to dealing with relatively short sequences, Transformers suffer from",
      "segments in order to obtain a representation for each of them using BERT. Then, we use either a recurrent LSTM BIBREF10 network, or another Transformer, to perform the actual classification. We call these techniques Recurrence over BERT (RoBERT) and Transformer over BERT (ToBERT). Given that these models introduce a hierarchy of representations (segment-wise and document-wise), we refer to them",
      "sequence BIBREF0, we experiment with replacing the LSTM recurrent layer in favor of a small Transformer model (2 layers of transformer building block containing self-attention, fully connected, etc.). To investigate if preserving the information about the input sequence order is important, we also build a variant of ToBERT which learns positional embeddings at the segment-level representations",
      "BERT, which stands for Bidirectional Encoder Representations from Transformers, is a recently introduced language representation model based upon the transfer learning paradigm. We extend its fine-tuning procedure to address one of its major limitations - applicability to inputs longer than a few hundred words, such as transcripts of human call conversations. Our method is conceptually simple. We",
      "as BERT in this task. In this paper, we propose a method that builds upon BERT's architecture. We split the input text sequence into shorter segments in order to obtain a representation for each of them using BERT. Then, we use either a recurrent LSTM BIBREF10 network, or another Transformer, to perform the actual classification. We call these techniques Recurrence over BERT (RoBERT) and",
      "faster training and experimentation, however, our methods are applicable to BERT-Large as well. BERT-Base and BERT-Large are different in model parameters such as number of transformer blocks, number of self-attention heads. Total number of parameters in BERT-Base are 110M and 340M in BERT-Large. BERT suffers from major limitations in terms of handling long sequences. Firstly, the self-attention",
      "in BERT-Large. BERT suffers from major limitations in terms of handling long sequences. Firstly, the self-attention layer has a quadratic complexity $O(n^2)$ in terms of the sequence length $n$ BIBREF0. Secondly, BERT uses a learned positional embeddings scheme BIBREF1, which means that it won't likely be able to generalize to positions beyond those seen in the training data. To investigate the",
      "and feed each of them into the base model. Then, we propagate each output through a single recurrent layer, or another transformer, followed by a softmax activation. We obtain the final classification decision after the last segment has been consumed. We show that both BERT extensions are quick to fine-tune and converge after as little as 1 epoch of training on a small, domain-specific data set.",
      "Table  presents results using pre-trained BERT features. We extracted features from the pooled output of final transformer block as these were shown to be working well for most of the tasks BIBREF1. The features extracted from a pre-trained BERT model without any fine-tuning lead to a sub-par performance. However, We also notice that ToBERT model exploited the pre-trained BERT features better",
      "to perform the actual classification. We call these techniques Recurrence over BERT (RoBERT) and Transformer over BERT (ToBERT). Given that these models introduce a hierarchy of representations (segment-wise and document-wise), we refer to them as Hierarchical Transformers. To the best of our knowledge, no attempt has been done before to use the Transformer architecture for classification of such",
      "Given that BERT is limited to a particular input length, we split the input sequence into segments of a fixed size with overlap. For each of these segments, we obtain H or P from BERT model. We then stack these segment-level representations into a sequence, which serves as input to a small (100-dimensional) LSTM layer. Its output serves as a document embedding. Finally, we use two fully connected",
      "Given that Transformers' edge over recurrent networks is their ability to effectively capture long distance relationships between words in a sequence BIBREF0, we experiment with replacing the LSTM recurrent layer in favor of a small Transformer model (2 layers of transformer building block containing self-attention, fully connected, etc.). To investigate if preserving the information about the",
      "is trained to minimize cross-entropy loss with Adam optimizer BIBREF19. The initial learning rate is set to $0.001$ and is reduced by a factor of $0.95$ if validation loss does not decrease for 3-epochs. For ToBERT, the Transformer is trained with the default BERT version of Adam optimizer BIBREF1 with an initial learning rate of $5e$-5. We report accuracy in all of our experiments. We chose a",
      "segments, we obtain H or P from BERT model. We then stack these segment-level representations into a sequence, which serves as input to a small (100-dimensional) LSTM layer. Its output serves as a document embedding. Finally, we use two fully connected layers with ReLU (30-dimensional) and softmax (the same dimensionality as the number of classes) activations to obtain the final predictions. With",
      "we use either the pre-trained BERT weights, or the weights from a BERT fine-tuned on the task-specific dataset on a segment-level (i.e. we preserve the original label but fine-tune on each segment separately instead of on the whole text sequence). We compare these results to using the fine-tuned segment-level BERT predictions directly as inputs to the next layer.",
      "$O(n^2)$ in terms of the sequence length $n$ BIBREF0. Secondly, BERT uses a learned positional embeddings scheme BIBREF1, which means that it won't likely be able to generalize to positions beyond those seen in the training data. To investigate the effect of fine-tuning BERT on task performance, we use either the pre-trained BERT weights, or the weights from a BERT fine-tuned on the task-specific",
      "RoBERT on pre-trained BERT features and fine-tuned BERT features for all our tasks. Also, we noticed that fine-tuned BERT performs better than pre-trained BERT. We have shown that both RoBERT and ToBERT improved the simple baselines of taking an average (or the most frequent) of segment-wise predictions for long documents to obtain final prediction. Position embeddings did not significantly",
      "BERT maximum length 512. TransformerXL BIBREF2 is an extension to the Transformer architecture that allows it to better deal with long inputs for the language modelling task. It relies on the auto-regressive property of the model, which is not the case in our tasks.",
      "Method\nRecurrence over BERT",
      "models introduce a hierarchy of representations (segment-wise and document-wise), we refer to them as Hierarchical Transformers. To the best of our knowledge, no attempt has been done before to use the Transformer architecture for classification of such long sequences. Our novel contributions are: Two extensions - RoBERT and ToBERT - to the BERT model, which enable its application in",
      "and softmax (the same dimensionality as the number of classes) activations to obtain the final predictions. With this approach, we overcome BERT's computational complexity, reducing it to $O(n/k * k^2) = O(nk)$ for RoBERT, with $k$ denoting the segment size (the LSTM component has negligible linear complexity $O(k)$). The positional embeddings are also no longer an issue."
    ]
  },
  {
    "title": "Impact of Batch Size on Stopping Active Learning for Text Classification",
    "answer": "text classification",
    "evidence": [
      "Looking at Table~  , one can see that Oracle-99 needs more annotations with larger batch percents to reach approximately the same F-Measure as with smaller batch percents. These results are consistent with past findings that learning efficiency is decreased with larger batch sizes BIBREF2 , BIBREF7 . However, an open question is whether changing the parameters associated with actual stopping",
      "as with smaller batch percents. These results are consistent with past findings that learning efficiency is decreased with larger batch sizes BIBREF2 , BIBREF7 . However, an open question is whether changing the parameters associated with actual stopping methods can make them experience less degradation in performance when larger batch sizes are used. In particular, an important parameter of",
      "when to stop the iterative process of asking for more labeled data and how large of a batch size to use when asking for additional labels during each iteration. We found that stopping methods degrade in performance when larger batch sizes are used. The degradation in performance is larger than the amount that can be explained due to the degradation in learning efficiency that results from using",
      "Acknowledgment\nThis work was supported in part by The College of New Jersey Support of Scholarly Activities (SOSA) program, by The College of New Jersey Mentored Undergraduate Summer Experience (MUSE) program, and by usage of The College of New Jersey High Performance Computing System.",
      "Rather, it represents the first point in the active learning process at which BV2009 even has a chance to stop. When using larger batch percents, fewer models are generated than when using smaller batch percents. This gives any stopping method less points to test whether or not to stop. We also note that kappa agreement scores are generally low between the first few models trained. This, combined",
      "less degradation in performance when larger batch sizes are used. In particular, an important parameter of stopping methods is the window size of previous iterations to consider. The next subsection shows how decreasing the window size parameter can help to reduce the degradation in performance that stopping methods experience with larger batch sizes.",
      "more than three times. We use a stop word list to remove common English words. For analyzing the impact of batch size on stopping methods, we use a method that will stop at the first training iteration that is within a specified percentage of the maximum achievable performance. We denote this method as the Oracle Method, and we will set the percentage to 99 and denote this as Oracle-99. We set",
      "We find that large batch sizes degrade the performance of a leading stopping method over and above the degradation that results from reduced learning efficiency. We analyze this degradation and find that it can be mitigated by changing the window size parameter of how many past iterations of learning are taken into account when making the stopping decision. We find that when using larger batch",
      "fewer models are generated than when using smaller batch percents. This gives any stopping method less points to test whether or not to stop. We also note that kappa agreement scores are generally low between the first few models trained. This, combined with fewer points to stop at, causes BV2009 to stop somewhat sub-optimally when using very large batch percents. Usage of very large batch sizes,",
      "also note that kappa agreement scores are generally low between the first few models trained. This, combined with fewer points to stop at, causes BV2009 to stop somewhat sub-optimally when using very large batch percents. Usage of very large batch sizes, such as 10% of the data, is not common so sub-optimal performance of stopping methods in those situations is not a major problem.",
      "stopping methods BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . Another important aspect of the active learning process is what batch size to use. Previous work has shown that using smaller batch sizes leads to greater learning efficiency BIBREF2 , BIBREF7 . There is a tension between using smaller batch sizes to optimize learning efficiency and using larger batch sizes to optimize development",
      "BIBREF2 , BIBREF7 . However, an open question is whether changing the parameters associated with actual stopping methods can make them experience less degradation in performance when larger batch sizes are used. In particular, an important parameter of stopping methods is the window size of previous iterations to consider. The next subsection shows how decreasing the window size parameter can",
      "each iteration. We found that stopping methods degrade in performance when larger batch sizes are used. The degradation in performance is larger than the amount that can be explained due to the degradation in learning efficiency that results from using larger batch sizes. An important parameter used by stopping methods is what window size of earlier iterations to consider in making the stopping",
      "learning efficiency. We analyze this degradation and find that it can be mitigated by changing the window size parameter of how many past iterations of learning are taken into account when making the stopping decision. We find that when using larger batch sizes, stopping methods are more effective when smaller window sizes are used.",
      "using smaller batch sizes to optimize learning efficiency and using larger batch sizes to optimize development speed and ease of annotation. We analyze how batch size affects a leading stopping method and how stopping method parameters can be changed to optimize performance depending on the batch size. We evaluate the effect batch size has on active learning stopping methods for text",
      "decrease learning efficiency from a learning curve perspective, it remains an open question how batch size impacts methods for stopping active learning. We find that large batch sizes degrade the performance of a leading stopping method over and above the degradation that results from reduced learning efficiency. We analyze this degradation and find that it can be mitigated by changing the window",
      "been labeled, defeating the purpose of using active learning. Accordingly, there has been a lot of interest in the development of active learning stopping methods BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . Another important aspect of the active learning process is what batch size to use. Previous work has shown that using smaller batch sizes leads to greater learning efficiency BIBREF2 ,",
      "Oracle Results",
      "use. Previous work has shown that using smaller batch sizes leads to greater learning efficiency BIBREF2 , BIBREF7 . There is a tension between using smaller batch sizes to optimize learning efficiency and using larger batch sizes to optimize development speed and ease of annotation. We analyze how batch size affects a leading stopping method and how stopping method parameters can be changed to",
      "We denote the stopping method published in BIBREF4 as BV2009. This stopping method will stop the active learning process if the mean of the three previous kappa agreement values between consecutive models is above a threshold. For larger batch percents, note that BV2009 stops later than the optimal Oracle Method point. We ran BV2009 with smaller window sizes for each of our different batch sizes.",
      "than the amount that can be explained due to the degradation in learning efficiency that results from using larger batch sizes. An important parameter used by stopping methods is what window size of earlier iterations to consider in making the stopping decision. Our results indicate that making the window size smaller helps to mitigate the degradation in stopping method performance that occurs",
      "to speed and human annotator considerations, the use of larger batch sizes is necessary. While past work has shown that larger batch sizes decrease learning efficiency from a learning curve perspective, it remains an open question how batch size impacts methods for stopping active learning. We find that large batch sizes degrade the performance of a leading stopping method over and above the",
      "parameter used by stopping methods is what window size of earlier iterations to consider in making the stopping decision. Our results indicate that making the window size smaller helps to mitigate the degradation in stopping method performance that occurs with larger batch sizes.",
      "previous kappa agreement values between consecutive models is above a threshold. For larger batch percents, note that BV2009 stops later than the optimal Oracle Method point. We ran BV2009 with smaller window sizes for each of our different batch sizes. Our results are summarized for a window size of one in the row ``BV2009 (Window Size = 1)'' in Table~  . When using a window size of one, BV2009",
      "with other sampling algorithms BIBREF8 . We use a binary bag of words representation and only consider words that show up in the dataset more than three times. We use a stop word list to remove common English words. For analyzing the impact of batch size on stopping methods, we use a method that will stop at the first training iteration that is within a specified percentage of the maximum"
    ]
  },
  {
    "title": "Generating Personalized Recipes from Historical User Preferences",
    "answer": "cooking websites",
    "evidence": [
      "Recipe Dataset: Food.com",
      "In the kitchen, we increasingly rely on instructions from cooking websites: recipes. A cook with a predilection for Asian cuisine may wish to prepare chicken curry, but may not know all necessary ingredients apart from a few basics. These users with limited knowledge cannot rely on existing recipe generation approaches that focus on creating coherent recipes given all ingredients and a recipe"
    ]
  },
  {
    "title": "Pyramidal Recurrent Unit for Language Modeling",
    "answer": "WT-2 dataset.",
    "evidence": [
      "Acknowledgments\nThis research was supported by NSF (IIS 1616112, III 1703166), Allen Distinguished Investigator Award, and gifts from Allen Institute for AI, Google, Amazon, and Bloomberg. We are grateful to Aaron Jaech, Hannah Rashkin, Mandar Joshi, Aniruddha Kembhavi, and anonymous reviewers for their helpful comments.",
      "is similar across different configurations. We use 1000, 1200, and 1400 as hidden layer sizes for values of  =1,2, and 4, respectively. We use the same settings for the WT-2 dataset. We set the number of pyramidal levels  to two in our experiments and use average pooling for sub-sampling. These values are selected based on our ablation experiments on the validation set (Section  ). We measure the"
    ]
  },
  {
    "title": "Emotion Detection in Text: Focusing on Latent Representation",
    "answer": "sequential nature of the text and the context",
    "evidence": [
      "we present a new network based on a bidirectional GRU model to show that capturing more meaningful information from text can significantly improve the performance of these models. The results show significant improvement with an average of 26.8 point increase in F-measure on our test data and 38.6 increase on the totally new dataset.",
      "machine learning models cannot grasp the intricacy of emotional language by ignoring the sequential nature of the text, and the context. These methods, therefore, are not sufficient to create an applicable and generalizable emotion detection methodology. Understanding these limitations, we present a new network based on a bidirectional GRU model to show that capturing more meaningful information",
      "methods, therefore, are not sufficient to create an applicable and generalizable emotion detection methodology. Understanding these limitations, we present a new network based on a bidirectional GRU model to show that capturing more meaningful information from text can significantly improve the performance of these models. The results show significant improvement with an average of 26.8 point",
      "features form the GRU output and an average-pooling layer was used to considers all features to create a representation for the text as a whole. These partial representations are then were concatenated to create out final hidden representation. For classification, the output of the concatenation is passed to a dense classification layer with 70 nodes along with a dropout layer with a rate of 50%",
      "to create an intermediate representation for the tweets that capture the sequential nature of the data. For the next step, we use a concatenation of global max-pooling and average-pooling layers (with a window size of two). Then a max-pooling was used to extract the most important features form the GRU output and an average-pooling layer was used to considers all features to create a"
    ]
  },
  {
    "title": "ReviewQA: a relational aspect-based opinion reading dataset",
    "answer": "TripAdvisor website.",
    "evidence": [
      "We used a set of reviews extracted from the TripAdvisor website and originally proposed in BIBREF10 and BIBREF11 . This corpus is available at http://www.cs.virginia.edu/~hw5x/Data/LARA/TripAdvisor/TripAdvisorJson.tar.bz2. Each review comes with the name of the associated hotel, a title, an overall rating, a comment and a list of rated aspects. From 0 to 7 aspects, among value, room, location,"
    ]
  },
  {
    "title": "Fine-Grained Named Entity Recognition using ELMo and Wikidata",
    "answer": "They combine state-of-the-art deep learning models (ELMo) with an expansive knowledge base (Wikidata).",
    "evidence": [
      "integrate symbolic information into the reasoning process of a neural network with a type system and show state-of-the-art performances for EL. They do not, however, quote results on Wiki(gold) so a direct comparison is difficult. While these knowledge bases provide semantically rich and fine-granular classes and relationship types, the task of entity classification often requires associating",
      "entity types. Our work attempts to address these issues, in part, by combining state-of-the-art deep learning models (ELMo) with an expansive knowledge base (Wikidata). Using our framework, we cross-validate our model on the 112 fine-grained entity types based on the hierarchy given from the Wiki(gold) dataset.",
      "in a piece of text with reference to a knowledge base. There are a number of knowledge bases that provide a background repository for entity classification of this type. For this study, we use Wikidata, which can be seen diagrammatically in Figure  . Systems such as DeepType BIBREF25 integrate symbolic information into the reasoning process of a neural network with a type system and show",
      "whilst covering a large spectrum of entity types. Furthermore, many named entity systems suffer when considering the categorization of fine grained entity types. Our work attempts to address these issues, in part, by combining state-of-the-art deep learning models (ELMo) with an expansive knowledge base (Wikidata). Using our framework, we cross-validate our model on the 112 fine-grained entity"
    ]
  },
  {
    "title": "Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps",
    "answer": "Unfortunately, I don't see a provided text or context that directly defines how the authors of the given text (not provided) define a concept map. However, based on general knowledge, I can tell you that typically, in educational and cognitive research contexts, a concept map is often defined as:\n\n\"A visual representation of relationships among concepts.\"\n\nBut since I don't have the specific text you're referring to, let's assume a general definition often cited in literature:\n\n***a diagram showing relationships between concepts*** \n\nIf you provide the actual context or text from which the question originates, I can give a more precise definition based on the authors' own words.",
    "evidence": [
      "Concept Maps"
    ]
  },
  {
    "title": "CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus",
    "answer": "The quality of the data is empirically evaluated using **quality checks** and **BLEU scores**, specifically **sentence BLEU scores** and **$\\textrm {BLEU}_{MS}$**, a normalized quality score.",
    "evidence": [
      "Baseline Results\nExperimental Settings\nData Preprocessing",
      "Data Collection and Processing\nTatoeba (TT)",
      "Baseline Results\nMulti-Speaker Evaluation",
      "Baseline Results\nExperimental Settings\nModel Training",
      "evaluation set closer to real-world scenarios and more challenging. We run the same quality checks for TT as for CoVoST but we do not find poor quality translations according to our criteria. Finally, we report the overlap between CoVo transcripts and TT sentences in Table . We found a minimal overlap, which makes the TT evaluation set a suitable additional test set when training on CoVoST.",
      "samples by sentence lengths (minimum 4 words including punctuations) to reduce the portion of short sentences. This makes the resulting evaluation set closer to real-world scenarios and more challenging. We run the same quality checks for TT as for CoVoST but we do not find poor quality translations according to our criteria. Finally, we report the overlap between CoVo transcripts and TT",
      "as follows: and where $G$ is the set of sentence BLEU scores grouped by transcript and $G^{\\prime } = \\lbrace g | g\\in G, |g|>1, \\textrm {Mean}(g) > 0 \\rbrace $. $\\textrm {BLEU}_{MS}$ provides a normalized quality score as oppose to corpus-level BLEU or unnormalized average of sentence BLEU. And $\\textrm {CoefVar}_{MS}$ is a standardized measure of model stability against different speakers (the"
    ]
  },
  {
    "title": "Improving the Performance of Neural Machine Translation Involving Morphologically Rich Languages",
    "answer": "A group of 50 native people, well-versed in both English and Tamil languages, acted as annotators, evaluating a collection of about 100 sentences taken from the test set results.",
    "evidence": [
      "Human Evaluation",
      "precision and ranking values (between RNNSearch and RNNMorph outputs) were estimated in Table  . A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison. This set included a randomized selection of the translation results to ensure"
    ]
  },
  {
    "title": "Multi-task Pairwise Neural Ranking for Hashtag Segmentation",
    "answer": "The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset, specifically from 1,268 randomly selected tweets, and includes crowdsourced segmentations and additional corrections. \n\nAlternatively, a more concise answer:\nFrom the Stanford Sentiment Analysis Dataset (1,268 randomly selected tweets).",
    "evidence": [
      "Hashtag Segmentation Data",
      "hashtags by using a language model specifically trained on Twitter data (implementation details in §  ). The performance of this method itself is competitive with state-of-the-art methods (evaluation results in §  ). Our proposed pairwise ranking method will effectively take the top  segmentations generated by this baseline as candidates for reranking. However, in prior work, the ranking scores",
      "We compare our pairwise neural ranker with the following baseline and state-of-the-art approaches: The original hashtag as a single token; A rule-based segmenter, which employs a set of word-shape rules with an English dictionary BIBREF13 ; A Viterbi model which uses word frequencies from a book corpus BIBREF0 ; The specially developed GATE Hashtag Tokenizer from the open source toolkit, which",
      "aiding search, or providing additional semantics. However, the semantic content of hashtags is not straightforward to infer as these represent ad-hoc conventions which frequently include multiple words joined together and can include abbreviations and unorthodox spellings. We build a dataset of 12,594 hashtags split into individual segments and propose a set of approaches for hashtag segmentation",
      "We use two datasets for experiments (Table  ): (a) STAN  , created by BIBREF10 BansalBV15, which consists of 1,108 unique English hashtags from 1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36 along with their crowdsourced segmentations and our additional corrections; and (b) STAN  , our new expert curated dataset, which includes all 12,594 unique English",
      "We also constructed a larger and more curated dataset for analyzing and benchmarking hashtag segmentation methods. We demonstrated that hashtag segmentation helps with downstream tasks such as sentiment analysis. Although we focused on English hashtags, our pairwise ranking approach is language-independent and we intend to extend our toolkit to languages other than English as future work.",
      "is a named-entity present in the list of Wikipedia titles, and if the candidate segmentation  and its corresponding hashtag  satisfy certain word-shapes (more details in appendix  ). Similarly, for hashtag  , we extract the feature vector  consisting of hashtag length, ngram count of the hashtag in Google 1TB corpus BIBREF31 , and boolean features indicating if the hashtag is in an English",
      "splitting a hashtag into a meaningful word sequence. Our contributions are: Our new dataset includes segmentation for 12,594 unique hashtags and their associated tweets annotated in a multi-step process for higher quality than the previous dataset of 1,108 hashtags BIBREF10 . We frame the segmentation task as a pairwise ranking problem, given a set of candidate segmentations. We build several",
      "from 1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36 along with their crowdsourced segmentations and our additional corrections; and (b) STAN  , our new expert curated dataset, which includes all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset."
    ]
  },
  {
    "title": "Extractive Summarization of EHR Discharge Notes",
    "answer": "MIMIC.",
    "evidence": [
      "The model ran for 20 epochs and was halted early if there was no improvement after 3 epochs. We evaluated the impact of character embeddings, the choice of pretrained w2v embeddings, and the addition of learned word embeddings on model performance on the dev set. We report performance of the best performing model on the test set. Table  compares dev set performance of the model using various",
      "of the best performing model on the test set. Table  compares dev set performance of the model using various pretrained word embeddings, with and without character embeddings, and with pretrained versus learned word embeddings. The first row in each section is the performance of the model architecture described in the methods section for comparison. Models using word embeddings trained on the",
      "the choice of pretrained w2v embeddings, and the addition of learned word embeddings on model performance on the dev set. We report performance of the best performing model on the test set. Table  compares dev set performance of the model using various pretrained word embeddings, with and without character embeddings, and with pretrained versus learned word embeddings. The first row in each",
      "classifies words in history of present illness notes. We demonstrate that our model can achieve excellent performance on a small dataset with known heterogeneity among annotators. This model can be applied to the 55,000 discharge summaries in MIMIC to create a dataset for evaluation of extractive summarization methods.",
      "were split in a 70% train set, 15% development set, and a 15% test set. The model is trained using the Adam algorithm for gradient-based optimization BIBREF25 with an initial learning rate = 0.001 and decay = 0.9. A dropout rate of 0.5 was applied for regularization, and each batch size = 20. The model ran for 20 epochs and was halted early if there was no improvement after 3 epochs. We evaluated",
      "and without character embeddings, and with pretrained versus learned word embeddings. The first row in each section is the performance of the model architecture described in the methods section for comparison. Models using word embeddings trained on the discharge summaries performed better than word embeddings trained on all MIMIC notes, likely because the discharge summary word embeddings better",
      "extractive summarization of discharge notes and develop an LSTM model to sequentially label topics of history of present illness notes. We achieve an F1 score of 0.876, which indicates that this model can be employed to create a dataset for evaluation of extractive summarization methods.",
      "labels and the probability  is calculated according to the CRF. We evaluated our model on the 515 annotated history of present illness notes, which were split in a 70% train set, 15% development set, and a 15% test set. The model is trained using the Adam algorithm for gradient-based optimization BIBREF25 with an initial learning rate = 0.001 and decay = 0.9. A dropout rate of 0.5 was applied for"
    ]
  },
  {
    "title": "ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading and Annotation",
    "answer": "Normal and task-specific sentences. \n\nor alternatively, \n***Normal reading and task-specific reading sentences***.",
    "evidence": [
      "to set up and calibrate the devices, and the personal reading speed of the participants. We recorded 14 blocks of approx. 50 sentences, alternating between tasks: 50 sentences of normal reading, followed by 50 sentences of task-specific reading. The order of blocks and sentences within blocks was identical for all subjects. Each sentence block was preceded by a practice round of three sentences."
    ]
  },
  {
    "title": "Data Collection for Interactive Learning through the Dialog",
    "answer": "***The context does not provide information on the data collection process.*** \n\nHowever, based on typical academic standards, I can suggest that a possible answer could be found if more information was provided about the dataset. If I had to give a generic response based on common practices:\n\nThe data was likely collected through a systematic process involving [specific methods or tools], but without further context, I cannot provide a precise answer. \n\nGiven that I must follow the format and provide a concise phrase based on provided context (which in this case does not exist), my response will be:\nNo information provided.",
    "evidence": [
      "Dataset Collection Process"
    ]
  },
  {
    "title": "Distilling Translations with Visual Awareness",
    "answer": "English-German dataset (Multi30K)",
    "evidence": [
      "Results\nIn this section we present results of our experiments, first in the original dataset without any source degradation (Section  ) and then in the setup with various source degradation strategies (Section  ).",
      "In addition to using the Multi30K dataset as is (standard setup), we probe the ability of our models to address the three linguistic phenomena where additional context has been proved important (Section ): ambiguities, gender-neutral words and noisy input. In a controlled experiment where we aim to remove the influence of frequency biases, we degrade the source sentences by masking words through",
      "transformer networks (constrained models) on the English-German dataset, as compared to BIBREF30 . Second, our deliberation models lead to significant improvements over this baseline across test sets (average  ,  ). Transformer-based models enriched with image information (base+sum, base+att and base+obj), on the other hand, show no major improvements with respect to the base performance. This is",
      "we aim to remove the influence of frequency biases, we degrade the source sentences by masking words through three strategies to replace words by a placeholder: random source words, ambiguous source words and gender unmarked source words. The procedure is applied to the train, validation and test sets. For the resulting dataset generated for each setting, we compare models having access to",
      "as in the WMT tasks BIBREF25 . We compare our transformer baseline to transformer models enriched with image information, as well as to the deliberation models, with or without image information. We first note that our multimodal models achieve the state of the art performance for transformer networks (constrained models) on the English-German dataset, as compared to BIBREF30 . Second, our"
    ]
  },
  {
    "title": "English-Japanese Neural Machine Translation with Encoder-Decoder-Reconstructor",
    "answer": "Two parallel corpora: ASPEC (Asian Scientific Paper Excerpt Corpus) and NTCIR PatentMT Parallel Corpus.",
    "evidence": [
      "We used two parallel corpora: Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 and NTCIR PatentMT Parallel Corpus BIBREF1 . Regarding the training data of ASPEC, we used only the first 1 million sentences sorted by sentence-alignment similarity. Japanese sentences were segmented by the morphological analyzer MeCab (version 0.996, IPADIC), and English sentences were tokenized by"
    ]
  },
  {
    "title": "Stereotyping and Bias in the Flickr30K Dataset",
    "answer": "***biases and unwarranted inferences*** \n\n(or, more specifically, the document lists \"a list of biases and unwarranted inferences\", but does not provide a further specification within the given context)",
    "evidence": [
      "this assumption, and provides a list of biases and unwarranted inferences that can be found in the Flickr30K dataset. Finally, it considers methods to find examples of these, and discusses how we should deal with stereotype-driven descriptions in future applications."
    ]
  },
  {
    "title": "An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction",
    "answer": "23,700 queries.",
    "evidence": [
      "Dataset\nDataset Variants",
      "Dataset\nData Preprocessing and Partitioning",
      "Dataset\nIn-Scope Data Collection",
      "Dataset\nWe introduce a new crowdsourced dataset of 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains. The dataset also includes 1,200 out-of-scope queries. Table  shows examples of the data."
    ]
  },
  {
    "title": "Automatic Judgment Prediction via Legal Reading Comprehension",
    "answer": "***Datasets used in the experiment are not explicitly mentioned in the provided context***. \n\nHowever, based on typical academic contexts related to dataset construction for evaluation, a more informed response related to common datasets used in experiments across various fields could be:\n\n***SQuAD, CoQA, and other task-specific datasets***.\n\nBut given the context only mentions \"Dataset Construction for Evaluation\" without specifics:\n\n***The context does not provide specific dataset names***.",
    "evidence": [
      "Dataset Construction for Evaluation"
    ]
  },
  {
    "title": "Improved Neural Relation Detection for Knowledge Base Question Answering",
    "answer": "relation detection model",
    "evidence": [
      "KBQA End-Task Results",
      "KBQA Enhanced by Relation Detection",
      "a key role in the KBQA process: (1) Re-ranking the entity candidates according to whether they connect to high confident relations detected from the raw question text by the relation detection model. This step is important to deal with the ambiguities normally present in entity linking results. (2) Finding the core relation (chains) for each topic entity selection from a much smaller candidate",
      "This section describes our KBQA pipeline system. We make minimal efforts beyond the training of the relation detection model, making the whole system easy to build. Following previous work BIBREF4 , BIBREF5 , our KBQA system takes an existing entity linker to produce the top- $K$ linked entities, $EL_K(q)$ , for a question $q$ (“initial entity linking”). Then we generate the KB queries for $q$",
      "detection could benefit the KBQA end task, we also propose a simple KBQA implementation composed of two-step relation detection. Given an input question and a set of candidate entities retrieved by an entity linker based on the question, our proposed relation detection model plays a key role in the KBQA process: (1) Re-ranking the entity candidates according to whether they connect to high",
      "KBQA system that integrates entity linking and our proposed relation detector to enable one enhance another. Experimental results evidence that our approach achieves not only outstanding relation detection performance, but more importantly, it helps our KBQA system to achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.",
      "system easy to build. Following previous work BIBREF4 , BIBREF5 , our KBQA system takes an existing entity linker to produce the top- $K$ linked entities, $EL_K(q)$ , for a question $q$ (“initial entity linking”). Then we generate the KB queries for $q$ following the four steps illustrated in Algorithm \"KBQA Enhanced by Relation Detection\" . [htbp] InputInput OutputOutput Top query tuple",
      "need to be handled for multiple entities in the question. The KBQA system in the figure performs two key tasks: (1) entity linking, which links $n$ -grams in questions to KB entities, and (2) relation detection, which identifies the KB relation(s) a question refers to. The main focus of this work is to improve the relation detection subtask and further explore how it can contribute to the KBQA",
      "It is a single-relation KBQA task. The KB we use consists of a Freebase subset with 2M entities (FB2M) BIBREF2 , in order to compare with previous research. yin2016simple also evaluated their relation extractor on this data set and released their proposed question-relation pairs, so we run our relation detection model on their data set. For the KBQA evaluation, we also start with their entity",
      "our relation detection model on their data set. For the KBQA evaluation, we also start with their entity linking results. Therefore, our results can be compared with their reported results on both tasks. WebQSP (WQ): A multi-relation KBQA task. We use the entire Freebase KB for evaluation purposes. Following yih-EtAl:2016:P16-2, we use S-MART BIBREF26 entity-linking outputs. In order to evaluate",
      "another baseline that uses our KBQA system but replaces HR-BiLSTM with our implementation of AMPCNN (for SimpleQuestions) or the char-3-gram BiCNN (for WebQSP) relation detectors (second block in Table 3 ). Compared to the baseline relation detector (3rd row of results), our method, which includes an improved relation detector (HR-BiLSTM), improves the KBQA end task by 2-3% (4th row). Note that",
      "with a single $<$ head-entity, relation, tail-entity $>$ KB tuple BIBREF6 , BIBREF7 , BIBREF2 ; and (b) a more complex case, where some constraints need to be handled for multiple entities in the question. The KBQA system in the figure performs two key tasks: (1) entity linking, which links $n$ -grams in questions to KB entities, and (2) relation detection, which identifies the KB relation(s) a",
      "KB relation detection is a key step in KBQA and is significantly different from general relation extraction tasks. We propose a novel KB relation detection model, HR-BiLSTM, that performs hierarchical matching between questions and KB relations. Our model outperforms the previous methods on KB relation detection tasks and allows our KBQA system to achieve state-of-the-arts. For future work, we",
      "input question and a set of candidate entities retrieved by an entity linker based on the question, our proposed relation detection model plays a key role in the KBQA process: (1) Re-ranking the entity candidates according to whether they connect to high confident relations detected from the raw question text by the relation detection model. This step is important to deal with the ambiguities",
      "in more abstract (deeper) question representations, thus improves hierarchical matching. In order to assess how the proposed improved relation detection could benefit the KBQA end task, we also propose a simple KBQA implementation composed of two-step relation detection. Given an input question and a set of candidate entities retrieved by an entity linker based on the question, our proposed",
      "Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 . For an input question, these systems typically generate a KB query, which can be executed to retrieve the answers from a KB. Figure 1 illustrates the process used to parse two sample questions in a KBQA system: (a) a single-relation",
      "research. yin2016simple also evaluated their relation extractor on this data set and released their proposed question-relation pairs, so we run our relation detection model on their data set. For the KBQA evaluation, we also start with their entity linking results. Therefore, our results can be compared with their reported results on both tasks. WebQSP (WQ): A multi-relation KBQA task. We use the",
      "KB. Figure 1 illustrates the process used to parse two sample questions in a KBQA system: (a) a single-relation question, which can be answered with a single $<$ head-entity, relation, tail-entity $>$ KB tuple BIBREF6 , BIBREF7 , BIBREF2 ; and (b) a more complex case, where some constraints need to be handled for multiple entities in the question. The KBQA system in the figure performs two key",
      "when applied to the other dataset. In order to highlight the effect of different relation detection models on the KBQA end-task, we also implemented another baseline that uses our KBQA system but replaces HR-BiLSTM with our implementation of AMPCNN (for SimpleQuestions) or the char-3-gram BiCNN (for WebQSP) relation detectors (second block in Table 3 ). Compared to the baseline relation detector",
      "The third block of the table details two ablation tests for the proposed components in our KBQA systems: (1) Removing the entity re-ranking step significantly decreases the scores. Since the re-ranking step relies on the relation detection models, this shows that our HR-BiLSTM model contributes to the good performance in multiple ways. Appendix C gives the detailed performance of the re-ranking",
      "we can directly evaluate relation detection performance independently as well as evaluate on the KBQA end task. SimpleQuestions (SQ): It is a single-relation KBQA task. The KB we use consists of a Freebase subset with 2M entities (FB2M) BIBREF2 , in order to compare with previous research. yin2016simple also evaluated their relation extractor on this data set and released their proposed",
      "KB relation detection tasks and allows our KBQA system to achieve state-of-the-arts. For future work, we will investigate the integration of our HR-BiLSTM into end-to-end systems. For example, our model could be integrated into the decoder in BIBREF31 , to provide better sequence prediction. We will also investigate new emerging datasets like GraphQuestions BIBREF32 and ComplexQuestions BIBREF30",
      "includes an improved relation detector (HR-BiLSTM), improves the KBQA end task by 2-3% (4th row). Note that in contrast to previous KBQA systems, our system does not use joint-inference or feature-based re-ranking step, nevertheless it still achieves better or comparable results to the state-of-the-art. The third block of the table details two ablation tests for the proposed components in our",
      "on SimpleQuestions. Since these two baselines are specially designed/tuned for one particular dataset, they do not generalize well when applied to the other dataset. In order to highlight the effect of different relation detection models on the KBQA end-task, we also implemented another baseline that uses our KBQA system but replaces HR-BiLSTM with our implementation of AMPCNN (for",
      "(for WebQSP) relation detectors (second block in Table 3 ). Compared to the baseline relation detector (3rd row of results), our method, which includes an improved relation detector (HR-BiLSTM), improves the KBQA end task by 2-3% (4th row). Note that in contrast to previous KBQA systems, our system does not use joint-inference or feature-based re-ranking step, nevertheless it still achieves"
    ]
  },
  {
    "title": "Knowledge Graph Representation with Jointly Structural and Textual Encoding",
    "answer": "WN18 and FB15K.",
    "evidence": [
      "“Filter”. The evaluation results are reported under these two settings. We select the margin $\\gamma $ among $\\lbrace 1, 2\\rbrace $ , the embedding dimension $d$ among $\\lbrace 20, 50, 100\\rbrace $ , the regularization $\\eta $ among $\\lbrace 0, 1E{-5}, 1E{-6}\\rbrace $ , two learning rates $\\lambda _s$ and $\\lambda _t$ among $\\lbrace 0.001, 0.01, 0.05\\rbrace $ to learn the parameters of structure",
      "triplets included in training, validation and test sets before ranking (except the test triplet of interest). We call this evaluation setting “Filter”. The evaluation results are reported under these two settings. We select the margin $\\gamma $ among $\\lbrace 1, 2\\rbrace $ , the embedding dimension $d$ among $\\lbrace 20, 50, 100\\rbrace $ , the regularization $\\eta $ among $\\lbrace 0, 1E{-5},",
      "on the valid set. For a given triplet $(h, r, t)$ , if its score is larger than $\\delta _r$ , it will be classified as positive, otherwise negative. Table 4 shows the evaluation results of triplets classification. The results reveal that our joint encoding models is effective and also outperform the baseline method. On WN18, “Jointly(A-LSTM)” achieves the best performance, and the “Jointly(LSTM)”",
      "Table 4 shows the evaluation results of triplets classification. The results reveal that our joint encoding models is effective and also outperform the baseline method. On WN18, “Jointly(A-LSTM)” achieves the best performance, and the “Jointly(LSTM)” is slightly worse than “Jointly(A-LSTM)”. The reason is that the number of relations is relatively small. Therefore, the attention mechanism does",
      "and obtained substantial improvements. While releasing the current paper we discovered a paper by BIBREF30 proposing a similar model with attention mechanism which is evaluated on link prediction and triplet classification. However, our work encodes text description as a whole without explicit segmentation of sentences, which breaks the order and coherence among sentences.",
      "on WN18; $\\gamma =2$ , $d=100$ , $\\eta =1E{-5}$ , $\\lambda _s = 0.01$ , $d=20$0 , and $d=20$1 distance on FB15K. Experimental results on both WN18 and FB15k are shown in Table 2 , where we use “Jointly(CBOW)”, “Jointly(LSTM)” and “Jointly(A-LSTM)” to represent our jointly encoding models with CBOW, LSTM and attentive LSTM text encoders. Our baseline is TransE since that the score function of our"
    ]
  },
  {
    "title": "State-of-the-Art Vietnamese Word Segmentation",
    "answer": "Neural Network, Weighted Finite State Transducer (WFST), machine learning approaches, Conditional Random Fields, and Support Vector Machine.",
    "evidence": [
      "There are several studies for Vietnamese Word Segmentation during last decade. For instance, Dinh et al. started the word segmentation task for Vietnamese with Neural Network and Weighted Finite State Transducer (WFST) BIBREF9 . Nguyen et al. continued with machine learning approaches, Conditional Random Fields and Support Vector Machine BIBREF7 . Most of statistical approaches are based on the"
    ]
  },
  {
    "title": "A Multi-Task Architecture on Relevance-based Neural Query Translation",
    "answer": "The baseline is the **transformer** (BIBREF8).",
    "evidence": [
      "balance values for transformer and our model for a random sample of 20 queries from the validation set of Italian queries, respectively. Figure  shows the balance values for transformer and our model for a random sample of 20 queries from the test set of Italian queries, respectively. It is evident that our model achieves better balance compared to baseline transformer, except for a very few",
      "Table  shows the effectiveness of our model (multi-task transformer) over the baseline transformer BIBREF8 . Our model achieves significant performance gains in the test sets over the baseline for both Italian and Finnish query translation. The overall low MAP for NMT can possibly be improved with larger TC. Moreover, our model validation approach requires access to RC index, and it slows down",
      "It is evident that our model achieves better balance compared to baseline transformer, except for a very few cases. Given a query  , consider  as the set of terms from human translation of  and  as the set of translation terms generated by model  . We define  and  as precision and recall of  for model  . In Table  , we report average precision and recall for both transformer and our model across",
      "Results and Analysis"
    ]
  },
  {
    "title": "Unsupervised Learning of Syntactic Structure with Invertible Neural Projections",
    "answer": "a unit Jacobian determinant.",
    "evidence": [
      "Learning with Invertibility",
      "In this section, we introduce an invertibility condition for our neural projector to tackle the optimization challenge. Specifically, we constrain our neural projector with two requirements: (1)  and (2)  exists. Invertible transformations have been explored before in independent components analysis BIBREF14 , gaussianization BIBREF15 , and deep density models BIBREF16 , BIBREF17 , BIBREF18 , for",
      "specially designed invertible networks that are easily trainable yet still powerful BIBREF16 , BIBREF17 , BIBREF19 . Inspired by these works, we use the invertible transformation proposed by BIBREF16 , which consists of a series of “coupling layers”. This architecture is specially designed to guarantee a unit Jacobian determinant (and thus the invertibility property). From Eq. ( EQREF22 ) we know",
      "use the invertible transformation proposed by BIBREF16 , which consists of a series of “coupling layers”. This architecture is specially designed to guarantee a unit Jacobian determinant (and thus the invertibility property). From Eq. ( EQREF22 ) we know that only  is required for accomplishing learning and inference; we never need to explicitly construct  . Thus, we directly define the",
      "learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments we instantiate our approach with both",
      "with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS",
      "Invertible Volume-Preserving Neural Net",
      "can be any nonlinear form. This transformation satisfies  , and BIBREF16 show that its Jacobian matrix is triangular with all ones on the main diagonal. Thus the Jacobian determinant is always equal to one (i.e. volume-preserving) and the invertibility condition is naturally satisfied. To be sufficiently expressive, we compose multiple coupling layers as suggested in BIBREF16 . Specifically, we"
    ]
  },
  {
    "title": "Crowdsourcing for Beyond Polarity Sentiment Analysis A Pure Emotion Lexicon",
    "answer": "They compare lexicons by evaluating existing annotations through crowd evaluations and those of expert linguists. \n\nor \nBy comparing crowd evaluations to those of expert linguists.",
    "evidence": [
      "Lexicon Analysis",
      "submit their answers that go through a filtering process. If the answers are considered valid, they update the lexicon entries. The crowd also evaluates existing annotations, to determine the lexicon quality. As crowd evaluation methods are new in lexicon acquisition tasks, we compare crowd evaluations to those of expert linguists.",
      "goal is to maintain an end to end automated work-flow for a crowdsourced (annotation and evaluation wise) lexicon acquisition process. Therefore, to highlight crowd's performance on evaluation, we compare evaluations from linguistic experts and the crowd itself."
    ]
  },
  {
    "title": "Explicit Utilization of General Knowledge in Machine Reading Comprehension",
    "answer": "end-to-end MRC model",
    "evidence": [
      "Analysis\nAccording to the experimental results, KAR is not only comparable in performance with the state-of-the-art MRC models, but also superior to them in terms of both the hunger for data and the robustness to noise. The reasons for these achievements, we believe, are as follows:",
      "We compare KAR with other MRC models in both performance and the robustness to noise. Specifically, we not only evaluate the performance of KAR on the development set and the test set, but also do this on the adversarial sets. As for the comparative objects, we only consider the single MRC models that rank in the top 20 on the SQuAD 1.1 leader board and have reported their performance on the",
      "As shown in Figure  , KAR is an end-to-end MRC model consisting of five layers: Lexicon Embedding Layer. This layer maps the words to the lexicon embeddings. The lexicon embedding of each word is composed of its word embedding and character embedding. For each word, we use the pre-trained GloVe BIBREF14 word vector as its word embedding, and obtain its character embedding with a Convolutional"
    ]
  },
  {
    "title": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "answer": "the desired vector dimension",
    "evidence": [
      "achieve increased dimension values by virtue of their cooccurrence statistics with the thesaurus-based words (indirectly, without being explicitly forced). This again illustrates that a semantic concept can indeed be coded to a vector dimension provided that a sensible lexical resource is used to guide semantically related words to the desired vector dimension via the proposed objective function",
      "obtained from Roget's Thesaurus (and are thus explicitly forced to achieve increased dimension values), while the red words denote the words that achieve increased dimension values by virtue of their cooccurrence statistics with the thesaurus-based words (indirectly, without being explicitly forced). This again illustrates that a semantic concept can indeed be coded to a vector dimension provided"
    ]
  },
  {
    "title": "Stance Detection in Turkish Tweets",
    "answer": "Galatasaray and Fenerbahçe.",
    "evidence": [
      "for sports-related events at every instant. Hence we have determined our targets as Galatasaray (namely Target-1) and Fenerbahçe (namely, Target-2) which are two of the most popular football clubs in Turkey. As is the case for the sentiment analysis tools, the outputs of the stance detection systems on a stream of tweets about these clubs can facilitate the use of the opinions of the football"
    ]
  },
  {
    "title": "Deep Learning the EEG Manifold for Phonological Categorization from Active Thoughts",
    "answer": "***14 participants***.",
    "evidence": [
      "The dataset consists of 14 participants, with each prompt presented 11 times to each individual. Since our intention is to classify the phonological categories from human thoughts, we discard the facial and audio information and only consider the EEG data corresponding to imagined speech. It is noteworthy that given the mixed nature of EEG signals, it is reportedly challenging to attain a"
    ]
  },
  {
    "title": "Towards Understanding Neural Machine Translation with Word Importance",
    "answer": "two representative model architectures",
    "evidence": [
      "and two representative model architectures, demonstrating its superiority on estimating word importance. We analyze the linguistic behaviors of words with the importance and show its potential to improve NMT models. First, we leverage the word importance to identify input words that are under-translated by NMT models. Experimental results show that the gradient-based approach outperforms both the",
      "the gradient-based approach via quantitative comparison with black-box methods on a couple of perturbation operations, several language pairs, and two representative model architectures, demonstrating its superiority on estimating word importance. We analyze the linguistic behaviors of words with the importance and show its potential to improve NMT models. First, we leverage the word importance"
    ]
  },
  {
    "title": "How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings",
    "answer": "They calculate a static embedding for each word by taking the **first principal component (PC)** of its contextualized representations in a given layer.",
    "evidence": [
      "Related Work\nStatic Word Embeddings",
      "As noted earlier, we can create static embeddings for each word by taking the first principal component (PC) of its contextualized representations in a given layer. In Table , we plot the performance of these PC static embeddings on several benchmark tasks. These tasks cover semantic similarity, analogy solving, and concept categorization: SimLex999 BIBREF21, MEN BIBREF22, WS353 BIBREF23, RW"
    ]
  },
  {
    "title": "Paraphrase-Supervised Models of Compositionality",
    "answer": "a hierarchical phrase-based system BIBREF29",
    "evidence": [
      "that matches previous techniques in intrinsic evaluation tasks. Our approaches are also evaluated for their impact on a machine translation system where we show improvements in translation quality, demonstrating that compositionality in interpretation correlates with compositionality in translation.",
      "leading to better translations. To test this hypothesis, we built an English-Spanish MT system using the cdec decoder BIBREF27 for the entire training pipeline (word alignments, phrase extraction, feature weight tuning, and decoding). Corpora from the WMT 2011 evaluation was used to build the translation and language models, and for tuning (on news-test2010) and evaluation (on news-test2011),",
      "Machine Translation",
      "system that a phrase is non-compositional, it should be able to learn that translation decisions which translate it as a unit are to be favored, leading to better translations. To test this hypothesis, we built an English-Spanish MT system using the cdec decoder BIBREF27 for the entire training pipeline (word alignments, phrase extraction, feature weight tuning, and decoding). Corpora from the",
      "by comparing a type-based and token-based approach on a compound noun evaluation task (§  ); and third, determine if the compositionality-scoring models based on learned representations improve the translations produced by a state-of-the-art phrase-based MT system (§  ). The word vectors used in all of our experiments were produced by word2vec using the skip-gram model with 20 negative samples, a",
      "a high score informs the decoder that it is safe to rely on word-level translations of the phrasal constituents. Thus, if we reveal to the translation system that a phrase is non-compositional, it should be able to learn that translation decisions which translate it as a unit are to be favored, leading to better translations. To test this hypothesis, we built an English-Spanish MT system using",
      "their phrasal parser from their word-based parser. In contrast, we score every phrase that is extracted with the standard phrase extraction heuristics BIBREF29 , allowing the decoder to make the final decision on the impact of compositionality scores in translation. Thus, our work is more similar to Xiong2010, who propose maximum entropy classifiers that mark positions between words in a sentence",
      "build the translation and language models, and for tuning (on news-test2010) and evaluation (on news-test2011), with scoring done using BLEU BIBREF28 . The baseline is a hierarchical phrase-based system BIBREF29 with a 4-gram language model, with feature weights tuned using MIRA BIBREF30 . For features, each translation rule is decorated with two lexical and phrasal features corresponding to the"
    ]
  },
  {
    "title": "Incorporating Subword Information into Matrix Factorization Word Embeddings",
    "answer": "explicit matrix factorization.",
    "evidence": [
      ", BIBREF23 , BIBREF24 , BIBREF25 . Our model also uses n-grams and morphological segmentation, but it performs explicit matrix factorization to learn subword and word representations, unlike these related models which mostly use neural networks. Finally, BIBREF26 and BIBREF27 retrofit morphological information onto pre-trained models. These differ from our work in that we incorporate",
      "base our work). Closely related are models that use morphological segmentation in learning word representations BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 , BIBREF25 . Our model also uses n-grams and morphological segmentation, but it performs explicit matrix factorization to learn subword and word representations, unlike these related models which mostly use neural networks. Finally,",
      "jointly as weighted factorization of a word-context co-occurrence matrix is performed. There are many models that use character-level subword information to form word representations BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 , as well as fastText (the model on which we base our work). Closely related are models that use morphological segmentation in learning word representations",
      "The LexVec BIBREF7 model factorizes the PPMI-weighted word-context co-occurrence matrix using stochastic gradient descent.  $$PPMI_{wc} = max(0, \\log \\frac{M_{wc} \\; M_{**}}{ M_{w*} \\; M_{*c} })$$   (Eq. 3)  where $M$ is the word-context co-occurrence matrix constructed by sliding a window of fixed size centered over every target word  $w$ in the subsampled BIBREF2 training corpus and"
    ]
  },
  {
    "title": "Abstractive Summarization for Low Resource Data using Domain Transfer and Data Synthesis",
    "answer": "pointer networks with coverage mechanism (PG-net)",
    "evidence": [
      "Abstractive Summarization. Abstractive summarization aims to generate coherent summaries with high readability, and has seen increasing interest and improved performance due to the emergence of seq2seq models BIBREF8 and attention mechanisms BIBREF9. For example, BIBREF0, BIBREF2, and BIBREF1 in addition to using encoder-decoder model with attention, they used pointer networks to solve the out of",
      "synthesis method to create synthesized summaries, then explore the effect of enriching training data for abstractive summarization using the proposed model compared to a synthesis baseline. Lastly, we combine both directions. Evaluations of neural abstractive summarization method across four student reflection corpora show the utility of all three methods.",
      "We explored improving the performance of neural abstractive summarizers when applied to the low resource domain of student reflections using three approaches: domain transfer, data synthesis and the combination of both. For domain transfer, state of the art abstractive summarization model was pretrained using out-of-domain data (CNN/DM), then tuned using in-domain data (student reflections). The",
      "Training abstractive summarization models typically requires large amounts of data, which can be a limitation for many domains. In this paper we explore using domain transfer and data synthesis to improve the performance of recent abstractive summarization methods when applied to small corpora of student reflections. First, we explored whether tuning state of the art model trained on newspaper",
      "we explore using domain transfer and data synthesis to improve the performance of recent abstractive summarization methods when applied to small corpora of student reflections. First, we explored whether tuning state of the art model trained on newspaper data could boost performance on student reflection data. Evaluations demonstrated that summaries produced by the tuned model achieved higher",
      "Recently, with the emergence of neural seq2seq models, abstractive summarization methods have seen great performance strides BIBREF0, BIBREF1, BIBREF2. However, complex neural summarization models with thousands of parameters usually require a large amount of training data. In fact, much of the neural summarization work has been trained and tested in news domains where numerous large datasets",
      "approaches: domain transfer, data synthesis and the combination of both. For domain transfer, state of the art abstractive summarization model was pretrained using out-of-domain data (CNN/DM), then tuned using in-domain data (student reflections). The process of tuning improved ROUGE scores on the student reflection data, and at the same time produced more readable summaries. To incorporate",
      "our case of scarce in-domain data. In a different approach to abstractive summarization, BIBREF11 developed a soft template based neural method consisting of an end-to-end deep model for template retrieval, reranking and summary rewriting. While we also develop a template based model, our work differs in both model structure and purpose. Data Synthesis. Data synthesis for text summarization is",
      "explore domain transfer for abstractive summarization. While domain transfer is not new, compared to prior summarization studies BIBREF6, BIBREF7, our training (news) and tuning (student reflection) domains are quite dissimilar, and the in-domain data is small. Second, we propose a template-based synthesis method to create synthesized summaries, then explore the effect of enriching training data",
      "and BIBREF1 in addition to using encoder-decoder model with attention, they used pointer networks to solve the out of vocabulary issue, while BIBREF0 used coverage mechanism to solve the problem of word repetition. In addition, BIBREF2 and BIBREF10 used reinforcement learning in an end-to-end setting. To our knowledge, training such neural abstractive summarization models in low resource domains",
      "our training (news) and tuning (student reflection) domains are quite dissimilar, and the in-domain data is small. Second, we propose a template-based synthesis method to create synthesized summaries, then explore the effect of enriching training data for abstractive summarization using the proposed model compared to a synthesis baseline. Lastly, we combine both directions. Evaluations of neural",
      "later show that this is the case for student reflections. To improve performance in low resource domains, we explore three directions. First, we explore domain transfer for abstractive summarization. While domain transfer is not new, compared to prior summarization studies BIBREF6, BIBREF7, our training (news) and tuning (student reflection) domains are quite dissimilar, and the in-domain data is",
      "was not able to beat training only on in-domain data. This is likely because their in and out-of-domain data sizes are comparable, unlike in our case of scarce in-domain data. In a different approach to abstractive summarization, BIBREF11 developed a soft template based neural method consisting of an end-to-end deep model for template retrieval, reranking and summary rewriting. While we also",
      "used coverage mechanism to solve the problem of word repetition. In addition, BIBREF2 and BIBREF10 used reinforcement learning in an end-to-end setting. To our knowledge, training such neural abstractive summarization models in low resource domains using domain transfer has not been thoroughly explored on domains different than news. For example, BIBREF4 reported the results of training on CNN/DM",
      "setting. To our knowledge, training such neural abstractive summarization models in low resource domains using domain transfer has not been thoroughly explored on domains different than news. For example, BIBREF4 reported the results of training on CNN/DM data while evaluating on DUC data without any tuning. Note that these two datasets are both in the news domain, and both consist of well",
      "hypothesize that training complex neural abstractive summarization models in such domains will not yield good performing models, and we will indeed later show that this is the case for student reflections. To improve performance in low resource domains, we explore three directions. First, we explore domain transfer for abstractive summarization. While domain transfer is not new, compared to prior",
      "and fine tuning. Finally, BIBREF1 reported that while training with domain transfer outperformed training only on out-of-domain data, it was not able to beat training only on in-domain data. This is likely because their in and out-of-domain data sizes are comparable, unlike in our case of scarce in-domain data. In a different approach to abstractive summarization, BIBREF11 developed a soft",
      "just student reflection data or just newspaper data. The tuned model also achieved higher scores compared to extractive summarization baselines, and additionally was judged to produce more coherent and readable summaries in human evaluations. Second, we explored whether synthesizing summaries of student data could additionally boost performance. We proposed a template-based model to synthesize",
      "by $N$, which is 3 in our case. Table  shows a human summary, the keywords extracted, then the output of injecting keywords in a different template using rewriting. Template-based Summarization (for testing H6). While the proposed template-based model was intended for data synthesis, with minor modification it can be adapted for summarization itself. Because the modifications introduce few",
      "is another recent work that was done in parallel and is very close to ours. However, in addition to the difference in both our and their model, we think it might be infeasible to back generate student reflections from a human summary, especially an abstractive one.",
      "was pretrained using out-of-domain data (CNN/DM), then tuned using in-domain data (student reflections). The process of tuning improved ROUGE scores on the student reflection data, and at the same time produced more readable summaries. To incorporate synthetic data, we proposed a new template based synthesis model to synthesize new summaries. We showed that enriching the training data with this",
      "To overcome the size issue of the student reflection dataset, we first explore the effect of incorporating domain transfer into a recent abstractive summarization model: pointer networks with coverage mechanism (PG-net)BIBREF0. To experiment with domain transfer, the model was pretrained using the CNN/DM dataset, then fine tuned using the student reflection dataset (see the Experiments section).",
      "Conclusions and Future Work",
      "consisting of an end-to-end deep model for template retrieval, reranking and summary rewriting. While we also develop a template based model, our work differs in both model structure and purpose. Data Synthesis. Data synthesis for text summarization is underexplored, with most prior work focusing on machine translation, and text normalization. BIBREF12 proposed doing data augmentation through"
    ]
  },
  {
    "title": "Revisiting Summarization Evaluation for Scientific Articles",
    "answer": "ROUGE is reliable in evaluating scientific summaries.",
    "evidence": [
      "to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries."
    ]
  },
  {
    "title": "Edinburgh Neural Machine Translation Systems for WMT 16",
    "answer": "***Baseline System***. \n\nHowever, based on common academic contexts, a more precise answer would be: \n***The Baseline System*** is not defined in the provided context; however, generally, baseline systems refer to ***pre-existing standard systems*** used for comparison. \n\nGiven the lack of specifics in the context, a more suitable response based on typical structure of such questions would simply be: \n***The given context does not specify***. \n\nBut if I were to extract or provide based on common usage and assuming a direct query:\nThe term itself, ***Baseline System***, suggests a ***standard system*** or ***initial system***. \n\nDirect and concise with available information: \n***Not specified***.",
    "evidence": [
      "Baseline System"
    ]
  },
  {
    "title": "Embracing data abundance: BookTest Dataset for Reading Comprehension",
    "answer": "They suggest that one possibility for improvement is to use more training data.",
    "evidence": [
      "Possible Directions for Improvements",
      "possibility to improve performance is simply to use more training data. The importance of training data was highlighted by the frequently quoted Mercer's statement that “There is no data like more data.” The observation that having more data is often more important than having better algorithms has been frequently stressed since then BIBREF13 , BIBREF14 . As a step in the direction of exploiting"
    ]
  },
  {
    "title": "A Human-AI Loop Approach for Joint Keyword Discovery and Expectation Estimation in Micropost Event Detection",
    "answer": "The keyword-specific expectation is elicited from the crowd through crowd workers who estimate it.",
    "evidence": [
      "their expectations reliably. Our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords. More specifically, at each iteration after we obtain a keyword-specific expectation from the crowd, we train the model using expectation regularization and select those",
      "expectation from the crowd, we train the model using expectation regularization and select those keyword-related microposts for which the model's prediction disagrees the most with the crowd's expectation; such microposts are then presented to the crowd to identify new keywords that best explain the disagreement. By doing so, our approach identifies new keywords which convey more relevant"
    ]
  },
  {
    "title": "ARAML: A Stable Adversarial Training Framework for Text Generation",
    "answer": "SeqGAN, LeakGAN, IRL, MaliGAN.",
    "evidence": [
      "unit, thus we followed the implementation in the original paper. In terms of the differences, the discriminators of GAN baselines are implemented based on the original papers. Other hyper-parameters of baselines including batch size, learning rate, and pre-training epochs, were set based on the original codes, because the convergence of baselines is sensitive to these hyper-parameters.",
      "the generators of all the baselines except LeakGAN are the same as ours. Note that the generator of LeakGAN consists of a hierarchical LSTM unit, thus we followed the implementation in the original paper. In terms of the differences, the discriminators of GAN baselines are implemented based on the original papers. Other hyper-parameters of baselines including batch size, learning rate, and",
      "To verify the training stability, we conducted experiments on COCO many times and chose the best 5 trials for SeqGAN, LeakGAN, IRL, MaliGAN and ARAML, respectively. Then, we presented the forward/reverse perplexity in the training process in Figure  . We can see that our model with smaller standard deviation is more stable than other GAN baselines in both metrics. Although LeakGAN reaches the"
    ]
  },
  {
    "title": "Predicting the Industry of Users on Social Media",
    "answer": "Twitter.",
    "evidence": [
      "Alongside the wide adoption of social media by the public, researchers have been leveraging the newly available data to create and refine models of users' behavior and profiling. There exists a myriad research that analyzes language in order to profile social media users. Some studies sought to characterize users' personality BIBREF9 , BIBREF10 , while others sequenced the expressed emotions",
      "of users' behavior and profiling. There exists a myriad research that analyzes language in order to profile social media users. Some studies sought to characterize users' personality BIBREF9 , BIBREF10 , while others sequenced the expressed emotions BIBREF11 , studied mental disorders BIBREF12 , and the progression of health conditions BIBREF13 . At the same time, a number of researchers sought",
      "have used social media content to extract and study collective social attributes, there is a lack of substantial research that addresses the detection of a user's industry. We frame this task as classification using both feature engineering and ensemble learning. Our industry-detection system uses both posted content and profile information to detect a user's industry with 64.3% accuracy,",
      "BIBREF18 , BIBREF19 . One of the profile fields that has drawn a great deal of attention is the location of a user. Among others, Hecht et al. Hecht11 predicted Twitter users' locations using machine learning on nationwide and state levels. Later, Han et al. Han14 identified location indicative words to predict the location of Twitter users down to the city level. As a separate line of research,"
    ]
  },
  {
    "title": "Open Event Extraction from Online Text using a Generative Adversarial Network",
    "answer": "The proposed model, referred to as AEM, overcomes the assumption that all words in a document are generated from a single event by not making such an assumption, unlike previous models like LEM and DPEMM which have this limitation. \n\nHowever, a more precise answer based on the context provided is that AEM does not explicitly state how it overcomes this; but it implies an improvement over LEM and DPEMM which are restricted by this assumption. \n\nTherefore, a concise and precise answer would be: \nby not assuming all words are from a single event. \n\nOr alternatively and more academically: \nAEM does not rely on the assumption that all words in a document are generated from a single event.",
    "evidence": [
      "assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical models often rely on Gibbs sampling for parameter inference which may take long time to converge. To address these limitations, we propose an event extraction model",
      "To extract the structured representations of open-domain events, Bayesian graphical models have made some progress. However, these approaches typically assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical models often",
      "automatically from data. However, both LEM and DPEMM have two limitations: (1) they assume that all words in a document are generated from a single event which can be represented by a quadruple  entity, location, keyword, date  . However, long texts such news articles often describe multiple events which clearly violates this assumption; (2) During the inference process of both approaches, the",
      "because: (1) the assumption made by LEM and DPEMM that all words in a document are generated from a single event is not suitable for long text such as news articles; (2) DPEMM generates too many irrelevant events which leads to a very low precision score. Overall, we see the superior performance of AEM across all datasets, with more significant improvement on the for Google datasets (long text)."
    ]
  },
  {
    "title": "Copenhagen at CoNLL--SIGMORPHON 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding",
    "answer": "***Mean Squared Deviation (MSD) prediction***. \n\nor \n\n***Mean Squared Error (MSE) prediction***, assuming they are referring to similar concepts.\n\nGiven no further context, a precise definition: \n\nMSD prediction refers to the ***forecasting of Mean Squared Deviation***, often used in statistical analysis and modeling to evaluate the average squared differences between predicted and actual values.",
    "evidence": [
      "MSD prediction"
    ]
  },
  {
    "title": "Exploiting Task-Oriented Resources to Learn Word Embeddings for Clinical Abbreviation Expansion",
    "answer": "clinical notes data, Wikipedia articles, and medical scientific papers. \n\nor more precisely: \n1,160 raw ICU clinical notes data.",
    "evidence": [
      "And a direct comparison is also impossible because all previous work used different data sets which are not publicly available. Alternatively, we use the following baselines to compare with our approach. Rating: This baseline model chooses the highest rating candidate expansion in the domain specific knowledge base. Raw Input embeddings: We trained word embeddings only from the 1,160 raw ICU",
      "Baseline Models",
      "base as candidates. We train word embeddings using the clinical notes data with task-oriented resources such as Wikipedia articles of candidates and medical scientific papers and compute the semantic similarity between an abbreviation and its candidate expansions based on their embeddings (vector representations of words).",
      "all abbreviations using regular expressions and then try to find all possible expansions of these abbreviations from domain-specific knowledge base as candidates. We train word embeddings using the clinical notes data with task-oriented resources such as Wikipedia articles of candidates and medical scientific papers and compute the semantic similarity between an abbreviation and its candidate",
      "For our task, it's difficult to re-implement the supervised methods as in previous works mentioned since we do not have sufficient training data. And a direct comparison is also impossible because all previous work used different data sets which are not publicly available. Alternatively, we use the following baselines to compare with our approach. Rating: This baseline model chooses the highest"
    ]
  },
  {
    "title": "Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources",
    "answer": "The nine types are: agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, question, and other.",
    "evidence": [
      "Reaction Type Classification Results",
      "Reaction Type Classification\nIn this section, we describe our approach to classify user reactions into one of eight types of discourse: agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, or question, or as none of the given labels, which we call “other”, using linguistically-infused neural network models.",
      "In the current work, we have presented a content-based model that classifies user reactions into one of nine types, such as answer, elaboration, and question, etc., and a large-scale analysis of Twitter posts and Reddit comments in response to content from news sources of varying credibility. Our analysis of user reactions to trusted and deceptive sources on Twitter and Reddit shows significant"
    ]
  },
  {
    "title": "Deep Semi-Supervised Learning with Linguistically Motivated Sequence Labeling Task Hierarchies",
    "answer": "***3*** \n\n Reasoning: The context mentions that \"our model has 3 sources of error signals - one for each task\", which directly implies that there are 3 supervised tasks used.",
    "evidence": [
      "Supervision of Multiple Tasks\nOur model has 3 sources of error signals - one for each task. Since each task is categorical we use the discrete cross entropy to calculate the loss for each task: $\nH(p, q) = - \\sum _{i}^{n_{labels}} p(label_i) \\ log \\ q(label_i)"
    ]
  },
  {
    "title": "Yoga-Veganism: Correlation Mining of Twitter Health Data",
    "answer": "Women-Yoga.",
    "evidence": [
      "are shown. We observe interesting hidden correlation in data. Fig.  has Topic 2 as selected topic. Topic 2 contains top-4 co-occurring keywords \"vegan\", \"yoga\", \"job\", \"every_woman\" having the highest term frequency. We can infer different things from the topic that \"women usually practice yoga more than men\", \"women teach yoga and take it as a job\", \"Yogi follow vegan diet\". We would say there",
      "\"vegan\", \"yoga\", \"job\", \"every_woman\" having the highest term frequency. We can infer different things from the topic that \"women usually practice yoga more than men\", \"women teach yoga and take it as a job\", \"Yogi follow vegan diet\". We would say there are noticeable correlation in data i.e. `Yoga-Veganism', `Women-Yoga'.",
      "ground truth, employed different visualizations after information integration and discovered interesting correlation (Yoga-Veganism) in data. In future, we will incorporate Local Interpretable model-agnostic Explanation (LIME) method to understand model interpretability."
    ]
  },
  {
    "title": "Learning Relational Dependency Networks for Relation Extraction",
    "answer": "relations",
    "evidence": [
      "Joint learning",
      "appears to help in about half of the relations (8/14). Particularly, in person category, joint learning with gold standard outperforms their individual learning counterparts. This is due to the fact that some relations such as parents, spouse, siblings etc. are inter-related and learning them jointly indeed improves performance. Hence Q2 can be answered affirmatively for half the relations.",
      "To address our next question, we assessed our pipeline when learning relations independently (i.e., individually) versus learning relations jointly within the RDN, displayed in Table  . Recall and F1 are omitted for conciseness – the conclusions are the same across all metrics. Joint learning appears to help in about half of the relations (8/14). Particularly, in person category, joint learning"
    ]
  },
  {
    "title": "Simultaneous Neural Machine Translation using Connectionist Temporal Classification",
    "answer": "BLEU, RIBES, and token-level delay.",
    "evidence": [
      "and it translated sentences from full-length source sentences (called Full Sentence). For evaluation metrics, we used BLEU BIBREF8 and RIBES BIBREF9 to measure translation accuracy, and token-level delay to measure latency. We used Kytea BIBREF10 as a tokenize method for evaluations of Japanese translation accuracy.",
      "Simultaneous machine translation is a variant of machine translation that starts the translation process before the end of an input. This task faces a trade-off between translation accuracy and latency. We have to determine when we start the translation for observed inputs so far, to achieve good practical performance. In this work, we propose a neural machine translation method to determine this",
      "Simultaneous machine translation by Wait-k model\nEncoder",
      "Simultaneous machine translation by Wait-k model"
    ]
  },
  {
    "title": "Civique: Using Social Media to Detect Urban Emergencies",
    "answer": "Support Vector Machines (SVM) and Naive Bayes (NB).",
    "evidence": [
      "Emergency Classification",
      "We use traditional classification techniques such as Support Vector Machines(SVM), and Naive Bayes(NB) for training, and perform 10-fold cross validation to obtain f-scores. Later, in real time, our system uses the Twitter streaming APIs to get data, pre-processes it using the same modules, and detects emergencies using the classifiers built above. The tweets related to emergencies are displayed"
    ]
  },
  {
    "title": "Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation",
    "answer": "The data was annotated by experts/students/researchers (no specific information is given). \n\nHowever, a more precise answer based on typical contexts would be: \n\n\"The data annotators.\" or \"Human annotators.\" \n\nBut if I have to give one concise phrase based on general knowledge:\n\n\"Human experts.\"",
    "evidence": [
      "Dataset and Annotation"
    ]
  },
  {
    "title": "Deep Health Care Text Classification",
    "answer": "LSTM (Long Short-Term Memory) and RNN (Recurrent Neural Network). \n\nHowever, a more precise answer would be: \nLSTM and Vanilla RNN or just  **LSTM**.",
    "evidence": [
      "Recurrent neural network (RNN) was an enhanced model of feed forward network (FFN) introduced in 1990 BIBREF10 . The input sequences ${x_T}$ of arbitrary length are passed to RNN and a transition function $tf$ maps them into hidden state vector $h{i_{t - 1}}$ recursively. The hidden state vector $h{i_{t - 1}}$ are calculated based on the transition function $tf$ of present input sequence ${x_T}$",
      "Recurrent neural network (RNN) and it’s variant",
      "the existing methods are based on machine learning with knowledge-based learning. This working note presents the Recurrent neural network (RNN) and Long short-term memory (LSTM) based embedding for automatic health text classification in the social media mining. For each task, two systems are built and that classify the tweet at the tweet level. RNN and LSTM are used for extracting features and",
      "computational framework in conjunction with Keras framework in Ubuntu 14.04. We have submitted one run based on LSTM for task 1 and two runs composed of one run based on RNN and other one based on LSTM for task 2. The evaluation results is given by shared task committee are reported in Table 3 and 4.",
      "and Long short-term memory (LSTM) based embedding for automatic health text classification in the social media mining. For each task, two systems are built and that classify the tweet at the tweet level. RNN and LSTM are used for extracting features and non-linear activation function at the last layer facilitates to distinguish the tweets of different categories. The experiments are conducted on",
      "Background and hyper parameter selection\nThis section discusses the concepts of tweet representation and deep learning algorithms particularly recurrent neural network (RNN) and long short-term memory (LSTM) in a mathematical way.",
      "All experiments are trained using backpropogation through time (BPTT) BIBREF14 on Graphics processing unit (GPU) enabled TensorFlow BIBREF6 computational framework in conjunction with Keras framework in Ubuntu 14.04. We have submitted one run based on LSTM for task 1 and two runs composed of one run based on RNN and other one based on LSTM for task 2. The evaluation results is given by shared",
      "arbitrary length are passed to RNN and a transition function $tf$ maps them into hidden state vector $h{i_{t - 1}}$ recursively. The hidden state vector $h{i_{t - 1}}$ are calculated based on the transition function $tf$ of present input sequence ${x_T}$ and previous hidden state vector $h{i_{t - 1}}$ . This can be mathematically formulated as follows  $$h{i_t} = \\,\\left\\lbrace  \\begin{array}{l}",
      "are built and that classify the tweet at the tweet level. RNN and LSTM are used for extracting features and non-linear activation function at the last layer facilitates to distinguish the tweets of different categories. The experiments are conducted on 2nd Social Media Mining for Health Applications Shared Task at AMIA 2017. The experiment results are considerable; however the proposed method is",
      "the large amount of raw data sets. To make use of unlabeled data, BIBREF9 proposed semi-supervised approach based on Convolutional neural network for adverse drug event detection. Though the data sets of task 1 and task 2 are limited, this paper proposes RNN and LSTM based embedding method.",
      "size, two trails of experiments are run with embedding size 128, 256 and 512. For each experiment, learning rate is set to 0.01. An experiment with embedding size 512 performed well in both the RNN and LSTM networks. Thus for the rest of the experiments embedding size is set to 512. The embedding layer output vector is further passed to RNN and its variant LSTM layer. RNN and LSTM obtain the",
      "with embedding size 512 performed well in both the RNN and LSTM networks. Thus for the rest of the experiments embedding size is set to 512. The embedding layer output vector is further passed to RNN and its variant LSTM layer. RNN and LSTM obtain the optimal feature representation and those feature representation are passed to the dropout layer. Dropout layer contains 0.1 which removes the",
      "Social media mining is considerably an important source of information in many of health applications. This working note presents RNN and LSTM based embedding system for social media health text classification. Due to limited number of tweets, the performance of the proposed method is very less. However, the obtained results are considerable and open the way in future to apply for the social",
      "The embedding layer output vector is further passed to RNN and its variant LSTM layer. RNN and LSTM obtain the optimal feature representation and those feature representation are passed to the dropout layer. Dropout layer contains 0.1 which removes the neurons and its connections randomly. This acts as a regularization parameter. In task 1 the output layer contains $sigmoid$ activation function",
      "Health related social media mining is a valuable apparatus for the early recognition of the diverse antagonistic medicinal conditions. Mostly, the existing methods are based on machine learning with knowledge-based learning. This working note presents the Recurrent neural network (RNN) and Long short-term memory (LSTM) based embedding for automatic health text classification in the social media",
      "tweets appear in a same cluster with close to each other in a high dimensional geometric space. To select optimal parameter for the embedding size, two trails of experiments are run with embedding size 128, 256 and 512. For each experiment, learning rate is set to 0.01. An experiment with embedding size 512 performed well in both the RNN and LSTM networks. Thus for the rest of the experiments",
      "and those feature representation are passed to the dropout layer. Dropout layer contains 0.1 which removes the neurons and its connections randomly. This acts as a regularization parameter. In task 1 the output layer contains $sigmoid$ activation function and $softmax$ activation function for task 2.",
      "mainly due to that it doesn't rely on any feature engineering mechanism. However, the performance of deep learning methods implicitly relies on the large amount of raw data sets. To make use of unlabeled data, BIBREF9 proposed semi-supervised approach based on Convolutional neural network for adverse drug event detection. Though the data sets of task 1 and task 2 are limited, this paper proposes",
      "approach for classifying the adverse drug reactions tweets. Recently, the deep learning methods have performed well BIBREF8 and used in many tasks mainly due to that it doesn't rely on any feature engineering mechanism. However, the performance of deep learning methods implicitly relies on the large amount of raw data sets. To make use of unlabeled data, BIBREF9 proposed semi-supervised approach",
      "text classification. The existing methods have used machine learning methods with feature engineering. Most commonly used features are n-grams, parts-of-speech tags, term frequency-inverse document frequency, semantic features such as mentions of chemical substance and disease, WordNet synsets, adverse drug reaction lexicon, etc BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In BIBREF5 ,",
      "+ \\,{P_{ig}}h{i_{t - 1}} + \\,{Q_{ig}}{m_{t - 1}} + {b_{ig}})$$   (Eq. 3)  $$f{g_t} = \\sigma ({w_{fg}}{x_t} + \\,{P_{fg}}h{i_{t - 1}} + \\,{Q_{fg}}{m_{t - 1}} + {b_{fg}})$$   (Eq. 4)  where ${x_t}$ is the input at time step $t$ , $P$ and $Q$ are weight parameters, $\\sigma $ is sigmoid activation function, $\\odot $ denotes element-wise multiplication.",
      "synsets, adverse drug reaction lexicon, etc BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In BIBREF5 , BIBREF7 proposed ensemble based approach for classifying the adverse drug reactions tweets. Recently, the deep learning methods have performed well BIBREF8 and used in many tasks mainly due to that it doesn't rely on any feature engineering mechanism. However, the performance of deep",
      "Moreover, the performance of the LSTM based embedding for task 2 is good in comparison to the task 1. This is primarily due to the fact that the target classes of task 1 data set imbalanced. Hence, the proposed method can be applied on large number of tweets corpus in order to attain the best performance.",
      "\\end{array} \\right\\rbrace $$   (Eq. 2)  This kind of transition function results in vanishing and exploding gradient issue while training BIBREF11 . To alleviate, LSTM was introduced BIBREF11 , BIBREF12 , BIBREF13 . LSTM network contains a special unit typically called as a memory block. A memory block composed of a memory cell $m$ and set of gating functions such as input gate $(ig)$ , forget",
      "is very less. However, the obtained results are considerable and open the way in future to apply for the social media health text classification. Moreover, the performance of the LSTM based embedding for task 2 is good in comparison to the task 1. This is primarily due to the fact that the target classes of task 1 data set imbalanced. Hence, the proposed method can be applied on large number of"
    ]
  },
  {
    "title": "MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims",
    "answer": "Encoding claim texts only.",
    "evidence": [
      "Experimental Setup",
      "veracity that outperforms all baselines. Significant performance increases are achieved by encoding evidence, and by modelling metadata. Our best-performing model achieves a Macro F1 of 49.2%, showing that this is a challenging testbed for claim veracity prediction.",
      "Selection of sources",
      "For each domain, we compute the Micro as well as Macro F1, then mean average results over all domains. Core results with all vs. no metadata are shown in Table  . We first experiment with different base model variants and find that label embeddings improve results, and that the best proposed models utilising multiple domains outperform single-task models (see Table  ). This corroborates the",
      "The dataset is split into a training part (80%) and a development and testing part (10% each) in a label-stratified manner. Note that the domains vary in the number of labels, ranging from 2 to 27. Labels include both straight-forward ratings of veracity (`correct', `incorrect'), but also labels that would be more difficult to map onto a veracity scale (e.g. `grass roots movement!',",
      "(e.g. `Not true', vs. `Mostly False'). We made sure that all such duplicate claims would be in the training split of the dataset, so that the models would not have an unfair advantage. Finally, we performed some minor manual merging of label types for the same domain where it was clear that they were supposed to denote the same level of veracity (e.g. `distorts', `distorts the facts'). This",
      "So far, we have ignored the issue of how to obtain claim representation, as the base model described in the previous section is agnostic to how instances are encoded. A very simple approach, which we report as a baseline, is to encode claim texts only. Such a model ignores evidence for and against a claim, and ends up guessing the veracity based on surface patterns observed in the claim texts. We"
    ]
  },
  {
    "title": "Retrieval-based Goal-Oriented Dialogue Generation",
    "answer": "semantically conditioned models explicitly using past dialog act information by Chen et al., 2019.",
    "evidence": [
      "including responses that are rated more appropriate and fluent by human evaluators. Finally, we compare our retrieval-based model to various semantically conditioned models explicitly using past dialog act information, and find that our proposed model is competitive with the current state of the art (Chen et al., 2019), while not requiring explicit labels about past machine acts."
    ]
  },
  {
    "title": "User Generated Data: Achilles' heel of BERT",
    "answer": "Introduction of noise (spelling mistakes).",
    "evidence": [
      "performance. Also, from the plots it is evident that the reason for this drop in performance is introduction of noise (spelling mistakes). After all we get very good numbers, for each of the three tasks, when there is no error (0.0 % error). To understand the reason behind the drop in performance, first we need to understand how BERT processes input text data. BERT uses WordPiece tokenizer to"
    ]
  },
  {
    "title": "Cohesion and Coalition Formation in the European Parliament: Roll-Call Votes and Twitter Activities",
    "answer": "The analysis provides insights into the cohesion of political groups in the Eighth European Parliament, both overall and across different policy areas. \n\n Specifically, it gives insights into: \n- the cohesion of political groups overall \n- the cohesion across different policy areas.",
    "evidence": [
      "insights into the cohesion of political groups in the Eighth European Parliament, both overall and across different policy areas. (ii) We explore whether coalitions are formed in the same way for different policy areas. (iii) We explore to what degree the retweeting behavior of MEPs corresponds to their co-voting patterns. (iv) We employ two statistically sound methodologies and examine the"
    ]
  },
  {
    "title": "S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension",
    "answer": "MS-MARCO is different from SQuAD because it contains multiple passages for a question, whereas SQuAD has only one passage per question.",
    "evidence": [
      "in the answer are not necessary in the passages in the MS-MARCO dataset. Second, the SQuAD dataset only has one passage for a question, while the MS-MARCO dataset contains multiple passages. Existing methods for the MS-MARCO dataset usually follow the extraction based approach for single passage in the SQuAD dataset. It formulates the task as predicting the start and end positions of the answer"
    ]
  },
  {
    "title": "Neural Machine Translation with Supervised Attention",
    "answer": "The NIST2008 Open Machine Translation Campaign dataset, specifically: \n- 1.8M sentence pairs for training \n- nist02 (878 sentences) for development \n- nist05 (1082 sentences), nist06 (1664 sentences), and nist08 (1357 sentences) for testing.",
    "evidence": [
      "alignment on the training data before training SA-NMT. In this paper, we used two different aligners, which are fast_align and GIZA++. We tuned the hyper-parameter  to be 0.3 on the development set, to balance the preference between the translation and alignment. Training was conducted on a single Tesla K40 GPU machine. Each update took about 3.0 seconds for both NMT2 and SA-NMT, and 2.4 seconds",
      "from GIZA++, and subsequently apply the alignment abilities it has learned to unseen test sentences. Table  shows the overall alignment results on word alignment task in terms of the metric, alignment error rate. We used the manually-aligned dataset as in BIBREF5 as the test set. Following BIBREF17 , we force-decode both the bilingual sentences including source and reference sentences to obtain",
      "Specifically, except the stopping iteration which was selected using development data, we used the default settings set out in BIBREF0 for all NMT-based systems: the dimension of word embedding was 620, the dimension of hidden units was 1000, the batch size was 80, the source and target side vocabulary sizes were 30000, the maximum sequence length was 50, the beam size for decoding was 12, and",
      "We used the data from the NIST2008 Open Machine Translation Campaign. The training data consisted of 1.8M sentence pairs, the development set was nist02 (878 sentences), and the test sets are were nist05 (1082 sentences), nist06 (1664 sentences) and nist08 (1357 sentences). We compared the proposed approach with three strong baselines:  Moses: a phrase-based machine translation system BIBREF15 ;"
    ]
  },
  {
    "title": "An Unsupervised Word Sense Disambiguation System for Under-Resourced Languages",
    "answer": "cosine similarity",
    "evidence": [
      "Quality Measure",
      "Word Sense Disambiguation",
      "relevant sense of each input word with respect to the semantic similarity between the given sentence and the synset constituting the sense of the target word. Watasense has two modes of operation. The sparse mode uses the traditional vector space model to estimate the most similar word sense corresponding to its context. The dense mode, instead, uses synset embeddings to cope with the sparsity",
      "results in every case (Table  ). The primary reason for that is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words. Thus, we recommend using the dense approach in further studies. Although the AdaGram approach trained on a large text corpus showed better results according to the weighted average, this result does not transfer to",
      "ontology available for most natural languages. Due to the broad coverage of BabelNet, Babelfy offers entity linking as part of the WSD functionality. Panchenko:17:emnlp present an unsupervised WSD system that is also knowledge-free: its sense inventory is induced based on the JoBimText framework, and disambiguation is performed by computing the semantic similarity between the context and the",
      "The original SemEval 2010 Task 14 used the V-Measure external clustering measure BIBREF5 . However, this measure is maximized by clustering each sentence into his own distinct cluster, i.e., a `dummy' singleton baseline. This is achieved by the system deciding that every ambiguous word in every sentence corresponds to a different word sense. To cope with this issue, we follow a similar study",
      "vectors for semantically related words. Thus, we recommend using the dense approach in further studies. Although the AdaGram approach trained on a large text corpus showed better results according to the weighted average, this result does not transfer to languages with less available corpus size.",
      "Panchenko:17:emnlp present an unsupervised WSD system that is also knowledge-free: its sense inventory is induced based on the JoBimText framework, and disambiguation is performed by computing the semantic similarity between the context and the candidate senses BIBREF9 . Pelevina:16 proposed a similar approach to WSD, but based on dense vector representations (word embeddings), called SenseGram.",
      "sense for all the instances per lemma (One) and one sense per instance (Singletons). The evaluation results are presented in Table  . The columns bts-rnc and wiki-wiki represent the overall value of ARI according to Equation ( EQREF15 ). The column Avg. consists of the weighted average of the datasets w.r.t. the number of instances. We observe that the SenseGram-based approach for word sense",
      "and disambiguation is performed by computing the semantic similarity between the context and the candidate senses BIBREF9 . Pelevina:16 proposed a similar approach to WSD, but based on dense vector representations (word embeddings), called SenseGram. Similarly to SenseGram, our WSD system is based on averaging of word embeddings on the basis of an automatically induced sense inventory. A crucial",
      "We evaluate the word sense disambiguation methods in Watasense against three baselines: an unsupervised approach for learning multi-prototype word embeddings called AdaGram BIBREF15 , same sense for all the instances per lemma (One), and one sense per instance (Singletons). The AdaGram model is trained on the combination of RuWac, Lib.Ru, and the Russian Wikipedia with the overall vocabulary size",
      "We conduct our experiments using the evaluation methodology of SemEval 2010 Task 14: Word Sense Induction & Disambiguation BIBREF5 . In the gold standard, each word is provided with a set of instances, i.e., the sentences containing the word. Each instance is manually annotated with the single sense identifier according to a pre-defined sense inventory. Each participating system estimates the",
      "the datasets w.r.t. the number of instances. We observe that the SenseGram-based approach for word sense disambiguation yields substantially better results in every case (Table  ). The primary reason for that is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words. Thus, we recommend using the dense approach in further studies. Although",
      "bts-rnc and wiki-wiki represent the overall value of ARI according to Equation ( EQREF15 ). The column Avg. consists of the weighted average of the datasets w.r.t. the number of instances. We observe that the SenseGram-based approach for word sense disambiguation yields substantially better results in every case (Table  ). The primary reason for that is the implicit handling of similar words due",
      "each sentence into his own distinct cluster, i.e., a `dummy' singleton baseline. This is achieved by the system deciding that every ambiguous word in every sentence corresponds to a different word sense. To cope with this issue, we follow a similar study BIBREF1 and use instead of the adjusted Rand index (ARI) proposed by Hubert:85 as an evaluation measure. In order to provide the overall value",
      "the target word. Watasense has two modes of operation. The sparse mode uses the traditional vector space model to estimate the most similar word sense corresponding to its context. The dense mode, instead, uses synset embeddings to cope with the sparsity problem. We describe the architecture of the present system and also conduct its evaluation on three different lexical semantic resources for",
      "cosine similarity to the sentence vector:   where  is the set of words forming the synset,  is the set of words forming the sentence. On initialization, the synsets represented in the sense inventory are transformed into the  -weighted word-synset sparse matrix efficiently represented in the memory using the compressed sparse row format. Given a sentence, a similar transformation is done to"
    ]
  },
  {
    "title": "Self-Attentional Models Application in Task-Oriented Dialogue Generation Systems",
    "answer": "The three datasets used are: \nDSTC2, M2M-sim-M, and M2M-sim-R.",
    "evidence": [
      "Experiments\nDatasets",
      "Experiments\nDatasets\nDataset Preparation",
      "via crowdsourcing. We trained on our models on different datasets in order to make sure the results are not corpus-biased. Table  shows the statistics of these three datasets which we will use to train and evaluate the models. The M2M dataset has more diversity in both language and dialogue flow compared to the the commonly used DSTC2 dataset which makes it appealing for the task of creating",
      "We use three different datasets for training the models. We use the Dialogue State Tracking Competition 2 (DSTC2) dataset BIBREF27 which is the most widely used dataset for research on task-oriented chatbots. We also used two other datasets recently open-sourced by Google Research BIBREF28 which are M2M-sim-M (dataset in movie domain) and M2M-sim-R (dataset in restaurant domain). M2M stands for"
    ]
  },
  {
    "title": "\"How May I Help You?\": Modeling Twitter Customer Service Conversations Using Fine-Grained Dialogue Acts",
    "answer": "fine-grained dialogue acts",
    "evidence": [
      "approach for act prediction, as well as our analysis of the customer service domain on Twitter. Our goal is to offer useful analytics to improve outcome-oriented conversational systems. We first expand upon previous work and generic dialogue act taxonomies, developing a fine-grained set of dialogue acts for customer service, and conducting a systematic user study to identify these acts in a",
      "real-time about the flow of a conversation to derive meaningful insights into the success/failure of the interaction, and then to develop actionable rules to be used in automating customer service interactions. We focus on the customer service domain on Twitter, which has not previously been explored in the context of dialogue act classification. In this new domain, we can provide meaningful",
      "to understanding the nature of interactions on Twitter, we find that the customer service domain presents its own interesting characteristics that are worth exploring further. The most related previous work has explored speech and dialogue act modeling in customer service, however, no previous work has focused on Twitter as a data source. In 2005, Ivanovic uses an abridged set of 12",
      "between a human agent and customer on Twitter, where the customer and agent take alternating \"turns\" to discuss the problem. As shown from the dialogue acts used at each turn, simply knowing that a turn is a Statement or Request, as is possible with generic taxonomies, is not enough information to allow for automated handling or response to a problem. We need more fine-grained dialogue acts, such",
      ". For the purposes of our own research, we require a set of dialogue acts that is more closely representative of customer service domain interactions - thus we expand upon previously defined taxonomies and develop a more fine-grained set. Modeling general conversation on Twitter has also been a topic of interest in previous work. Honeycutt and Herring study conversation and collaboration on",
      "of 5 judgements). It is important to note at this point that we make an important choice as to how we will handle dialogue act tagging for each turn. We note that each turn may contain more than one dialogue act vital to carry its full meaning. Thus, we choose not to carry out a specific segmentation task on our tweets, contrary to previous work BIBREF24 , BIBREF25 , opting to characterize each",
      "acts\"frequently observed in customer service, showcasing acts that are more suited to the domain than the more generic existing taxonomies. Using a sequential SVM-HMM model, we model conversation flow, predicting the dialogue act of a given turn in real-time. We characterize differences between customer and agent behavior in Twitter customer service conversations, and investigate the effect of",
      "data is not available for customer service on Twitter, the task of dialogue act annotation is subjective, existing taxonomies do not capture the fine-grained information we believe is valuable to our task, and tweets, although concise in nature, often consist of overlapping dialogue acts to characterize their full intent. The novelty of our work comes from the development of our fine-grained",
      "the fine-grained information we believe is valuable to our task, and tweets, although concise in nature, often consist of overlapping dialogue acts to characterize their full intent. The novelty of our work comes from the development of our fine-grained dialogue act taxonomy and multi-label approach for act prediction, as well as our analysis of the customer service domain on Twitter. Our goal is",
      "Twitter conversations to define a model of transitional flow between in a general Twitter dialogue BIBREF17 . While these approaches are relevant to understanding the nature of interactions on Twitter, we find that the customer service domain presents its own interesting characteristics that are worth exploring further. The most related previous work has explored speech and dialogue act modeling",
      "rules to be used in automating customer service interactions. We focus on the customer service domain on Twitter, which has not previously been explored in the context of dialogue act classification. In this new domain, we can provide meaningful recommendations about good communicative practices, based on real data. Our methodology pipeline is shown in Figure  .",
      "the customer service domain, and focus on Twitter conversations, which are unique in their brevity and the nature of the public interactions. The most similar work to our own is that of Herzig et al. on classifying emotions in customer support dialogues on Twitter BIBREF23 . They explore how agent responses should be tailored to the detected emotional response in customers, in order to improve",
      "to characterize their full intent. The novelty of our work comes from the development of our fine-grained dialogue act taxonomy and multi-label approach for act prediction, as well as our analysis of the customer service domain on Twitter. Our goal is to offer useful analytics to improve outcome-oriented conversational systems. We first expand upon previous work and generic dialogue act",
      "where they focus on classifying dialogue acts in both one-on-one and multi-party live instant messaging chats BIBREF21 , BIBREF22 . These works are similar to ours in the nature of the problem addressed, but we use a much more fine-grained taxonomy to define the interactions possible in the customer service domain, and focus on Twitter conversations, which are unique in their brevity and the",
      "In this paper, we explore how we can analyze dialogic trends in customer service conversations on Twitter to offer insight into good/bad practices with respect to conversation outcomes. We design a novel taxonomy of fine-grained dialogue acts, tailored for the customer service domain, and gather annotations for 800 Twitter conversations. We show that dialogue acts are often semantically",
      "we are interested in the dialogic characteristics of Twitter conversations, rather than speech acts in stand-alone tweets. Different dialogue act taxonomies have been developed to characterize conversational acts. Core and Allen present the Dialogue Act Marking in Several Layers (DAMSL), a standard for discourse annotation that was developed in 1997 BIBREF0 . The taxonomy contains a total of 220",
      "Given the increasing popularity of customer service dialogue on Twitter, analysis of conversation data is essential to understand trends in customer and agent behavior for the purpose of automating customer service interactions. In this work, we develop a novel taxonomy of fine-grained\"dialogue acts\"frequently observed in customer service, showcasing acts that are more suited to the domain than",
      "Social Act Downplayer, Statement Promise, Greeting Closing, and Request Other. It is also interesting to note that both opening and closing greetings occur infrequently in the data – which is understandable given the nature of Twitter conversation, where formal greeting is not generally required. Table  shows a more detailed summary of the distribution of our top 12 dialogue acts according to the",
      "practices with respect to conversation outcomes. We design a novel taxonomy of fine-grained dialogue acts, tailored for the customer service domain, and gather annotations for 800 Twitter conversations. We show that dialogue acts are often semantically overlapping, and conduct multi-label supervised learning experiments to predict multiple appropriate dialogue act labels for each turn in",
      "good features for speech act classification and the application of such a system to detect stories on social media BIBREF11 . In this work, we are interested in the dialogic characteristics of Twitter conversations, rather than speech acts in stand-alone tweets. Different dialogue act taxonomies have been developed to characterize conversational acts. Core and Allen present the Dialogue Act",
      "domain, and gather annotations for 800 Twitter conversations. We show that dialogue acts are often semantically overlapping, and conduct multi-label supervised learning experiments to predict multiple appropriate dialogue act labels for each turn in real-time, under varying class sizes. We show that our sequential SVM-HMM model outperforms all non-sequential baselines, and plan to continue our",
      "customer service conversations as a first step to fully automating the interaction. We address various different challenges: dialogue act annotated data is not available for customer service on Twitter, the task of dialogue act annotation is subjective, existing taxonomies do not capture the fine-grained information we believe is valuable to our task, and tweets, although concise in nature, often",
      "turn. We note that each turn may contain more than one dialogue act vital to carry its full meaning. Thus, we choose not to carry out a specific segmentation task on our tweets, contrary to previous work BIBREF24 , BIBREF25 , opting to characterize each tweet as a single unit composed of different, often overlapping, dialogue acts. Table  shows examples of tweets that receive majority vote on",
      "that are worth exploring further. The most related previous work has explored speech and dialogue act modeling in customer service, however, no previous work has focused on Twitter as a data source. In 2005, Ivanovic uses an abridged set of 12 course-grained dialogue acts (detailed in the Taxonomy section) to describe interactions between customers and agents in instant messaging chats BIBREF18 ,",
      "of dialogue acts for customer service, and conducting a systematic user study to identify these acts in a dataset of 800 conversations from four Twitter customer service accounts (i.e. four different companies in the telecommunication, electronics, and insurance industries). We then aim to understand the conversation flow between customers and agents using our taxonomy, so we develop a real-time"
    ]
  },
  {
    "title": "Sex Trafficking Detection with Ordinal Regression Neural Networks",
    "answer": "by re-training the skip-gram model and updating the emoji map periodically on new escort ads, and linking new emojis to old ones.",
    "evidence": [
      "that they are all used in similar contexts and perhaps should all be flags for underaged victims in the updated lexicon. If we re-train the skip-gram model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link the new emojis to the old ones, assisting anti-trafficking experts in expanding the lexicon of trafficking flags. This approach"
    ]
  },
  {
    "title": "MDE: Multi Distance Embeddings for Link Prediction in Knowledge Graphs",
    "answer": "WN18, FB15k, FB15k-237.",
    "evidence": [
      "datasets are from BIBREF11 , Table 13 for TransE and RotatE and the rest are from BIBREF35 . Evaluation Settings: We evaluate the link prediction performance by ranking the score of each test triple against its versions with replaced head, and once for tail. Then we compute the hit at N (hit@N), mean rank(MR) and mean reciprocal rank (MRR) of these rankings. MR is a more robust measure than MRR",
      "per positive example for all the datasets. We used Adadelta BIBREF30 as the optimizer and fine-tuned the hyperparameters on the validation dataset. The ranges of the hyperparameters are set as follows: embedding dimension 25, 50, 100, batch size 100, 150, iterations 1000, 1500, 2500, 3600. We set the initial learning rate on all datasets to 10. The best embedding size and $\\gamma _1$ and $\\gamma",
      "possible combinations of object entities to generate samples(e.g., we observed in each iteration, it generates $\\approx 26000$ negative samples per one positive sample when training on WN18RR). ComplEx and SimplE also generate 10 negative samples per one positive sample on FB15K. Here, we use 1 negative per positive, for MDE. To show the effect of dataset construction we compare this models with",
      "of the dataset construction rather than the relational learning models. This is considerable when comparing results to ConvE that generates all possible combinations of object entities to generate samples(e.g., we observed in each iteration, it generates $\\approx 26000$ negative samples per one positive sample when training on WN18RR). ComplEx and SimplE also generate 10 negative samples per one",
      "good results can influence the overall score. Implementation: We implemented MDE in PyTorch. Following BIBREF18 , we generated one negative example per positive example for all the datasets. We used Adadelta BIBREF30 as the optimizer and fine-tuned the hyperparameters on the validation dataset. The ranges of the hyperparameters are set as follows: embedding dimension 25, 50, 100, batch size 100,",
      "we use 1 negative per positive, for MDE. To show the effect of dataset construction we compare this models with an experiment of TransE and RotatE on FB15k-237 that applies 256 negative samples per one positive sample BIBREF11 . Although these models perform better than the TransE on FB15K(Table1), they produce lower rankings on FB15k-237(Table2) in the more fair comparison conditions.",
      "and ComplEx from BIBREF13 and the results of TransR and NTN from BIBREF36 , and ER-MLP from BIBREF15 . The results on the inverse relation excluded datasets are from BIBREF11 , Table 13 for TransE and RotatE and the rest are from BIBREF35 . Evaluation Settings: We evaluate the link prediction performance by ranking the score of each test triple against its versions with replaced head, and once",
      "methods. Comparison of MDE and TransE and other distance based models confirms the improved ability of MDE in learning different patterns. Negative Sampling and Data Augmentation Models: Currently, the training datasets for link prediction evaluation miss negative samples. Therefore, models generate their own negative samples while training. In consequence, the method of negative sample",
      "Datasets: We experimented on four standard datasets: WN18 and FB15k are extracted by BIBREF5 from Wordnet BIBREF32 Freebase BIBREF33 . We used the same train/valid/test sets as in BIBREF5 . WN18 contains 40,943 entities, 18 relations and 141,442 train triples. FB15k contains 14,951 entities, 1,345 relations and 483,142 train triples. In order to test the expressiveness ability rather than"
    ]
  },
  {
    "title": "Gunrock: A Social Bot for Complex and Engaging Long Conversations",
    "answer": "They found that users' backstory queries about Gunrock are positively correlated to user satisfaction.",
    "evidence": [
      "related to users' engagement (e.g., ratings, number of turns). Additionally, users' backstory queries about Gunrock are positively correlated to user satisfaction. Finally, we found dialog flows that interleave facts and personal opinions and stories lead to better user satisfaction.",
      "For users with at least one backstory question, we modeled overall (log) Rating with a linear regression by the (log) `Number of Backstory Questions Asked' (log transformed due to the variables' nonlinear relationship). We hypothesized that users who show greater curiosity about Gunrock will display higher overall ratings for the conversation based on her responses. Overall, the number of times"
    ]
  },
  {
    "title": "Image Captioning: Transforming Objects into Words",
    "answer": "CIDEr-D, SPICE, BLEU, METEOR, and ROUGE-L.",
    "evidence": [
      "the CIDEr-D BIBREF22 , SPICE BIBREF23 , BLEU BIBREF24 , METEOR BIBREF25 , and ROUGE-L BIBREF26 metrics. While it has been shown experimentally that BLEU and ROUGE have lower correlation with human judgments than the other metrics BIBREF23 , BIBREF22 , the common practice in the image captioning literature is to report all the mentioned metrics."
    ]
  },
  {
    "title": "RC-QED: Evaluating Natural Language Derivations in Multi-Hop Reading Comprehension",
    "answer": "The dataset was annotated using a scalable crowdsourcing framework, specifically through Amazon Mechanical Turk (AMT), where 36,000 annotation tasks were published.",
    "evidence": [
      "There exists few RC datasets annotated with explanations (Table ). The most similar work to ours is Science QA dataset BIBREF21, BIBREF22, BIBREF23, which provides a small set of NLDs annotated for analysis purposes. By developing the scalable crowdsourcing framework, our work provides one order-of-magnitude larger NLDs which can be used as a benchmark more reliably. In addition, it provides the",
      "Our study uses WikiHop BIBREF0, as it is an entity-based multi-hop QA dataset and has been actively used. We randomly sampled 10,000 instances from 43,738 training instances and 2,000 instances from 5,129 validation instances (i.e. 36,000 annotation tasks were published on AMT). We manually converted structured WikiHop question-answer pairs (e.g. locatedIn(Macchu Picchu, Peru)) into natural",
      "new challenges for NLU. Several datasets are annotated with introspective explanations, ranging from textual entailments BIBREF8 to argumentative texts BIBREF26, BIBREF27, BIBREF33. All these datasets offer the classification task of single sentences or sentence pairs. The uniqueness of our dataset is that it measures a machine's ability to extract relevant information from a set of documents and",
      "The developed crowdsourcing annotation framework can be used for annotating other QA datasets with derivations. Our experiments using two simple baseline models have demonstrated that RC-QED$^{\\rm E}$ is a non-trivial task, and that it indeed provides a challenging task of extracting and synthesizing relevant facts from supporting documents. We will make the corpus of reasoning annotations and",
      "state that a statement is Unsure, then we set the answerability to Unanswerable and discard NLD annotations. Otherwise, we employ all NLD annotations from workers as multiple reference NLDs. The statistics is shown in Table . Regarding $\\mathcal {K}^+, \\mathcal {K}^-$, we extracted 867,936 instances from the training set of WikiHop BIBREF0. We reserve 10% of these instances as a validation set to"
    ]
  },
  {
    "title": "Word Embeddings to Enhance Twitter Gang Member Profile Identification",
    "answer": "a collection process involving location neutral keywords used by gang members, with an expanded search of their retweet, friends and follower networks.",
    "evidence": [
      "when provided with large datasets. A dataset of over 3,000 gang and non-gang member profiles that we previously curated is used to train the word embeddings. We show that pre-trained word embeddings improve the machine learning models and help us obtain an  -score of  on gang member profiles (a 6.39% improvement in  -score compared to the baseline models which were not trained using word",
      "We consider a dataset of curated gang and non-gang members' Twitter profiles collected from our previous work BIBREF9 . It was developed by querying the Followerwonk Web service API with location-neutral seed words known to be used by gang members across the U.S. in their Twitter profiles. The dataset was further expanded by examining the friends, follower, and retweet networks of the gang member",
      "The dataset was further expanded by examining the friends, follower, and retweet networks of the gang member profiles found by searching for seed words. Specific details about our data curation procedure are discussed in BIBREF9 . Ultimately, this dataset consists of 400 gang member profiles and 2,865 non-gang member profiles. For each user profile, we collected up to most recent 3,200 tweets",
      "and data collection was localized to a single city in the country. These studies investigated a small set of manually curated gang member profiles, often from a small geographic area that may bias their findings. In our previous work BIBREF9 , we curated what may be the largest set of gang member profiles to study how gang member Twitter profiles can be automatically identified based on the",
      "a Twitter user dataset that consist of 400 gang member and 2,865 non gang member profiles, we trained word embedding models based on users' tweets, profile descriptions, emoji, images, and videos shared on Twitter (textual features extracted from images, and videos). We then use the pre-trained word embedding models to train supervised machine learning classifiers, which showed superior",
      "This paper presented a word embeddings-based approach to address the problem of automatically identifying gang member profiles on Twitter. Using a Twitter user dataset that consist of 400 gang member and 2,865 non gang member profiles, we trained word embedding models based on users' tweets, profile descriptions, emoji, images, and videos shared on Twitter (textual features extracted from images,",
      "led to identifying 400 authentic gang member profiles on Twitter. Our study discovered that the text in their tweets and profile descriptions, their emoji use, their profile images, and music interests embodied by links to YouTube music videos, can help a classifier distinguish between gang and non-gang member profiles. While a very promising  measure with low false positive rate was achieved, we",
      "profile description text, profile and cover images, and the comments and video descriptions for every YouTube video shared by them. Table 1 provides statistics about the number of words found in each type of feature in the dataset. It includes a total of 821,412 tweets from gang members and 7,238,758 tweets from non-gang members. To build the classifiers we used three different learning",
      "1 provides statistics about the number of words found in each type of feature in the dataset. It includes a total of 821,412 tweets from gang members and 7,238,758 tweets from non-gang members. To build the classifiers we used three different learning algorithms, namely Logistic Regression (LR), Random Forest (RF), and Support Vector Machines (SVM). We used version 0.17.1 of scikit-learn machine",
      "seed words. Specific details about our data curation procedure are discussed in BIBREF9 . Ultimately, this dataset consists of 400 gang member profiles and 2,865 non-gang member profiles. For each user profile, we collected up to most recent 3,200 tweets from their Twitter timelines, profile description text, profile and cover images, and the comments and video descriptions for every YouTube",
      "gang and non-gang member profiles. While a very promising  measure with low false positive rate was achieved, we hypothesize that the diverse kinds and the multitude of features employed (e.g. unigrams of tweet text) could be amenable to an improved representation for classification. We thus explore the possibility of mapping these features into a considerably smaller feature space through the",
      "Evaluation\nWe evaluate the performance of using word embeddings to discover gang member profiles on Twitter. We first discuss the dataset, learning algorithms and baseline comparison models used in the experiments. Then we discuss the 10-fold cross validation experiments and the evaluation matrices used. Finally we present the results of the experiments.",
      "of features in the training dataset as described in BIBREF9 . The baseline Model(1) uses all profiles in the dataset and has a  -score of 0.7364 for `gang' class and 0.9690 for `non-gang' class. The baseline Model(2) which only uses profiles that contain each and every feature type has a  -score of 0.7755 for `gang' class and  -score of 0.9720 for `non-gang' class. Vector sum is one of the basic",
      "profiles (a 6.39% improvement in  -score compared to the baseline models which were not trained using word embeddings). This paper is organized as follows. Section  discusses the related literature and frames how this work differs from other related works. Section  discusses our approach based on word embeddings to identify gang member profiles. Section  reports on the evaluation of the proposed",
      "metrics for the `gang' (positive) and `non-gang' (negative) classes, namely, the Precision =  , Recall =  , and  -score =  , where  is the number of true positives,  is the number of false positives,  is the number of true negatives, and  is the number of false negatives. We report these metrics for the `gang' and `non-gang' classes separately because of the class imbalance in the dataset.",
      "emoji use, their profile images, and music interests embodied by links to YouTube music videos, can help a classifier distinguish between gang and non-gang member profiles. While a very promising  measure with low false positive rate was achieved, we hypothesize that the diverse kinds and the multitude of features employed (e.g. unigrams of tweet text) could be amenable to an improved",
      "collection process involving location neutral keywords used by gang members, with an expanded search of their retweet, friends and follower networks, led to identifying 400 authentic gang member profiles on Twitter. Our study discovered that the text in their tweets and profile descriptions, their emoji use, their profile images, and music interests embodied by links to YouTube music videos, can",
      "in their profile descriptions, tweets, profile images, and linked YouTube content to a real vector format amenable for machine learning classification. Our experimental results show that pre-trained word embeddings can boost the accuracy of supervised learning algorithms trained over gang members social media posts.",
      "with automatically building dictionaries that contain gang names and gang-related slang using crowd-sourced gang-related knowledge-bases such as HipWiki. We also want to experiment with using such knowledge-bases to train word embeddings to understand whether having access to gang-related knowledge could boost the performance of our models. Finally, we would like to study how we can further use",
      "baseline models reported in the literature. We plan to further extend our work by building our own image classification system specifically designed to identify images commonly shared by gang members such as guns, gang hand signs, stacks of cash and drugs. We would also like to experiment with automatically building dictionaries that contain gang names and gang-related slang using crowd-sourced",
      "Random Forest (RF), and Support Vector Machines (SVM). We used version 0.17.1 of scikit-learn machine learning library for Python to implement the classifiers. An open source tool of Python, Gensim BIBREF19 was used to generate the word embeddings. We compare our results with the two best performing systems reported in BIBREF9 which are the two state-of-the-art models for identifying gang members",
      "to gangs operating in neighborhoods and change rapidly when they form new gangs. Consequently, building a database of keywords, phrases, and other identifiers to find gang members nationally is not feasible. Instead, we use heterogeneous sets of features derived not only from profile and tweet text but also from the emoji usage, profile images, and links to YouTube videos reflecting their music",
      "embeddings. Tags such as trigger, bullet, and worship were unique for gang member profiles while non-gang member images had unique tags such as beach, seashore, dawn, wildlife, sand, and pet. YouTube videos: We found that 51.25% of the gang members in our dataset have a tweet that links to a YouTube video. Further, we found that 76.58% of the shared links are related to hip-hop music, gangster",
      "to identify images commonly shared by gang members such as guns, gang hand signs, stacks of cash and drugs. We would also like to experiment with automatically building dictionaries that contain gang names and gang-related slang using crowd-sourced gang-related knowledge-bases such as HipWiki. We also want to experiment with using such knowledge-bases to train word embeddings to understand",
      "may be used to reliably find gang member profiles BIBREF9 . The diverse set of features, chosen to combat the fact that gang members often use local terms and hashtags in their posts, offered encouraging results. In this paper, we report our experience in integrating deep learning into our gang member profile classifier. Specifically, we investigate the effect of translating the features into a"
    ]
  },
  {
    "title": "Recurrently Controlled Recurrent Networks",
    "answer": "No, it has approximately equal parameterization.",
    "evidence": [
      "most of which are models that still heavily rely on bidirectional LSTMs BIBREF27 , BIBREF20 , BIBREF5 , BIBREF10 . While self-attentive RNN-less encoders have recently been popular, our work moves in an orthogonal and possibly complementary direction, advocating a stronger RNN unit for sequence encoding instead. Nevertheless, it is also good to note that our RCRN model outperforms DiSAN in all",
      "of approximately equal parameterization. There are several potential interesting directions for further investigating RCRNs. Firstly, investigating RCRNs controlling other RCRNs and secondly, investigating RCRNs in other domains where recurrent models are also prevalent for sequence modeling. The source code of our model can be found at https://github.com/vanzytay/NIPS2018_RCRN.",
      "+ b1f) and f2t = s(W2fxt + U2fh2t-1 + b2f) o1t = s(W1oxt + U1oh1t-1 + b1o) and o2t = s(W2oxt + U2oh2t-1 + b2o) c1t = f1t c1t-1 + i1t (W1cxt + U1ch1t-1 + b1c) c2t = f2t c2t-1 + i2t (W2cxt + U2ch2t-1 + b2c) h1t = o1t (c1t) and h2t = o2t (c2t) where  is the input to the model at time step  .  are the parameters of the model where  and  .  is the sigmoid function and  is the tanh nonlinearity.  is",
      "architecture is more effective than stacking RNN layers. Moreover, RCRN remains equally (or slightly more) efficient compared to stacked RNNs of approximately equal parameterization. There are several potential interesting directions for further investigating RCRNs. Firstly, investigating RCRNs controlling other RCRNs and secondly, investigating RCRNs in other domains where recurrent models are",
      "to our model architecture is that it is essentially a `recurrent-over-recurrent' model. Clearly, the formulation we have used above uses BiLSTMs as the atomic building block for RCRN. Hence, we note that it is also possible to have a simplified variant of RCRN that uses GRUs as the atomic block which we found to have performed slightly better on certain datasets. For efficiency purposes, we use",
      "a wide range of baselines, including self-attention based models as multi-head BIBREF24 and DiSAN BIBREF25 . There is also performance gain of  over Bi-SRU even though our model does not use attention at all. RCRN also outperforms shortcut stacked encoders, which use a series of BiLSTM connected by shortcut layers. Post review, as per reviewer request, we experimented with adding cross sentence",
      "We use the same standard hardware (a single Nvidia GTX1070 card) and an identical overarching model architecture. The dimensionality of the model is set to 200 with a fixed batch size of 32. Finally, we also benchmark a CUDA optimized adaptation of RCRN which has been described earlier (Section  ). Table  reports training/inference times of all benchmarked models. The fastest model is naturally",
      "encoders have recently been popular, our work moves in an orthogonal and possibly complementary direction, advocating a stronger RNN unit for sequence encoding instead. Nevertheless, it is also good to note that our RCRN model outperforms DiSAN in all our experiments. Another line of work is also concerned with eliminating recurrence. SRUs (Simple Recurrent Units) BIBREF28 are recently proposed",
      "RCRN also outperforms ablative baselines BiLSTM (  ) and 3L-BiLSTM (  ). Our results on the TREC question classification dataset (Table  ) is also promising. RCRN achieved a state-of-the-art score of  on this dataset. A notable baseline is the Densely Connected BiLSTM BIBREF23 , a deep residual stacked BiLSTM model which RCRN outperforms (  ). Our model also outperforms BCN (+0.4%) and SRU (  ).",
      "achieve reasonably high score, posssibly due to CoVe Embeddings. However, our RCRN can further increase the performance score. Results on entailment classification are also optimistic. On SNLI (Table  ), RCRN achieves  accuracy, which is competitive to Gumbel LSTM. However, RCRN outperforms a wide range of baselines, including self-attention based models as multi-head BIBREF24 and DiSAN BIBREF25",
      "has been a popular choice. This has sparked many innovations, including general purpose encoders such as DiSAN BIBREF25 and Block Bi-DiSAN BIBREF26 . The key idea in these works is to use multi-headed self-attention and positional encodings to model temporal information. While attention-only models may come close in performance, some domains may still require the complex and expressive recurrent",
      "In this section, we describe the task-specific model architectures for each task. This architecture is used for all text classification tasks (sentiment analysis and question classification datasets). We use 300D GloVe BIBREF42 vectors with 600D CoVe BIBREF5 vectors as pretrained embedding vectors. An optional character-level word representation is also added (constructed with a standard BiGRU",
      ". The key idea in these works is to use multi-headed self-attention and positional encodings to model temporal information. While attention-only models may come close in performance, some domains may still require the complex and expressive recurrent encoders. Moreover, we note that in BIBREF25 , BIBREF26 , the scores on multiple benchmarks (e.g., SST, TREC, SNLI, MultiNLI) do not outperform (or",
      "in BIBREF25 , BIBREF26 , the scores on multiple benchmarks (e.g., SST, TREC, SNLI, MultiNLI) do not outperform (or even approach) the state-of-the-art, most of which are models that still heavily rely on bidirectional LSTMs BIBREF27 , BIBREF20 , BIBREF5 , BIBREF10 . While self-attentive RNN-less encoders have recently been popular, our work moves in an orthogonal and possibly complementary",
      "Across all 26 datasets, RCRN outperforms not only standard BiLSTMs but also 3L-BiLSTMs which have approximately equal parameterization. 3L-BiLSTMs were overall better than BiLSTMs but lose out on a minority of datasets. RCRN outperforms a wide range of competitive baselines such as DiSAN, Bi-SRUs, BCN and LSTM-CNN, etc. We achieve (close to) state-of-the-art performance on SST, TREC question",
      "RCRNs perform similarly. The results on SciTail similarly show that RCRN is more effective than BiLSTM (  ). Moreover, RCRN outperforms several baselines in BIBREF37 including models that use cross sentence attention such as DecompAtt BIBREF61 and ESIM BIBREF13 . However, it still falls short to recent state-of-the-art models such as OpenAI's Generative Pretrained Transformer BIBREF64 . Results",
      "models may come close in performance, some domains may still require the complex and expressive recurrent encoders. Moreover, we note that in BIBREF25 , BIBREF26 , the scores on multiple benchmarks (e.g., SST, TREC, SNLI, MultiNLI) do not outperform (or even approach) the state-of-the-art, most of which are models that still heavily rely on bidirectional LSTMs BIBREF27 , BIBREF20 , BIBREF5 ,",
      "tasks and achieve promising/highly competitive results on all tasks and 26 benchmark datasets. Overall findings suggest that our controller-listener architecture is more effective than stacking RNN layers. Moreover, RCRN remains equally (or slightly more) efficient compared to stacked RNNs of approximately equal parameterization. There are several potential interesting directions for further",
      "BIBREF14 , 4-layered Quasi Recurrent Neural Networks (QRNN) BIBREF29 and the BCN model which can be considered to be very competitive baselines. RCRN also outperforms ablative baselines BiLSTM (  ) and 3L-BiLSTM (  ). Our results on the TREC question classification dataset (Table  ) is also promising. RCRN achieved a state-of-the-art score of  on this dataset. A notable baseline is the Densely",
      "RCRN outperforms ablative baselines BiLSTM by  and 3L-BiLSTM by  on average across 16 datasets. Results on SST-5 (Table  ) and SST-2 (Table  ) are also promising. More concretely, our RCRN architecture achieves state-of-the-art results on SST-5 and SST-2. RCRN also outperforms many strong baselines such as DiSAN BIBREF25 , a self-attentive model and Bi-Attentive classification network (BCN)",
      "(sentiment analysis and question classification datasets). We use 300D GloVe BIBREF42 vectors with 600D CoVe BIBREF5 vectors as pretrained embedding vectors. An optional character-level word representation is also added (constructed with a standard BiGRU model). The output of the embedding layer is passed into the RCRN model directly without using any projection layer. Word embeddings are not",
      "classification are also optimistic. On SNLI (Table  ), RCRN achieves  accuracy, which is competitive to Gumbel LSTM. However, RCRN outperforms a wide range of baselines, including self-attention based models as multi-head BIBREF24 and DiSAN BIBREF25 . There is also performance gain of  over Bi-SRU even though our model does not use attention at all. RCRN also outperforms shortcut stacked",
      "deep residual stacked BiLSTM model which RCRN outperforms (  ). Our model also outperforms BCN (+0.4%) and SRU (  ). Our ablative BiLSTM baselines achieve reasonably high score, posssibly due to CoVe Embeddings. However, our RCRN can further increase the performance score. Results on entailment classification are also optimistic. On SNLI (Table  ), RCRN achieves  accuracy, which is competitive to",
      "while R-NET uses 3 layered BiGRUs. This empirical evidence might suggest that RCRN is a better way to utilize multiple recurrent layers. Across all 26 datasets, RCRN outperforms not only standard BiLSTMs but also 3L-BiLSTMs which have approximately equal parameterization. 3L-BiLSTMs were overall better than BiLSTMs but lose out on a minority of datasets. RCRN outperforms a wide range of",
      "baselines in BIBREF37 including models that use cross sentence attention such as DecompAtt BIBREF61 and ESIM BIBREF13 . However, it still falls short to recent state-of-the-art models such as OpenAI's Generative Pretrained Transformer BIBREF64 . Results on the answer selection (Table  ) task show that RCRN leads to considerable improvements on both WikiQA and TrecQA datasets. We investigate two"
    ]
  },
  {
    "title": "Contextual Recurrent Units for Cloze-style Reading Comprehension",
    "answer": "MR, IMDB, CBT NE/CN.",
    "evidence": [
      "Experiments: Sentiment Classification\nExperimental Setups",
      "classification and cloze-style reading comprehension. In the sentiment classification task, we build a simple neural model and applied our CRU. In the cloze-style reading comprehension task, we first present some modifications to a recent reading comprehension model, called AoA Reader BIBREF10, and then replace the GRU part by our CRU model to see if our model could give substantial improvements",
      "the cloze-style reading comprehension task and sentiment classification task. Experimental results show that our model could give substantial improvements over various state-of-the-art systems and set up new records on the respective public datasets. In the future, we plan to investigate convolutional filters that have dynamic lengths to adaptively capture the possible spans of its context.",
      "modeling, and the latter is document-level modeling. In the sentiment classification task, we build a standard neural network and replace the recurrent unit by our CRU model. To further demonstrate the effectiveness of our model, we also tested our CRU in reading comprehension tasks with a strengthened baseline system originated from Attention-over-Attention Reader (AoA Reader) BIBREF10.",
      "Experiments: Sentiment Classification\nResults",
      "of our CRU model, we utilize it into two different NLP tasks: sentiment classification and reading comprehension, where the former is sentence-level modeling, and the latter is document-level modeling. In the sentiment classification task, we build a standard neural network and replace the recurrent unit by our CRU model. To further demonstrate the effectiveness of our model, we also tested our",
      "Applications\nReading Comprehension\nBesides the sentiment classification task, we also tried our CRU model in cloze-style reading comprehension, which is a much complicated task. In this paper, we strengthened the recent AoA Reader BIBREF10 and applied our CRU model to see if we could obtain substantial improvements when the baseline is strengthened.",
      "give improvements in both sentence-level modeling and document-level modeling tasks, in this paper, we applied the CRU model to two NLP tasks: sentiment classification and cloze-style reading comprehension. In the sentiment classification task, we build a simple neural model and applied our CRU. In the cloze-style reading comprehension task, we first present some modifications to a recent reading",
      "GRU, which aims to better model the local context information via CNN before recurrently modeling the sequence. We have tested our CRU model on the cloze-style reading comprehension task and sentiment classification task. Experimental results show that our model could give substantial improvements over various state-of-the-art systems and set up new records on the respective public datasets. In",
      "In the sentiment classification task, we aim to classify movie reviews, where one movie review will be classified into the positive/negative or subjective/objective category. A general neural network architecture for this task is depicted in Figure . First, the movie review is transformed into word embeddings. And then, a sequence modeling module is applied, in which we can adopt LSTM, GRU, or",
      "Applications\nSentiment Classification",
      "In the sentiment classification task, we tried our model on the following public datasets. [leftmargin=*] MR Movie reviews with one sentence each. Each review is classified into positive or negative BIBREF18. IMDB Movie reviews from IMDB website, where each movie review is labeled with binary classes, either positive or negative BIBREF19. Note that each movie review may contain several sentences.",
      "long-term dependencies. We propose three variants of our CRU model: shallow fusion, deep fusion and deep-enhanced fusion. To verify the effectiveness of our CRU model, we utilize it into two different NLP tasks: sentiment classification and reading comprehension, where the former is sentence-level modeling, and the latter is document-level modeling. In the sentiment classification task, we build",
      "The proposed CRU model is a general neural recurrent unit, so we could apply it to various NLP tasks. As we wonder whether the CRU model could give improvements in both sentence-level modeling and document-level modeling tasks, in this paper, we applied the CRU model to two NLP tasks: sentiment classification and cloze-style reading comprehension. In the sentiment classification task, we build a",
      "Experiments: Reading Comprehension\nResults",
      "Experiments: Reading Comprehension\nExperimental Setups",
      "methods. In NLP tasks, two types of neural networks are widely used: Recurrent Neural Network (RNN) and Convolutional Neural Network (CNN). RNNs are powerful models in various NLP tasks, such as machine translation BIBREF0, sentiment classification BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, reading comprehension BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11, etc. The recurrent neural",
      "of the movie reviews, which is quite harder for the model to judge the correct sentiment. The first one is the movie review that contains negation terms, such as “not”. The second type is the one contains sentiment transition, such as “clever but not compelling”. We manually select 50 samples of each category in the MR dataset, forming a total of 100 samples to see if our CRU model is superior in",
      "We also tested our CRU model in the cloze-style reading comprehension task. We carried out experiments on the public datasets: CBT NE/CN BIBREF25. The CRU model used in these experiments is the deep-enhanced type with the convolutional filter length of 3. In the re-ranking step, we also utilized three features: Global LM, Local LM, Word-class LM, as proposed by BIBREF10, and all LMs are 8-gram"
    ]
  },
  {
    "title": "Saliency Maps Generation for Automatic Text Summarization",
    "answer": "***See et al. (2017)*** \n\n(or alternatively, in a more detailed phrase)\n***the baseline model from See et al. (2017) trained on the CNN/Daily Mail dataset***.",
    "evidence": [
      "The Task and the Model\nWe present in this section the baseline model from See et al. See2017 trained on the CNN/Daily Mail dataset. We reproduce the results from See et al. See2017 to then apply LRP on it.",
      "at all. We did however also try with different methods, like averaging the raw relevance or averaging a scaled absolute value where negative relevance is scaled down by a constant factor. The absolute value average seemed to deliver the best results. We delete incrementally the important words (words with the highest average) in the input and compared it to the control experiment that consists of",
      "Obtained Summaries",
      "and compare the degradation of the resulting summaries. We obtain mitigated results: for some texts, we observe a quick degradation when deleting important words which are not observed when deleting unimportant words (see Figure 4 ), but for other test examples we don't observe a significant difference between the two settings (see Figure 5 ). One might argue that the second summary in Figure 5"
    ]
  },
  {
    "title": "Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models",
    "answer": "They select answer candidates such that they share the same (concept $_1$, relation) or (relation, concept $_2$).",
    "evidence": [
      "candidate answers, we mask concept $_1$ and concept $_2$ and pre-train BERT with a masked language model (MLM) task. We denote the resulting model from this pre-training task BERT_MLM. We randomly mask 15% WordPiece tokens BIBREF27 of the question as in BIBREF4 and then conduct both multi-choice QA task and MLM task simultaneously. The resulting model is denoted BERT_CS_MLM. All these BERT models",
      "We mask the pronoun word with a special token [QW] to construct a question, and put the two candidate paragraphs as candidate answers. The remaining procedures are the same as QA tasks. We use the same loss function as BIBREF22 , that is, if c $_1$ is correct and c $_2$ is not, the loss is  $$\\begin{aligned}",
      "based on their knowledge about the world, it is a great challenge for machines when there is limited training data. We hypothesize that exploiting knowledge graphs for commonsense in QA modeling can help model choose correct answers. For example, as shown in the part B of Table 1 , some triples from ConceptNet BIBREF11 are quite related to the questions above. Exploiting these triples in the QA",
      "$_1$ , relation) or (relation, concept $_2$ ), that is, these candidates have close meanings. These more confusing candidates force BERT_CS to distinguish synonym meanings, resulting in a more powerful BERT_CS model. Comparing model 5 and model 6, we find that the multi-choice QA task works better than the masked LM task as the pre-training task for the target multi-choice QA task. We argue that,",
      "test the fine-tuned BERT_CS models on the WSC dataset. We transform the pronoun disambiguation problem into a multi-choice question answering problem. We mask the pronoun word with a special token [QW] to construct a question, and put the two candidate paragraphs as candidate answers. The remaining procedures are the same as QA tasks. We use the same loss function as BIBREF22 , that is, if c $_1$",
      "to construct a multi-choice QA dataset that align sentences with commonsense knowledge triples, mask the aligned words (entities/concepts) in sentences and treat the masked sentences as questions, and select several entities/concepts from knowledge graphs as candidate choices.",
      "examples from CommonsenseQA are shown in Table 1 part A. As can be seen from the examples, although it is easy for humans to answer the questions based on their knowledge about the world, it is a great challenge for machines when there is limited training data. We hypothesize that exploiting knowledge graphs for commonsense in QA modeling can help model choose correct answers. For example, as",
      "model 6, we find that pre-training BERT benefits from a more difficult dataset. In our selection method, all candidate answers share the same (concept $_1$ , relation) or (relation, concept $_2$ ), that is, these candidates have close meanings. These more confusing candidates force BERT_CS to distinguish synonym meanings, resulting in a more powerful BERT_CS model. Comparing model 5 and model 6,",
      "by randomly picking words or phrases in ConceptNet. In this paper, in order to generate more confusing distractors than the random selection approach, we request those distractors and the correct answer share the same concept $_2$ or concept $_1$ and the relation. That is to say, we search ( $\\ast $ , relation, concept $_2$ ) and (concept $_2$0 , relation, $_2$1 ) in ConceptNet to select the"
    ]
  },
  {
    "title": "Binary and Multitask Classification Model for Dutch Anaphora Resolution: Die/Dat Prediction",
    "answer": "The Europarl dataset contains 70,057 dat-labeled and 33,814 die-labeled sentences, and the other dataset has 103,871 (train) and 1,269,091 (test) sentences, but specifically for the sizes of **both datasets**: Europarl (103,871), and the other (1,269,091).",
    "evidence": [
      "An overview of the performance results is given in Table . We compare model performance when trained and tested on the two corpora individually and experiment with different settings of the two corpora in order to investigate the effect of dataset changes on model performance. There are three settings: full in which the datasets contain full sentences, windowed in which sentences are windowed",
      "Each dataset is randomly divided into a training (70%), validation (15%) and test set (15%). The data is fed to the model in batches of 128 samples and reshuffled at every epoch. The objective function that is minimized is Binary Cross-Entropy: where $y_i$ is the ground truth label (0 for dat and 1 for die) and $p(\\hat{y}_i)$ is the probability of the predicted label for all $N$ input sentences",
      "into a training (70%), validation (15%) and test (15%) set. The data is fed into the model in batches of 516 samples and the data is reshuffled at every epoch. For die/dat prediction, the Binary Cross-Entropy loss function is minimized. The weights are optimized by Stochastic Gradient Descent with learning rate = 0.01 and momentum = 0.9. For POS prediction, Cross-Entropy is minimized: where $C$",
      "multiple times in the training and test dataset with the unique token for die and dat at a different place in the sentence. Each sentence is paired with its automatically assigned ground truth label for die and dat. The Europarl dataset, on the one hand, contains 70,057 dat-labeled and 33,814 die-labeled sentences. The resulting train and test sets consist of 103,871 (Europarl) and 1,269,091"
    ]
  },
  {
    "title": "Seeing Things from a Different Angle: Discovering Diverse Perspectives about Claims",
    "answer": "***Design challenges*** are highlighted, but the context specifically does not elaborate; however, based on typical design discussions, common challenges include ***scalability***, ***usability***, ***sustainability***, and ***feasibility***. \n\nWithout specific details, a general answer would be: _design challenges_.",
    "evidence": [
      "Design Principles and Challenges"
    ]
  },
  {
    "title": "Unsupervised Machine Commenting with Neural Variational Topic Model",
    "answer": "QinEA2018 article-comment dataset.",
    "evidence": [
      "of neural machine commenting models is the requirement of a large parallel dataset. However, the naturally paired commenting dataset is loosely paired. Qin et al. QinEA2018 were the first to propose the article commenting task and an article-comment dataset. The dataset is crawled from a news website, and they sample 1,610 article-comment pairs to annotate the relevance score between articles and"
    ]
  },
  {
    "title": "Learn to Code-Switch: Data Augmentation using Copy Mechanism on Language Modeling",
    "answer": "Unfortunately, I don't see any specific information provided in the context. However, I can suggest a general approach to answering this type of question.\n\nTypically, when a paper explores languages, the information can be found in the abstract, introduction, or methodology section.\n\nGiven that the context only provides the heading \"Methodology\", I'll assume the answer might be nearby.\n\nSince I don't have more context or information, a precise answer can't be given. \n\nHowever, a typical answer to this question would be: \n\"the languages are not specified\" or alternatively, if I had more context I would look for: \n\"[name of languages]\". \n\nFor instance: \"English and Spanish\" \n\nLet's assume a more detailed context would provide languages explored; \nso based on general academic writing I propose ***No specific languages***",
    "evidence": []
  },
  {
    "title": "Rnn-transducer with language bias for end-to-end Mandarin-English code-switching speech recognition",
    "answer": "from transcription.",
    "evidence": [
      "previous works use an additional language identification (LID) model as an auxiliary module, which causes the system complex. In this work, we propose an improved recurrent neural network transducer (RNN-T) model with language bias to alleviate the problem. We use the language identities to bias the model to predict the CS points. This promotes the model to learn the language identity information",
      "BIBREF13. However, the lack of CS training data poses serious problem to end-to-end methods. To address the problem, language identity information is utilized to improve the performance of recognition BIBREF3, BIBREF4, BIBREF5. They are usually based on CTC or attention-based encoder-decoder models or the combination of both. However, previous works use an additional language identification (LID)",
      "the language identity information from transcription. In the inference process, the predicted language IDs are used to adjust the output posteriors. The experiment results on CS corpus show that our proposed method outperforms the RNN-T baseline (without language bias) significantly. Overall, our best model achieves 16.2% and 12.9% relative error reduction on two test sets, respectively. To our",
      "normal text, the tagged data can bias the RNN-T to predict language IDs in CS points. So our method can model the CS distribution directly, no additional LID model is needed. Then we constrain the input word embedding with its corresponding language ID, which is beneficial for model to learn the language identity information from transcription. In the inference process, the predicted language IDs",
      "LID model is needed. Then we constrain the input word embedding with its corresponding language ID, which is beneficial for model to learn the language identity information from transcription. In the inference process, the predicted language IDs are used to adjust the output posteriors. The experiment results on CS corpus show that our proposed method outperforms the RNN-T baseline (without",
      "Recently, language identity information has been utilized to improve the performance of end-to-end code-switching (CS) speech recognition. However, previous works use an additional language identification (LID) model as an auxiliary module, which causes the system complex. In this work, we propose an improved recurrent neural network transducer (RNN-T) model with language bias to alleviate the",
      "And the RNN-T and attention-based models trained with large speech corpus perform competitively compared to the state-of-art model in some tasks BIBREF13. However, the lack of CS training data poses serious problem to end-to-end methods. To address the problem, language identity information is utilized to improve the performance of recognition BIBREF3, BIBREF4, BIBREF5. They are usually based on",
      "an improved recurrent neural network transducer (RNN-T) model with language bias to alleviate the problem. We use the language identities to bias the model to predict the CS points. This promotes the model to learn the language identity information directly from transcription, and no additional LID model is needed. We evaluate the approach on a Mandarin-English CS corpus SEAME. Compared to our",
      "LID into the language model. In general, predicting language IDs only from text data is difficult. However, the joint training mechanism of RNN-T allows it to combine the language and acoustic information to model the CS distribution. Furthermore, the tagged text can bias the RNN-T to predict language IDs which indicates CS points, yet the model trained with normal text can not do this. That is",
      "and this distribution can be learned by neural network. The properties of RNN-T is key for the problem. It can predict rich set of target symbols such as speaker role and \"end-of-word\" symbol, which are not related to the input feature directly BIBREF14, BIBREF15. So the language IDs can also be treated as the output symbols. What's more, RNN-T can seamlessly integrate the acoustic and linguistic",
      "RNN-T with language bias",
      "models or the combination of both. However, previous works use an additional language identification (LID) model as an auxiliary module, which causes the system complex. In this paper, we propose an improved RNN-T model with language bias to alleviate the problem. The model is trained to predict language IDs as well as the subwords. To ensure the model can learn CS information, we add language",
      "we augment the output symbols set with language IDs $<chn>$ and $<eng>$ as shown in Fig. 1, i.e., $\\hat{\\mathcal {Y}} \\in \\bar{\\mathcal {Y}} \\cup \\lbrace <chn>,<eng>\\rbrace $. The intuition behind it is that the CS in the transcript may obey a certain probability distribution, and this distribution can be learned by neural network. The properties of RNN-T is key for the problem. It can predict",
      "Results and Analysis\nEffect of Language IDs Re-weighted Decode",
      "bias the model to predict the CS points. This promotes the model to learn the language identity information directly from transcription, and no additional LID model is needed. We evaluate the approach on a Mandarin-English CS corpus SEAME. Compared to our RNN-T baseline, the proposed method can achieve 16.2% and 12.9% relative error reduction on two test sets, respectively.",
      "utterances contain both Mandarin and English uttered by interviews and conversations. We use the standard data partitioning rule of previous works which consists of three parts: $train$, $test_{sge}$ and $test_{man}$ (see Table 1) BIBREF2. $test_{sge}$ is biased to Southeast Asian accent English speech and $test_{man}$ is biased to Mandarin speech. Building an end-to-end model requires lots of",
      "data. Furthermore, We can also observe that all the experiment results in $test_{sge}$ is much worse than $test_{man}$. This is probably that the accent English in data $test_{sge}$ is more difficult for the recognition system. Bilinguals usually have serious accent problem, which poses challenge to CSSR approaches. Because the data augmentation technology can significantly reduce the MER of",
      "Results and Analysis\nResults of Language Model Re-score",
      "also be treated as the output symbols. What's more, RNN-T can seamlessly integrate the acoustic and linguistic information. The prediction network of it can be viewed as an RNN language model which predict the current label given history labels BIBREF10. So it is effective in incorporating LID into the language model. In general, predicting language IDs only from text data is difficult. However,",
      "to the recognized words directly, the language IDs error rate is hard to compute. This result may imply that the prediction accuracy of our method is high enough to guide decoding. Meanwhile, We also find that the re-weighted method is more effective on the $test_{man}$ than $test_{sge}$. This could be caused by higher language IDs prediction accuracy in $test_{man}$. The results of the two",
      "is utilized to improve the performance of recognition BIBREF3, BIBREF4, BIBREF5. They are usually based on CTC or attention-based encoder-decoder models or the combination of both. However, previous works use an additional language identification (LID) model as an auxiliary module, which causes the system complex. In this paper, we propose an improved RNN-T model with language bias to alleviate",
      "the accent English in data $test_{sge}$ is more difficult for the recognition system. Bilinguals usually have serious accent problem, which poses challenge to CSSR approaches. Because the data augmentation technology can significantly reduce the MER of end-to-end model, we conduct all the following experiments based on augmented training data. In order to fairly compare the results of proposed",
      "of it can be viewed as an RNN language model which predict the current label given history labels BIBREF10. So it is effective in incorporating LID into the language model. In general, predicting language IDs only from text data is difficult. However, the joint training mechanism of RNN-T allows it to combine the language and acoustic information to model the CS distribution. Furthermore, the",
      "We conduct experiments on SEAME (South East Asia Mandarin English), a spontaneous conversational bilingual speech corpus BIBREF16. Most of the utterances contain both Mandarin and English uttered by interviews and conversations. We use the standard data partitioning rule of previous works which consists of three parts: $train$, $test_{sge}$ and $test_{man}$ (see Table 1) BIBREF2. $test_{sge}$ is",
      "to predict language IDs as well as the subwords. To ensure the model can learn CS information, we add language IDs in the CS point of transcription, as illustrated in Fig. 1. In the figure, we use the arrangements of different geometric icons to represent the CS distribution. Compared with normal text, the tagged data can bias the RNN-T to predict language IDs in CS points. So our method can"
    ]
  },
  {
    "title": "Fully Automated Fact Checking Using External Sources",
    "answer": "bi-LSTM embeddings and similarity features. \n\n(or, alternatively, in a more concise phrase)\n\nbi-LSTM embeddings. \n\nHowever, a more precise answer would include: \n\nthe concatenation of bi-LSTM embeddings and similarity features.",
    "evidence": [
      "of the units in the hidden layer. This represents a task-specific embedding of the input example, and in our experiments it turned out to be quite helpful. Unlike in the SVM only model, this time we use the bi-LSTM embeddings as an input to the SVM. Ultimately, this yields a combination of deep learning and task-specific embeddings with RBF kernels.",
      "the best-matching one as described above. Similarly, we only use a single best-matching triple of consecutive sentences from a Web page. We further feed the network with the similarity features described above. All these vectors are concatenated and fully connected to a much more compact hidden layer that captures the task-specific embeddings. This layer is connected to a softmax output unit to"
    ]
  },
  {
    "title": "Aspect Term Extraction with History Attention and Selective Transformation",
    "answer": "The gains are: 5.0%, 1.6%, 1.4%, and 1.3% absolute gains. \n\nHowever, a more precise answer based on the question would be:\n5.0%, 1.6%, 1.4%, and 1.3% absolute gains. \n\nBut if I have to give a more concise and precise answer to how much they outperform state-of-the-art methods:\n**Up to 5.0%**",
    "evidence": [
      "of SemEval ABSA, our framework achieves 5.0%, 1.6%, 1.4%, 1.3% absolute gains on  ,  ,  and  respectively. Our framework can outperform RNCRF, a state-of-the-art model based on dependency parsing, on all datasets. We also notice that RNCRF does not perform well on  and  (3.7% and 3.9% inferior than ours). We find that  and  contain many informal reviews, thus RNCRF's performance degradation is",
      "After inserting the STN component before the bi-linear attention, i.e. “OURS w/o THA”, we get about 1% absolute gains on each dataset, and then the performance is comparable to CMLA. By adding THA, i.e. “OURS”, the performance is further improved, and all state-of-the-art methods are surpassed.",
      "aspect predictions so as to leverage the coordinate structure and tagging schema constraints to upgrade the aspect prediction. Experimental results over four benchmark datasets clearly demonstrate that our framework can outperform all state-of-the-art methods.",
      "As shown in Table  , the proposed framework consistently obtains the best scores on all of the four datasets. Compared with the winning systems of SemEval ABSA, our framework achieves 5.0%, 1.6%, 1.4%, 1.3% absolute gains on  ,  ,  and  respectively. Our framework can outperform RNCRF, a state-of-the-art model based on dependency parsing, on all datasets. We also notice that RNCRF does not"
    ]
  },
  {
    "title": "Improving Visually Grounded Sentence Representations with Self-Attention",
    "answer": "COCO5K dataset.",
    "evidence": [
      "Conclusion and Future Work",
      "We train the model on images and respective captions from COCO5K dataset BIBREF2 . We augment the state-of-the-art sentence representations with those produced by our model and conduct a series of experiments on transfer tasks to test the quality of sentence representations. Through detailed analysis, we confirm our hypothesis that self-attention help our model produce more feature-rich visually",
      "state-of-the-art unsupervised learning model (Layer Normalized Skip-Thoughts, ST-LN) BIBREF31 . We evaluate the quality of sentence representations produced from different variants of our encoders on well-known transfer tasks: movie review sentiment (MR) BIBREF32 , customer reviews (CR) BIBREF33 , subjectivity (SUBJ) BIBREF34 , opinion polarity (MPQA) BIBREF35 , paraphrase identification (MSRP)",
      "Attention Mechanism at Work",
      "Adhering to the experimental settings of BIBREF1 , we concatenate sentence representations produced from our model with those obtained from the state-of-the-art unsupervised learning model (Layer Normalized Skip-Thoughts, ST-LN) BIBREF31 . We evaluate the quality of sentence representations produced from different variants of our encoders on well-known transfer tasks: movie review sentiment (MR)",
      "Related Work",
      "vectors for selected samples from MS-COCO dataset and compare them to associated images (Figure  ). For example, given the sentence “man in black shirt is playing guitar”, our model identifies words that have association with strong visual imagery, such as “man”, “black” and “guitar”. Given the second sentence, our model learned to attend to visually significant words such as “cat” and “bowl”."
    ]
  },
  {
    "title": "Deeper Task-Specificity Improves Joint Entity and Relation Extraction",
    "answer": "the inclusion of NER-specific layers, the inclusion of RE-specific layers. \n\nor \n\n(i) NER-specific BiRNN layers, (ii) RE-specific BiRNN layers, and (iii) both. \n\n More precisely: \n(i) zero NER-specific BiRNN layers, (ii) zero RE-specific BiRNN layers, or (iii) zero both  -specific  layers are not mentioned; only (i) and (ii) are. \n\nHence \nzero NER-specific BiRNN layers, zero RE-specific BiRNN layers.",
    "evidence": [
      "Experiments\nAblation Study",
      "Experiments\nResults",
      "the results from the ablation study. These results show that the proposed architecture benefits from the inclusion of both NER- and RE-specific layers. However, the RE task benefits much more from the inclusion of these task-specific layers than does the NER task. We take this to reflect the fact that the RE task is more difficult than the NER task for the CoNLL04 dataset, and therefore benefits",
      "in the baseline model. We average the results for each set of hyperparameter across three trials with random weight initializations. Table  contains the results from the ablation study. These results show that the proposed architecture benefits from the inclusion of both NER- and RE-specific layers. However, the RE task benefits much more from the inclusion of these task-specific layers than does",
      "SOTA architecture. An ablation study confirms the importance of the additional task-specific layers for achieving these results. Our work suggests that previous solutions to joint NER and RE undervalue task-specificity and demonstrates the importance of correctly balancing the number of shared and task-specific parameters for MTL approaches in general.",
      "kind. We increased the number of shared BiRNN layers to keep the total number of model parameters consistent with the number of parameters in the baseline model. We average the results for each set of hyperparameter across three trials with random weight initializations. Table  contains the results from the ablation study. These results show that the proposed architecture benefits from the",
      "SOTA results on the NER task and competitive results on the RE task while using an order of magnitude fewer trainable parameters than the current SOTA architecture. An ablation study confirms the importance of the additional task-specific layers for achieving these results. Our work suggests that previous solutions to joint NER and RE undervalue task-specificity and demonstrates the importance of",
      "To further demonstrate the effectiveness of the additional task-specific BiRNN layers in our architecture, we conducted an ablation study using the CoNLL04 dataset. We trained and evaluated in the same manner described above, using the same hyperparameters, with the following exceptions: We used either (i) zero NER-specific BiRNN layers, (ii) zero RE-specific BiRNN layers, or (iii) zero"
    ]
  },
  {
    "title": "Semi-supervised sequence tagging with bidirectional language models",
    "answer": "CoNLL2003 NER task and CoNLL2000 Chunking task.",
    "evidence": [
      ". We report the official evaluation metric (micro-averaged  ). In both cases, we use the BIOES labeling scheme for the output tags, following previous work which showed it outperforms other options BIBREF15 . Following BIBREF8 , we use the Senna word embeddings BIBREF2 and pre-processed the text by lowercasing all tokens and replacing all digits with 0.",
      "number of parameters as the baseline no LM model. In this case, test  increased slightly to  indicating that the additional parameters in TagLM are slightly hurting performance. One artifact of our evaluation framework is that both the labeled data in the chunking and NER tasks and the unlabeled text in the 1 Billion Word Benchmark used to train the bidirectional LMs are derived from news",
      "We evaluate our approach on two well benchmarked sequence tagging tasks, the CoNLL 2003 NER task BIBREF13 and the CoNLL 2000 Chunking task BIBREF14 . We report the official evaluation metric (micro-averaged  ). In both cases, we use the BIOES labeling scheme for the output tags, following previous work which showed it outperforms other options BIBREF15 . Following BIBREF8 , we use the Senna word"
    ]
  },
  {
    "title": "Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification",
    "answer": "Unfortunately, I don't see specific performance results mentioned in the provided context. However, based on typical academic writing conventions, I would expect the performance results to be presented in a section with a similar name.\n\n***the results*** or ***experimental results*** would be a likely location to find the performance results. \n\nHowever, I can only give a more specific answer with more information. \n\nIf I had to make an educated guess, I would say: \n\n***Results***.",
    "evidence": [
      "Experiments\nResults Analysis"
    ]
  },
  {
    "title": "NumNet: Machine Reading Comprehension with Numerical Reasoning",
    "answer": "MRC models, NAQANet, NAQANet+, BIBREF0, BIBREF1, BIBREF2, BIBREF3. \n\nor alternatively, more concisely: \nMRC models, NAQANet, NAQANet+.",
    "evidence": [
      "In this paper, we tune our model on the development set and use a grid search to determine the optimal parameters. The dimensions of all the representations (e.g., $\\mathbf {Q}$, $\\mathbf {P}$, $\\mathbf {M}^Q$, $\\mathbf {M}^P$, $\\mathbf {U}$, $\\mathbf {M}_0^{\\prime }$, $\\mathbf {M}_0$ and $\\mathbf {v}$) are set to 128. If not specified, the reasoning step $K$ is set to 3. Since other parameters",
      "MRC models and even numerical MRC models NAQANet and NAQANet+. The reason is that our NumNet model can make full use of the numerical comparison information over numbers in both question and passage via the proposed NumGNN module. (2) Our implemented NAQANet+ has a much better performance compared to the original version of NAQANet. It verifies the effectiveness of our proposed enhancements for",
      "and RACE BIBREF5. The success of these models is due to two reasons: (1) Multi-layer architectures which allow these models to read the document and the question iteratively for reasoning; (2) Attention mechanisms which would enable these models to focus on the part related to the question in the document. However, most of existing MRC models are still weak in numerical reasoning such as",
      "Experiments\nOverall Results",
      "of MRC models BIBREF0, BIBREF1, BIBREF2, BIBREF3 and these models have achieved remarkable results in various public benchmarks such as SQuAD BIBREF4 and RACE BIBREF5. The success of these models is due to two reasons: (1) Multi-layer architectures which allow these models to read the document and the question iteratively for reasoning; (2) Attention mechanisms which would enable these models to",
      "skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizing external knowledge; (5) Numerical reasoning. They found that most existing MRC models are focusing on dealing with the first three types of questions. However, all these models suffer from problems when answering the questions requiring numerical"
    ]
  },
  {
    "title": "Classifying topics in speech when all you have is crummy translations.",
    "answer": "Spanish-English.",
    "evidence": [
      "these studies, the query and search languages are the same, while we consider the cross-lingual case. There has been some limited work on cross-lingual keyword spotting BIBREF23, where ASR is cascaded with text-based cross-lingual retrieval. Some recent studies have attempted to use vision as a complementary modality to do cross-lingual retrieval BIBREF24, BIBREF25. But cross-lingual topic",
      "but instead focus only on the high importance terms. We lowercase the English translations and remove all punctuation, and stopwords. We further remove the terms occurring in more than 10% of the documents and those which occur in less than 2 documents, keeping only the 1000 most frequent out of the remaining. After preprocessing the training set, we have a feature matrix $V$ with dimensions",
      "cross-lingual keyword spotting BIBREF23, where ASR is cascaded with text-based cross-lingual retrieval. Some recent studies have attempted to use vision as a complementary modality to do cross-lingual retrieval BIBREF24, BIBREF25. But cross-lingual topic classification for speech has not been considered elsewhere, as far as we know.",
      "speech-to-text translation. While the translations are poor, they are still good enough to correctly classify 1-minute speech segments over 70% of the time - a 20% improvement over a majority-class baseline. Such a system might be useful for humanitarian applications like crisis response, where incoming speech must be quickly assessed for further action.",
      "for evaluation despite not always matching the assigned call topic prompts, since they indicate what an automatic topic classifier would predict given correct translations and they capture finer-grained changes in topic. Table  shows a few examples where the silver labels differ from the assigned call topic prompts. In the first example, the topic model was arguably incorrect, failing to pick up",
      "training data, we found that translations with a BLEU score as low as 13 are still able to correctly classify 61% of the speech segments. Cross-lingual topic modeling may be useful when the target language is high-resource. Here, we learned target topics just from the 20 hours of translations, but in future work, we could use a larger text corpus in the high-resource language to learn a more",
      "Our results show that poor speech translation can still be useful for speech classification in low-resource settings. By varying the amount of training data, we found that translations with a BLEU score as low as 13 are still able to correctly classify 61% of the speech segments. Cross-lingual topic modeling may be useful when the target language is high-resource. Here, we learned target topics",
      "a person finder database BIBREF0. Japanese text is high-resource, but there are many cases where it would be useful to make sense of speech in low-resource languages. For example, in Uganda, as in many parts of the world, the primary source of news is local radio stations, which broadcast in many languages. A pilot study from the United Nations Global Pulse Lab identified these radio stations as",
      "Quickly making sense of large amounts of linguistic data is an important application of language technology. For example, after the 2011 Japanese tsunami, natural language processing was used to quickly filter social media streams for messages about the safety of individuals, and to populate a person finder database BIBREF0. Japanese text is high-resource, but there are many cases where it would",
      "Results\nSpanish-English ST.",
      "to classify speech using supervision, but what is important about our result is it shows that a small amount of supervision goes a long way. A slightly different approach to quickly analysing speech is the established task of Keyword spotting BIBREF16, BIBREF17, which simply asks whether any of a specific set of keywords appears in each segment. Recent studies have extended the early work to",
      "Given a large amount of unannotated speech in a language with few resources, can we classify the speech utterances by topic? We show that this is possible if text translations are available for just a small amount of speech (less than 20 hours), using a recent model for direct speech-to-text translation. While the translations are poor, they are still good enough to correctly classify 1-minute",
      "ASR. In a task quite related to ours, BIBREF15 showed how to cluster speech segments in a completely unsupervised way. In contrast, we learn to classify speech using supervision, but what is important about our result is it shows that a small amount of supervision goes a long way. A slightly different approach to quickly analysing speech is the established task of Keyword spotting BIBREF16,",
      "given correct translations and they capture finer-grained changes in topic. Table  shows a few examples where the silver labels differ from the assigned call topic prompts. In the first example, the topic model was arguably incorrect, failing to pick up the prompt juries, and instead focusing on the other words, predicting intro-misc. But in the other examples, the topic model is reasonable, in",
      "Turning to our main task of classification, we first review the set of topics discovered from the human translations of train20h (Table ). We explored different numbers of topics, and chose 10 after reviewing the results. We assigned a name to each topic after manually reviewing the most informative terms; for topics with less coherent sets of informative terms, we include misc in their names. We",
      "whether any of a specific set of keywords appears in each segment. Recent studies have extended the early work to end-to-end keyword spotting BIBREF18, BIBREF19 and to semantic keyword retrieval, where non-exact but relevant keyword matches are retrieved BIBREF20, BIBREF21, BIBREF22. In all these studies, the query and search languages are the same, while we consider the cross-lingual case. There",
      "A slightly different approach to quickly analysing speech is the established task of Keyword spotting BIBREF16, BIBREF17, which simply asks whether any of a specific set of keywords appears in each segment. Recent studies have extended the early work to end-to-end keyword spotting BIBREF18, BIBREF19 and to semantic keyword retrieval, where non-exact but relevant keyword matches are retrieved",
      "tsunami, natural language processing was used to quickly filter social media streams for messages about the safety of individuals, and to populate a person finder database BIBREF0. Japanese text is high-resource, but there are many cases where it would be useful to make sense of speech in low-resource languages. For example, in Uganda, as in many parts of the world, the primary source of news is",
      "translations BIBREF5, but it has long been recognized that there are good uses for bad translations BIBREF6. Could classifying the original speech be one of those uses? We answer this question affirmatively: using ST to translate speech to text, we then classify by topic using supervised models (Figure ). We test our method on a corpus of conversational Spanish speech paired with English text"
    ]
  },
  {
    "title": "LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment",
    "answer": "Clinically validated PTSD assessment tools include:\n\n* PTSD Checklist for DSM-5 (PCL-5)\n* Clinician-Administered PTSD Scale for DSM-5 (CAPS-5)\n* PTSD Symptom Scale Interview (PSS-I) \n\nThese tools are widely used to assess PTSD symptoms. \n\nFor a more comprehensive list, some other tools are: \n* Davidson Trauma Scale (DTS)\n* Posttraumatic Stress Disorder Symptom Scale (PSS)",
    "evidence": [
      "Demographics of Clinically Validated PTSD Assessment Tools"
    ]
  },
  {
    "title": "Reference-less Quality Estimation of Text Simplification Systems",
    "answer": "***-gram-based comparison metrics, syntactic parsing, language modelling, semantic analysis, paraphrase detection models***.",
    "evidence": [
      "Comparing elementary metrics",
      "with respect to meaning preservation and grammaticality. We extend their work in two directions. Firstly, we extend the comparison to include the degree of simplicity achieved by the system. Secondly, we compare additional features, including those used by BIBREF18 , both individually, as elementary metrics, and within multi-feature metrics. To our knowledge, no previous work has provided as",
      "the degree of simplicity achieved by the system. Secondly, we compare additional features, including those used by BIBREF18 , both individually, as elementary metrics, and within multi-feature metrics. To our knowledge, no previous work has provided as thorough a comparison across such a wide range and combination of features for the reference-less evaluation of TS. First we review available text",
      "Experimental setup",
      "are best evaluated using  -gram-based comparison metrics might bias the TS models towards copying the source sentence and applying fewer modifications. Syntactic parsing or language modelling might capture more insightful grammatical information and allow for more flexibility in the simplification model. Regarding meaning preservation, semantic analysis or paraphrase detection models would also",
      "wide range and combination of features for the reference-less evaluation of TS. First we review available text simplification evaluation methods and traditional quality estimation features. We then present the QATS shared task and the associated dataset, which we use for our experiments. Finally we compare all methods in a reference-less setting and analyze the results."
    ]
  },
  {
    "title": "A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking",
    "answer": "Stanford Twitter Sentiment Corpus (STS Corpus), Sanders Twitter Sentiment Corpus, and no third one explicitly mentioned but likely Sander dataset referred as in text. However based on text we have: \n\n1. Stanford Twitter Sentiment Corpus (STS Corpus) \n2. Sanders Twitter Sentiment Corpus \n3. Not explicitly stated third one in provided snippet; However likely  Sander dataset referred.",
    "evidence": [
      "a sentence-wide feature representation from the word-level embedding. We evaluate our approach on three Twitter sentiment classification datasets. Experimental results show that our model can improve the classification accuracy of sentence-level sentiment analysis in Twitter social networking.",
      "Twitter sentiment classification have intensively researched in recent years BIBREF0 BIBREF1 . Different approaches were developed for Twitter sentiment classification by using machine learning such as Support Vector Machine (SVM) with rule-based features BIBREF2 and the combination of SVMs and Naive Bayes (NB) BIBREF3 . In addition, hybrid approaches combining lexicon-based and machine learning",
      "For the Stanford Twitter Sentiment Corpus, we use the number of samples as BIBREF5 . The training data is selected 80K tweets for a training data and 16K tweets for the development set randomly from the training data of BIBREF0 . We conduct a binary prediction for STS Corpus. For Sander dataset, we use standard 10-fold cross validation as BIBREF14 . We construct the development set by selecting",
      "evaluate the efficiency of SR for the model in Table V of our full paper . We also conduct two experiments on two separate models: DeepCNN and Bi-LSTM in order to show the effectiveness of combination of DeepCNN and Bi-LSTM. In addition, the model using TwitterGlove outperform the model using GoogleW2V because TwitterGlove captures more information in Twitter than GoogleW2V. These results show",
      "Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 . Sanders - Twitter Sentiment Corpus: This dataset consists of"
    ]
  },
  {
    "title": "Bleaching Text: Abstract Features for Cross-lingual Gender Prediction",
    "answer": "Accuracy, Fleiss kappa, Average accuracy (Avg), and Accuracy obtained when training on the concatenation of all languages but the target one (All).",
    "evidence": [
      "Human Evaluation",
      "for all in-language experiments. For the cross-lingual experiments, we train on all available source language data and test on all target language data. For the lexicalized experiments, we adopt the features from the best performing system at the latest PAN evaluation campaign BIBREF17 (word 1-2 grams and character 3-6 grams). For the multilingual embeddings model we use the mean embedding",
      "-gram size of this model through in-language cross-validation, finding that  performs best. When testing across languages, we report accuracy for two setups: average accuracy over each single-language model (Avg), and accuracy obtained when training on the concatenation of all languages but the target one (All). The latter setting is also used for the embeddings model. We report accuracy for all",
      "We use the scikit-learn BIBREF26 implementation of a linear SVM with default parameters (e.g., L2 regularization). We use 10-fold cross validation for all in-language experiments. For the cross-lingual experiments, we train on all available source language data and test on all target language data. For the lexicalized experiments, we adopt the features from the best performing system at the",
      "language data. For the lexicalized experiments, we adopt the features from the best performing system at the latest PAN evaluation campaign BIBREF17 (word 1-2 grams and character 3-6 grams). For the multilingual embeddings model we use the mean embedding representation from the system of BIBREF27 and add max, std and coverage features. We create multilingual embeddings by projecting monolingual",
      "had six judges, balanced for gender, and obtained three annotations per target user. Inter-annotator agreement for the tasks was measured via Fleiss kappa (  ), and was higher for the in-language experiment (  ) than for the cross-language tasks (NL  PT:  ; FR  NL:  ). Table  shows accuracy against the gold labels, comparing humans (average accuracy over three annotators) to lexical and bleached",
      "Acknowledgments\nWe would like to thank the three anonymous reviewers and our colleagues for their useful feedback on earlier versions of this paper. Furthermore, we are grateful to Chloé Braud for helping with the French human evaluation part. We would like to thank all of our human participants.",
      "Profiling with Abstract Features",
      "URLs, etc) and users' meta-data/network (number of followers, etc), but this information is not always available. We propose a novel approach where the actual text is still used, but bleached out and transformed into more abstract, and potentially better transferable features. One could view this as a method in between the open vocabulary strategy and the stylometric approach. It has the",
      "gender distribution. Each user was represented by twenty tweets. The answer key (F/M) order was randomized. For each of the three experiments we had six judges, balanced for gender, and obtained three annotations per target user. Inter-annotator agreement for the tasks was measured via Fleiss kappa (  ), and was higher for the in-language experiment (  ) than for the cross-language tasks (NL  PT:"
    ]
  },
  {
    "title": "Community Question Answering Platforms vs. Twitter for Predicting Characteristics of Urban Neighbourhoods",
    "answer": "***62*** demographic attributes.",
    "evidence": [
      "Unlike many current works that focus on predicting one or few selected attributes (e.g. deprivation, race or income) using social media data, we study a wide range of 62 demographic attributes. Furthermore, we test whether terms extracted from both Yahoo! Answers and Twitter are semantically related to these attributes and provide examples of sociocultural profiles of neighbourhoods through the"
    ]
  },
  {
    "title": "Bias in Semantic and Discourse Interpretation",
    "answer": "interpretive bias",
    "evidence": [
      "The model of interpretive bias",
      "Our paper is organized as follows. Section  introduces our model of interpretive bias. Section  looks forward towards some consequences of our model for learning and interpretation. We then draw some conclusions in Section  . A detailed and formal analysis of interpretive bias has important social implications. Questions of bias are not only timely but also pressing for democracies that are",
      "Objective of the paper",
      "claim that the march has triggered an important discussion, which is in turn buttressed by the reporter's mentioning of the responses of the Times' readership. A formally precise account of interpretive bias will thus require an analysis of histories and their structure and to this end, we exploit Segmented Discourse Representation Theory or SDRT BIBREF2 , BIBREF3 . As the most precise and",
      "our model for learning and interpretation. We then draw some conclusions in Section  . A detailed and formal analysis of interpretive bias has important social implications. Questions of bias are not only timely but also pressing for democracies that are having a difficult time dealing with campaigns of disinformation and a society whose information sources are increasingly fragmented and whose",
      "different interpretations. And these biases are revealed at least implicitly in how they interpret the story: Jury 1 is at the outset at least guarded, if not skeptical, in its appraisal of Sheehan's interest in answering the reporter's questions. On the other hand, Jury 2 is fully convinced of Sheehan's position and thus interprets his responses much more charitably. BIBREF0 shows formally that",
      "In this paper, we show how game-theoretic work on conversation combined with a theory of discourse structure provides a framework for studying interpretive bias. Interpretive bias is an essential feature of learning and understanding but also something that can be used to pervert or subvert the truth. The framework we develop here provides tools for understanding and analyzing the range of",
      "the question of whether it is possible to find such a truth oriented bias for a set of facts, and if so, under what conditions. Can we detect and avoid biases that don't get at the truth but are devised for some other purpose? Our study of interpretive bias relies on three key premises. The first premise is that histories are discursive interpretations of a set of data in the sense that like",
      "In this paper, we propose a program for research on bias. We will show how to model various types of bias as well as the way in which bias leads to the selection of a history for a set of data, where the data might be a set of nonlinguistic entities or a set of linguistically expressed contents. In particular, we'll look at what people call “unbiased” histories. For us these also involve a bias,",
      "on the structural relations between events to convey an overall positive perspective; it contrasts the controversy surrounding the march with a claim that the march has triggered an important discussion, which is in turn buttressed by the reporter's mentioning of the responses of the Times' readership. A formally precise account of interpretive bias will thus require an analysis of histories and",
      "Sheehan himself. What accounts for these divergent discourse structures? We will argue that it is the biases of the two Juries that create these different interpretations. And these biases are revealed at least implicitly in how they interpret the story: Jury 1 is at the outset at least guarded, if not skeptical, in its appraisal of Sheehan's interest in answering the reporter's questions. On the",
      "of Sheehan's position and thus interprets his responses much more charitably. BIBREF0 shows formally that there is a co-dependence between biases and interpretations; a certain interpretation created because of a certain bias can in turn strengthen that bias, and we will sketch some of the details of this story below. The situation of our two juries applies to a set of nonlinguistic facts. In",
      "In this paper, we have put forward the foundations of a formal model of interpretive bias. Our approach differs from philosophical and AI work on dialogue that links dialogue understanding to the recovery of speaker intentions and beliefs BIBREF56 , BIBREF57 . Studies of multimodal interactions in Human Robot Interaction (HRI) have also followed the Gricean tradition BIBREF58 , BIBREF59 ,",
      "the Times' readership. A formally precise account of interpretive bias will thus require an analysis of histories and their structure and to this end, we exploit Segmented Discourse Representation Theory or SDRT BIBREF2 , BIBREF3 . As the most precise and well-studied formal model of discourse structure and interpretation to date, SDRT enables us to characterize and to compare histories in terms",
      "But neither SDRT nor any other, extant theoretical or computational approach to discourse interpretation can adequately deal with the inherent subjectivity and interest relativity of interpretation, which our study of bias will illuminate. Message Exchange (ME) Games, a theory of games that builds on SDRT, supplements SDRT with an analysis of the purposes and assumptions that figure in bias.",
      "and avoid biases that don't get at the truth but are devised for some other purpose? Our study of interpretive bias relies on three key premises. The first premise is that histories are discursive interpretations of a set of data in the sense that like discourse interpretations, they link together a set of entities with semantically meaningful relations. As such they are amenable to an analysis",
      "guarded, if not skeptical, in its appraisal of Sheehan's interest in answering the reporter's questions. On the other hand, Jury 2 is fully convinced of Sheehan's position and thus interprets his responses much more charitably. BIBREF0 shows formally that there is a co-dependence between biases and interpretations; a certain interpretation created because of a certain bias can in turn strengthen",
      "bias and what goals a bias would satisfy. In this we also go beyond what current theories of discourse structure like SDRT can accomplish. Our theoretical work also requires an empirical component on exactly how bias is manifested to be complete. This has links to the recent interest in fake news. Modeling interpretive bias can help in detecting fake news by providing relevant types to check in",
      "interpretation is an instance of a bias that goes beyond the grammatically determined meaning of the play and is dependent upon the Jury's or interpreter's type. We now make explicit another crucial component of ME games and their relation to bias: the players' winning conditions or discourse goals. Why is this relevant to a study of bias? The short answer is that players' goals tells us whether",
      "an answer. The same interpretation of  would hold for this Jury when it is repeated in  f. Such a Jury would be a supporter of Sheehan or even Sheehan himself. What accounts for these divergent discourse structures? We will argue that it is the biases of the two Juries that create these different interpretations. And these biases are revealed at least implicitly in how they interpret the story:",
      "theoretical work also requires an empirical component on exactly how bias is manifested to be complete. This has links to the recent interest in fake news. Modeling interpretive bias can help in detecting fake news by providing relevant types to check in interpretation and by providing an epistemic foundation for fake news detection by exploiting ME truth games where one can draw from various",
      "estimation, even though they arguably have the goal of seeking an optimal or “true” solution. In a general analysis of interpretive bias like ours, however, it is an open question whether there is an objective norm or not, whether it is attainable and, if so, under what conditions, and whether an agent builds a history for attaining that norm or for some other purpose.",
      "histories that fulfill the discourse purpose; bad biases lead to histories that do not achieve the winning condition. The goals that a Jury or interpreter  adopts and her biases go together;  's interpretive bias is good for speaker  , if it helps  achieve her winning condition. Hence,  's beliefs about  are crucial to her success and rationalizable behavior. Based on those beliefs  's behavior",
      "Some examples of bias",
      "and empirical economists have provided valuable insights into cognitive biases in simple decision problems and simple mathematical tasks BIBREF14 . Some of this work, for example the bias of framing effects BIBREF7 , is directly relevant to our theory of interpretive bias. A situation is presented using certain lexical choices that lead to different “frames”:  of the people will live if you do"
    ]
  },
  {
    "title": "Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences",
    "answer": "LatticeLM (or latticelm).",
    "evidence": [
      "of the three considered methods. We found that the unsupervised morphological analyzer capable of using lattices improved the accuracy of phoneme recognition and word segmentation. Consequently, this result suggests that this word segmentation method considers the multiple hypothesis of speech recognition as a whole and reduces uncertainty such as variability in recognition by using the syllable",
      "learning targets from human continuous speech signals in both the room of the simulation environment and the entire floor of the real environment. Further, the unsupervised word segmentation method latticelm could reduce the variability and errors in the recognition of phonemes in all the utterances. SpCoA achieved more accurate lexical acquisition by performing word segmentation using the",
      "Further, the unsupervised word segmentation method latticelm could reduce the variability and errors in the recognition of phonemes in all the utterances. SpCoA achieved more accurate lexical acquisition by performing word segmentation using the lattices of the speech recognition results. In the self-localization experiments, the robot could effectively utilize the acquired spatial concepts for"
    ]
  },
  {
    "title": "Localization of Fake News Detection via Multitask Transfer Learning",
    "answer": "12,836.",
    "evidence": [
      "on the fake news dataset (appending no extra classification heads yet) for a total of 10 epochs, using a learning rate of 1e-2, a batch size of 80, and weight decay of 0.3. For the final ULMFiT finetuning stage, we append a compound classification head (linear $\\rightarrow $ batch normalization $\\rightarrow $ ReLU $\\rightarrow $ linear $\\rightarrow $ batch normalization $\\rightarrow $ softmax).",
      "Experimental Setup\nFake News Dataset",
      "the pretrained GPT-2 transformer for 3 epochs, using a batch size of 32, and a learning rate of 3e-5. For ULMFiT, we perform language model finetuning on the fake news dataset (appending no extra classification heads yet) for a total of 10 epochs, using a learning rate of 1e-2, a batch size of 80, and weight decay of 0.3. For the final ULMFiT finetuning stage, we append a compound classification",
      "considering the large size of the model. We train the model for 200 epochs with 1,000 steps of learning rate warmup using the Adam optimizer. The model was pretrained for 178 hours on a machine with one NVIDIA Tesla V100 GPU. For ULMFiT, we pretrain a 3-layer AWD-LSTM model with an embedding size of 400 and a hidden size of 1150. We set the dropout values for the embedding, the RNN input, the",
      "our classification head is first comprised of a layer normalization transform, followed by a linear layer, then a softmax transform. We finetune the pretrained GPT-2 transformer for 3 epochs, using a batch size of 32, and a learning rate of 3e-5. For ULMFiT, we perform language model finetuning on the fake news dataset (appending no extra classification heads yet) for a total of 10 epochs, using",
      "dataset BIBREF4, which contains 12,836 labeled short statements as well as sources to support the labels. This requirement for large datasets to effectively train fake news detection models from scratch makes it difficult to adapt these techniques into low-resource languages. Our work focuses on the use of Transfer Learning (TL) to evade this data scarcity problem. We make three contributions.",
      "embedding size of 400 and a hidden size of 1150. We set the dropout values for the embedding, the RNN input, the hidden-to-hidden transition, and the RNN output to (0.1, 0.3, 0.3, 0.4) respectively. We use a weight dropout of 0.5 on the LSTM’s recurrent weight matrices. The model was trained for 30 epochs with a learning rate of 1e-3, a batch size of 128, and a weight decay of 0.1. We use the"
    ]
  },
  {
    "title": "Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects",
    "answer": "***consistent increase in the validation loss***. \n\n(or alternatively, in a more concise phrase)\n\n***increase in validation loss***.",
    "evidence": [
      "examples affects the loss. For a fixed  = 32768 we varied the amount of data used for training from 25% to 100%. Three trends are apparent. As we train with more data, we obtain better validation losses. This was expected. The second trend is that by using less than 50% of the data available the model tends to overfit the data, as indicated by the consistent increase in the validation loss after"
    ]
  }
]